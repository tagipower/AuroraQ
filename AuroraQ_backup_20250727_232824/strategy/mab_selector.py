# mab_selector.py - ê°œì„  ë²„ì „
import random
import logging
import numpy as np
from collections import defaultdict
from typing import Dict, List, Optional, Tuple
from core.strategy_score_manager import get_all_current_scores, update_strategy_metrics

logger = logging.getLogger(__name__)


class MABSelector:
    """
    Multi-Armed Bandit ê¸°ë°˜ ë©”íƒ€ ì „ëµ ì„ íƒê¸° (ê°œì„  ë²„ì „)
    - UCB (Upper Confidence Bound) ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€
    - Epsilon decay ê¸°ëŠ¥
    - Thompson Sampling ì˜µì…˜
    """

    def __init__(
        self, 
        strategy_names: List[str], 
        epsilon: float = 0.1,
        algorithm: str = "epsilon_greedy",  # epsilon_greedy, ucb, thompson
        decay_rate: float = 0.995,
        min_epsilon: float = 0.01,
        ucb_c: float = 2.0
    ):
        """
        Args:
            strategy_names: ì „ëµ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            epsilon: íƒìƒ‰ í™•ë¥  (0~1)
            algorithm: MAB ì•Œê³ ë¦¬ì¦˜ íƒ€ì…
            decay_rate: epsilon ê°ì†Œìœ¨
            min_epsilon: ìµœì†Œ epsilon ê°’
            ucb_c: UCB íƒìƒ‰ íŒŒë¼ë¯¸í„°
        """
        self.strategy_names = strategy_names
        self.epsilon = epsilon
        self.initial_epsilon = epsilon
        self.algorithm = algorithm
        self.decay_rate = decay_rate
        self.min_epsilon = min_epsilon
        self.ucb_c = ucb_c
        
        # í†µê³„ ì¶”ì  - ì¦ë¶„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì¶”ê°€ í•„ë“œ
        self.counts = defaultdict(int)
        self.total_rewards = defaultdict(float)
        self.squared_rewards = defaultdict(float)  # ë¶„ì‚° ê³„ì‚°ìš©
        self.running_mean = defaultdict(float)  # ì¦ë¶„ í‰ê· 
        self.running_variance = defaultdict(float)  # ì¦ë¶„ ë¶„ì‚°
        
        # Thompson Samplingìš© ë² íƒ€ ë¶„í¬ íŒŒë¼ë¯¸í„°
        self.alpha = defaultdict(lambda: 1.0)
        self.beta = defaultdict(lambda: 1.0)
        
        self.total_selections = 0
        
        logger.info(
            f"MAB Selector ì´ˆê¸°í™”: algorithm={algorithm}, "
            f"epsilon={epsilon}, strategies={len(strategy_names)}"
        )

    def select(self) -> str:
        """ì „ëµ ì„ íƒ (ì•Œê³ ë¦¬ì¦˜ë³„ ë¶„ê¸°)"""
        self.total_selections += 1
        
        if self.algorithm == "epsilon_greedy":
            return self._select_epsilon_greedy()
        elif self.algorithm == "ucb":
            return self._select_ucb()
        elif self.algorithm == "thompson":
            return self._select_thompson()
        else:
            return self._select_epsilon_greedy()

    def _select_epsilon_greedy(self) -> str:
        """Epsilon-Greedy ì„ íƒ"""
        current_scores = get_all_current_scores()
        
        # ë¯¸íƒìƒ‰ ì „ëµ ìš°ì„ 
        unexplored = [s for s in self.strategy_names if self.counts[s] == 0]
        if unexplored:
            chosen = random.choice(unexplored)
            logger.info(f"[MAB-EG] ğŸ” ë¯¸íƒìƒ‰ ì „ëµ: {chosen}")
            return chosen
        
        # Epsilon decay
        self._decay_epsilon()
        
        # íƒìƒ‰ vs í™œìš©
        if random.random() < self.epsilon:
            chosen = random.choice(self.strategy_names)
            logger.info(f"[MAB-EG] ğŸ² íƒìƒ‰: {chosen} (Îµ={self.epsilon:.3f})")
        else:
            # í˜„ì¬ ì ìˆ˜ + ê³¼ê±° í‰ê·  ë³´ìƒ ê³ ë ¤
            combined_scores = {}
            for s in self.strategy_names:
                current_score = current_scores.get(s, 0)
                avg_reward = self._get_average_reward(s)
                combined_scores[s] = 0.7 * current_score + 0.3 * avg_reward
            
            chosen = max(combined_scores, key=combined_scores.get)
            logger.info(
                f"[MAB-EG] âœ… í™œìš©: {chosen} "
                f"(score={combined_scores[chosen]:.3f})"
            )
        
        return chosen

    def _select_ucb(self) -> str:
        """Upper Confidence Bound ì„ íƒ"""
        current_scores = get_all_current_scores()
        
        # ë¯¸íƒìƒ‰ ì „ëµ ìš°ì„ 
        unexplored = [s for s in self.strategy_names if self.counts[s] == 0]
        if unexplored:
            chosen = random.choice(unexplored)
            logger.info(f"[MAB-UCB] ğŸ” ë¯¸íƒìƒ‰ ì „ëµ: {chosen}")
            return chosen
        
        # UCB ê³„ì‚°
        ucb_values = {}
        for s in self.strategy_names:
            avg_reward = self._get_average_reward(s)
            exploration_bonus = self.ucb_c * np.sqrt(
                np.log(self.total_selections) / self.counts[s]
            )
            ucb_values[s] = avg_reward + exploration_bonus
        
        chosen = max(ucb_values, key=ucb_values.get)
        logger.info(
            f"[MAB-UCB] ğŸ“Š ì„ íƒ: {chosen} "
            f"(UCB={ucb_values[chosen]:.3f})"
        )
        
        return chosen

    def _select_thompson(self) -> str:
        """Thompson Sampling ì„ íƒ"""
        # ê° ì „ëµì—ì„œ ìƒ˜í”Œë§
        samples = {}
        for s in self.strategy_names:
            # Beta ë¶„í¬ì—ì„œ ìƒ˜í”Œë§
            sample = np.random.beta(self.alpha[s], self.beta[s])
            samples[s] = sample
        
        chosen = max(samples, key=samples.get)
        logger.info(
            f"[MAB-TS] ğŸ¯ ì„ íƒ: {chosen} "
            f"(sample={samples[chosen]:.3f})"
        )
        
        return chosen

    def update(self, strategy_name: str, reward: float, metrics: Optional[Dict] = None):
        """ë³´ìƒ ì—…ë°ì´íŠ¸ (ì¦ë¶„ í†µê³„ ì‚¬ìš©)"""
        # ì¦ë¶„ í†µê³„ ì—…ë°ì´íŠ¸
        n = self.counts[strategy_name]
        old_mean = self.running_mean[strategy_name]
        
        # Welford's online algorithmë¡œ í‰ê· ê³¼ ë¶„ì‚° ì¦ë¶„ ì—…ë°ì´íŠ¸
        self.counts[strategy_name] += 1
        new_n = self.counts[strategy_name]
        
        # í‰ê·  ì—…ë°ì´íŠ¸
        delta = reward - old_mean
        self.running_mean[strategy_name] = old_mean + delta / new_n
        
        # ë¶„ì‚° ì—…ë°ì´íŠ¸
        delta2 = reward - self.running_mean[strategy_name]
        self.running_variance[strategy_name] += delta * delta2
        
        # ì „ì²´ í†µê³„ (í˜¸í™˜ì„± ìœ ì§€)
        self.total_rewards[strategy_name] += reward
        self.squared_rewards[strategy_name] += reward ** 2
        
        # Thompson Samplingìš© ë² íƒ€ ë¶„í¬ ì—…ë°ì´íŠ¸
        if reward > 0.5:  # ì„±ê³µìœ¼ë¡œ ê°„ì£¼
            self.alpha[strategy_name] += 1
        else:
            self.beta[strategy_name] += 1
        
        # Score Manager ì—…ë°ì´íŠ¸
        if metrics is None:
            metrics = {"reward_shaping_score": reward}
        else:
            metrics["reward_shaping_score"] = reward
            
        update_strategy_metrics(strategy_name, metrics)
        
        logger.info(
            f"[MAB] ğŸ“ˆ ì—…ë°ì´íŠ¸: {strategy_name} â†’ "
            f"reward={reward:.4f}, count={self.counts[strategy_name]}, "
            f"mean={self.running_mean[strategy_name]:.4f}"
        )

    def _get_average_reward(self, strategy_name: str) -> float:
        """í‰ê·  ë³´ìƒ ê³„ì‚° (ì¦ë¶„ í‰ê·  ì‚¬ìš©)"""
        if self.counts[strategy_name] == 0:
            return 0.0
        # ì¦ë¶„ í‰ê·  ì‚¬ìš© (ë” ì •í™•í•˜ê³  ë¹ ë¦„)
        return self.running_mean[strategy_name]

    def _get_reward_std(self, strategy_name: str) -> float:
        """ë³´ìƒ í‘œì¤€í¸ì°¨ ê³„ì‚° (ì¦ë¶„ ë¶„ì‚° ì‚¬ìš©)"""
        if self.counts[strategy_name] < 2:
            return 0.0
        
        # ì¦ë¶„ ë¶„ì‚°ì—ì„œ í‘œì¤€í¸ì°¨ ê³„ì‚°
        variance = self.running_variance[strategy_name] / (self.counts[strategy_name] - 1)
        return np.sqrt(max(0, variance))

    def _decay_epsilon(self):
        """Epsilon ê°ì†Œ"""
        if self.epsilon > self.min_epsilon:
            self.epsilon *= self.decay_rate
            self.epsilon = max(self.epsilon, self.min_epsilon)

    def get_statistics(self) -> Dict[str, Dict]:
        """ìƒì„¸ í†µê³„ ë°˜í™˜"""
        stats = {}
        
        for s in self.strategy_names:
            avg_reward = self._get_average_reward(s)
            std_reward = self._get_reward_std(s)
            
            stats[s] = {
                "count": self.counts[s],
                "total_reward": round(self.total_rewards[s], 4),
                "avg_reward": round(avg_reward, 4),
                "std_reward": round(std_reward, 4),
                "selection_rate": round(
                    self.counts[s] / max(1, self.total_selections), 3
                )
            }
            
            # Thompson Sampling íŒŒë¼ë¯¸í„°
            if self.algorithm == "thompson":
                stats[s]["alpha"] = round(self.alpha[s], 2)
                stats[s]["beta"] = round(self.beta[s], 2)
        
        return stats

    def reset_strategy(self, strategy_name: str):
        """íŠ¹ì • ì „ëµ í†µê³„ ë¦¬ì…‹"""
        self.counts[strategy_name] = 0
        self.total_rewards[strategy_name] = 0.0
        self.squared_rewards[strategy_name] = 0.0
        self.running_mean[strategy_name] = 0.0
        self.running_variance[strategy_name] = 0.0
        self.alpha[strategy_name] = 1.0
        self.beta[strategy_name] = 1.0
        
        logger.info(f"[MAB] ì „ëµ ë¦¬ì…‹: {strategy_name}")

    def get_best_strategy(self) -> Tuple[str, float]:
        """ìµœê³  ì„±ê³¼ ì „ëµ ë°˜í™˜"""
        if not any(self.counts.values()):
            return self.strategy_names[0], 0.0
        
        avg_rewards = {
            s: self._get_average_reward(s) 
            for s in self.strategy_names 
            if self.counts[s] > 0
        }
        
        best_strategy = max(avg_rewards, key=avg_rewards.get)
        return best_strategy, avg_rewards[best_strategy]

    def should_explore(self) -> bool:
        """íƒìƒ‰ ì—¬ë¶€ ê²°ì • (ë””ë²„ê¹…ìš©)"""
        if self.algorithm == "epsilon_greedy":
            return random.random() < self.epsilon
        return False