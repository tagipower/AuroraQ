import os
import sys
import logging
import pandas as pd
from datetime import timedelta, datetime

# âœ… ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from core.ppo_agent_trainable import PPOAgentTrainable
from envs.env_router import get_env
from utils.reward_config_loader import load_reward_config
from utils.reward_calculator import RewardCalculator
from config.rule_param_loader import get_rule_params
from report.strategy_param_logger import log_strategy_params
from report.html_report_generator import generate_html_report
from strategy_score_manager import update_score  # âœ… ì „ëµ ì ìˆ˜ ì—…ë°ì´íŠ¸

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("TrainLoop")

def get_device():
    import torch
    return "cuda" if torch.cuda.is_available() else "cpu"

def train():
    device = get_device()
    print(f"Device set to use {device}")

    # âœ… ì„¤ì •ê°’
    env_name = "rule_e"
    window_size = 10
    timesteps = 100_000
    model_path_pt = "models/ppo_latest.pt"
    model_path_zip = "models/ppo_latest.zip"
    csv_path = "data/train_data.csv"

    logger.info(f"í•™ìŠµ ë°ì´í„°: {csv_path}")
    logger.info(f"í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜: {timesteps}")
    logger.info(f"ì €ì¥ ìœ„ì¹˜ (.pt): {model_path_pt}")
    logger.info(f"ì €ì¥ ìœ„ì¹˜ (.zip): {model_path_zip}")

    # âœ… íŒŒë¼ë¯¸í„° ê¸°ë¡
    if env_name.startswith("rule_"):
        params = get_rule_params(env_name.upper().replace("RULE_", "Rule"))
        log_strategy_params(env_name, params)

    # âœ… ë°ì´í„° ë¡œë”©
    data = pd.read_csv(csv_path)
    if len(data) < window_size:
        raise ValueError(f"í•™ìŠµ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ (ìµœì†Œ {window_size}í–‰ í•„ìš”)")

    # âœ… ê²°ì¸¡ ì»¬ëŸ¼ ë³´ì™„
    data["sentiment_score"] = data.get("sentiment_score", 0.0)
    data["event_risk"] = data.get("event_risk", False)
    data["regime"] = data.get("regime", "neutral")
    data["long_term_trend"] = data.get("long_term_trend", "sideways")
    data["sentiment_delta"] = data.get("sentiment_delta", 0.0)
    if "timestamp" not in data.columns:
        data["timestamp"] = [datetime(2025, 1, 1) + timedelta(minutes=5 * i) for i in range(len(data))]
    data["news_text"] = data.get("news_text", "")

    # âœ… ë³´ìƒ ê³„ì‚°ê¸° êµ¬ì„±
    reward_config = load_reward_config()
    reward_calculator = RewardCalculator(config=reward_config, sentiment_mode="backtest")

    # âœ… í™˜ê²½ ìƒì„±
    env = get_env(
        env_name=env_name,
        data=data,
        reward_calculator=reward_calculator,
        config={"reward": reward_config},
        window_size=window_size
    )

    # âœ… PPO í•™ìŠµ
    agent = PPOAgentTrainable(env)
    logger.info("ğŸ“š PPO í•™ìŠµ ì‹œì‘")
    training_info = agent.train(timesteps)

    # âœ… ëª¨ë¸ ì €ì¥ (ì„ íƒ)
    agent.save(model_path_pt, model_path_zip)

    # âœ… ì „ëµ ì ìˆ˜ í‰ê°€ ë° ì €ì¥
    try:
        ppo_score = agent.score()  # PPOAgentTrainableì— .score() ë©”ì„œë“œ êµ¬í˜„ í•„ìš”
        update_score("PPOStrategy", ppo_score, is_real_trade=False)
        logger.info(f"ğŸ“Š PPO ì ìˆ˜í™” ì™„ë£Œ: {ppo_score}")

        # âœ… ë¦¬í¬íŠ¸ ìƒì„±
        result = {
            "pnl_pct": round(ppo_score.get("total_score", 0.0) * 100, 2),
            "action": "N/A (train)",
            "leverage": 1
        }
        generate_html_report("ppo", result, output_dir="report/train/")

    except Exception as e:
        logger.warning(f"âš ï¸ PPO ì ìˆ˜í™” ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    train()
