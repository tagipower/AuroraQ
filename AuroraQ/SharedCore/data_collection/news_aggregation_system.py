#!/usr/bin/env python3
"""
News Aggregation System
ëª¨ë“  ë¬´ë£Œ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°ë¥¼ í†µí•©í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import asyncio
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import logging
from pathlib import Path

from .base_collector import (
    NewsAggregator, NewsArticle, NewsCategory, 
    CollectorConfig, SentimentScore
)
from .collectors.google_news_collector import GoogleNewsCollector
from .collectors.yahoo_finance_collector import YahooFinanceCollector
from .collectors.reddit_collector import RedditCollector
from .collectors.newsapi_collector import NewsAPICollector
from .collectors.finnhub_collector import FinnhubCollector
from .bitcoin_futures_filter import BitcoinFuturesNewsFilter, MarketRelevanceScore
from .advanced_news_query_engine import AdvancedNewsQueryEngine, SearchQuery, MarketImpactCategory


class AuroraQNewsAggregator(NewsAggregator):
    """AuroraQ ì „ìš© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        super().__init__()
        self.setup_collectors()
        
        # ë¹„íŠ¸ì½”ì¸ ì„ ë¬¼ì‹œì¥ íŠ¹í™” í•„í„° ì´ˆê¸°í™”
        self.btc_futures_filter = BitcoinFuturesNewsFilter()
        
        # ê³ ê¸‰ ë‰´ìŠ¤ ì¿¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™”
        self.query_engine = AdvancedNewsQueryEngine()
        
        # ìˆ˜ì§‘ í†µê³„
        self.collection_stats = {
            "total_articles": 0,
            "by_category": {},
            "by_source": {},
            "sentiment_distribution": {"positive": 0, "negative": 0, "neutral": 0},
            "last_collection": None,
            "high_impact_articles": 0,
            "market_moving_events": []
        }
    
    def setup_collectors(self):
        """ëª¨ë“  ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ë° ë“±ë¡"""
        try:
            # 1. Google News (í•­ìƒ ì‚¬ìš© ê°€ëŠ¥)
            google_collector = GoogleNewsCollector()
            self.register_collector("google_news", google_collector)
            self.logger.info("âœ… Google News collector registered")
            
            # 2. Yahoo Finance (í•­ìƒ ì‚¬ìš© ê°€ëŠ¥)
            yahoo_collector = YahooFinanceCollector()
            self.register_collector("yahoo_finance", yahoo_collector)
            self.logger.info("âœ… Yahoo Finance collector registered")
            
            # 3. Reddit (API ì°¨ë‹¨ìœ¼ë¡œ ë¹„í™œì„±í™”ë¨)
            reddit_collector = RedditCollector()
            self.register_collector("reddit", reddit_collector)
            self.logger.warning("âš ï¸ Reddit collector registered but API is blocked - will return empty results")
            
            # 4. NewsAPI (API í‚¤ í•„ìš”)
            if os.getenv("NEWSAPI_KEY"):
                newsapi_collector = NewsAPICollector()
                self.register_collector("newsapi", newsapi_collector)
                self.logger.info("âœ… NewsAPI collector registered")
            else:
                self.logger.warning("âš ï¸ NewsAPI key not found - skipping")
            
            # 5. Finnhub (API í‚¤ í•„ìš”)
            if os.getenv("FINNHUB_API_KEY"):
                finnhub_collector = FinnhubCollector()
                self.register_collector("finnhub", finnhub_collector)
                self.logger.info("âœ… Finnhub collector registered")
            else:
                self.logger.warning("âš ï¸ Finnhub key not found - skipping")
                
        except Exception as e:
            self.logger.error(f"Error setting up collectors: {e}")
    
    async def collect_comprehensive_news(self, 
                                       categories: List[NewsCategory] = None,
                                       hours_back: int = 6,
                                       articles_per_category: int = 20) -> Dict[str, List[NewsArticle]]:
        """í¬ê´„ì  ë‰´ìŠ¤ ìˆ˜ì§‘"""
        if categories is None:
            categories = [
                NewsCategory.HEADLINE,
                NewsCategory.CRYPTO,
                NewsCategory.FINANCE,
                NewsCategory.MACRO,
                NewsCategory.BREAKING
            ]
        
        results = {}
        since = datetime.now() - timedelta(hours=hours_back)
        
        self.logger.info(f"ğŸ”„ Starting comprehensive news collection for {len(categories)} categories")
        
        # ë³‘ë ¬ë¡œ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘
        tasks = []
        for category in categories:
            task = self._collect_category_news(category, since, articles_per_category)
            tasks.append(task)
        
        category_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì •ë¦¬
        for i, category in enumerate(categories):
            if isinstance(category_results[i], Exception):
                self.logger.error(f"Error collecting {category.value}: {category_results[i]}")
                results[category.value] = []
            else:
                results[category.value] = category_results[i]
                self.logger.info(f"âœ… {category.value}: {len(category_results[i])} articles")
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_collection_stats(results)
        
        return results
    
    async def _collect_category_news(self, category: NewsCategory, 
                                   since: datetime, 
                                   count: int) -> List[NewsArticle]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        articles = []
        
        # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤í•‘
        category_keywords = {
            NewsCategory.CRYPTO: ["bitcoin", "ethereum", "crypto", "blockchain"],
            NewsCategory.FINANCE: ["stock", "market", "trading", "finance"],
            NewsCategory.MACRO: ["federal reserve", "inflation", "gdp", "cpi"],
            NewsCategory.PERSON: ["ceo", "elon musk", "jerome powell"],
            NewsCategory.BREAKING: ["breaking", "urgent", "alert"]
        }
        
        keywords = category_keywords.get(category, ["news"])
        
        # ê° ìˆ˜ì§‘ê¸°ì—ì„œ ë³‘ë ¬ ìˆ˜ì§‘
        tasks = []
        for name, collector in self.collectors.items():
            if category == NewsCategory.HEADLINE:
                task = collector.collect_headlines(count=count//len(self.collectors))
            elif category == NewsCategory.BREAKING:
                task = collector.get_breaking_news(minutes=60)
            else:
                task = collector.search_news(keywords, since=since, count=count//len(self.collectors))
            
            tasks.append(task)
        
        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.error(f"Error in collector {i}: {result}")
                    continue
                
                for article in result:
                    article.category = category  # ì¹´í…Œê³ ë¦¬ ê°•ì œ ì„¤ì •
                    articles.append(article)
        
        except Exception as e:
            self.logger.error(f"Error in category collection: {e}")
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        return self._deduplicate_and_sort(articles)[:count]
    
    async def get_market_moving_news(self, minutes: int = 30) -> Dict[str, Any]:
        """ì‹œì¥ ì˜í–¥ ë‰´ìŠ¤ ë¶„ì„"""
        self.logger.info(f"ğŸ“ˆ Analyzing market-moving news (last {minutes} minutes)")
        
        # ìµœê·¼ ì†ë³´ ìˆ˜ì§‘
        breaking_news = await self.get_breaking_news_all(minutes=minutes)
        
        # ê³ ì¤‘ìš”ë„ ë‰´ìŠ¤ í•„í„°ë§
        high_impact_news = []
        market_keywords = [
            "federal reserve", "inflation", "recession", "rate hike",
            "earnings", "bitcoin", "crypto", "stock market",
            "gdp", "unemployment", "cpi", "fomc"
        ]
        
        for article in breaking_news:
            text = (article.title + " " + article.summary).lower()
            if any(keyword in text for keyword in market_keywords):
                high_impact_news.append(article)
        
        # ê°ì • ë¶„ì„
        sentiment_scores = []
        for article in high_impact_news:
            if article.sentiment_score:
                sentiment_scores.append(article.sentiment_score)
        
        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0
        
        # ì£¼ìš” ì´ë²¤íŠ¸ ì¶”ì¶œ
        major_events = self._extract_major_events(high_impact_news)
        
        return {
            "analysis_time": datetime.now().isoformat(),
            "time_window_minutes": minutes,
            "total_breaking_news": len(breaking_news),
            "high_impact_count": len(high_impact_news),
            "market_sentiment": {
                "score": avg_sentiment,
                "label": "bullish" if avg_sentiment > 0.2 else "bearish" if avg_sentiment < -0.2 else "neutral"
            },
            "major_events": major_events,
            "top_articles": [
                {
                    "title": article.title,
                    "source": article.source,
                    "sentiment": article.sentiment_score,
                    "url": article.url,
                    "published": article.published_date.isoformat()
                }
                for article in high_impact_news[:5]
            ]
        }
    
    def _extract_major_events(self, articles: List[NewsArticle]) -> List[Dict[str, Any]]:
        """ì£¼ìš” ì´ë²¤íŠ¸ ì¶”ì¶œ"""
        events = []
        
        # ì´ë²¤íŠ¸ íŒ¨í„´ ë§¤ì¹­
        event_patterns = {
            "fed_announcement": ["federal reserve", "fed", "fomc", "jerome powell"],
            "earnings_release": ["earnings", "quarterly results", "revenue"],
            "crypto_movement": ["bitcoin", "ethereum", "crypto"],
            "economic_data": ["inflation", "cpi", "gdp", "unemployment"],
            "market_volatility": ["crash", "surge", "plunge", "rally"]
        }
        
        for event_type, keywords in event_patterns.items():
            matching_articles = []
            
            for article in articles:
                text = (article.title + " " + article.summary).lower()
                if any(keyword in text for keyword in keywords):
                    matching_articles.append(article)
            
            if matching_articles:
                # ê°€ì¥ ë†’ì€ ê´€ë ¨ì„±ì„ ê°€ì§„ ê¸°ì‚¬
                best_article = max(matching_articles, 
                                 key=lambda x: x.relevance_score or 0)
                
                events.append({
                    "event_type": event_type,
                    "article_count": len(matching_articles),
                    "primary_article": {
                        "title": best_article.title,
                        "source": best_article.source,
                        "sentiment": best_article.sentiment_score
                    },
                    "overall_sentiment": sum(a.sentiment_score or 0 for a in matching_articles) / len(matching_articles)
                })
        
        return events
    
    async def analyze_sentiment_trends(self, hours: int = 24) -> Dict[str, Any]:
        """ê°ì • íŠ¸ë Œë“œ ë¶„ì„"""
        self.logger.info(f"ğŸ“Š Analyzing sentiment trends (last {hours} hours)")
        
        # ì‹œê°„ë³„ ë°ì´í„° ìˆ˜ì§‘
        time_windows = []
        window_size = hours // 6  # 6ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        
        for i in range(6):
            window_start = datetime.now() - timedelta(hours=hours - (i * window_size))
            window_end = datetime.now() - timedelta(hours=hours - ((i + 1) * window_size))
            
            # í•´ë‹¹ ì‹œê°„ëŒ€ ë‰´ìŠ¤ ìˆ˜ì§‘
            window_articles = []
            for name, collector in self.collectors.items():
                try:
                    articles = await collector.search_news(
                        keywords=["bitcoin", "stock", "market"],
                        since=window_end,
                        until=window_start,
                        count=20
                    )
                    window_articles.extend(articles)
                except:
                    continue
            
            # ê°ì • ì§‘ê³„
            sentiments = {"positive": 0, "negative": 0, "neutral": 0}
            total_score = 0
            
            for article in window_articles:
                if article.sentiment_label:
                    if article.sentiment_label.value > 0:
                        sentiments["positive"] += 1
                    elif article.sentiment_label.value < 0:
                        sentiments["negative"] += 1
                    else:
                        sentiments["neutral"] += 1
                
                if article.sentiment_score:
                    total_score += article.sentiment_score
            
            avg_sentiment = total_score / len(window_articles) if window_articles else 0
            
            time_windows.append({
                "time_start": window_start.isoformat(),
                "time_end": window_end.isoformat(),
                "article_count": len(window_articles),
                "sentiment_distribution": sentiments,
                "average_sentiment": avg_sentiment
            })
        
        return {
            "analysis_period_hours": hours,
            "time_windows": time_windows,
            "overall_trend": self._calculate_trend(time_windows),
            "timestamp": datetime.now().isoformat()
        }
    
    def _calculate_trend(self, windows: List[Dict[str, Any]]) -> str:
        """íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(windows) < 2:
            return "insufficient_data"
        
        early_sentiment = sum(w["average_sentiment"] for w in windows[:3]) / 3
        late_sentiment = sum(w["average_sentiment"] for w in windows[-3:]) / 3
        
        change = late_sentiment - early_sentiment
        
        if change > 0.1:
            return "improving"
        elif change < -0.1:
            return "deteriorating"
        else:
            return "stable"
    
    async def get_personalized_feed(self, interests: List[str], count: int = 50) -> List[NewsArticle]:
        """ê°œì¸í™”ëœ ë‰´ìŠ¤ í”¼ë“œ"""
        self.logger.info(f"ğŸ¯ Creating personalized feed for interests: {interests}")
        
        # ê´€ì‹¬ì‚¬ë³„ ê°€ì¤‘ì¹˜
        interest_weights = {interest: 1.0 for interest in interests}
        
        # ê° ê´€ì‹¬ì‚¬ë³„ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
        all_articles = []
        articles_per_interest = count // len(interests) if interests else count
        
        for interest in interests:
            articles = await self.search_all_sources(
                keywords=[interest],
                since=datetime.now() - timedelta(hours=24),
                count_per_source=articles_per_interest
            )
            
            # ê´€ì‹¬ì‚¬ ê°€ì¤‘ì¹˜ ì ìš©
            for article in articles:
                if article.relevance_score:
                    article.relevance_score *= interest_weights[interest]
            
            all_articles.extend(articles)
        
        # ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        unique_articles = self._deduplicate_and_sort(all_articles)
        
        return unique_articles[:count]
    
    def _update_collection_stats(self, results: Dict[str, List[NewsArticle]]):
        """ìˆ˜ì§‘ í†µê³„ ì—…ë°ì´íŠ¸"""
        total_articles = sum(len(articles) for articles in results.values())
        self.collection_stats["total_articles"] = total_articles
        self.collection_stats["last_collection"] = datetime.now().isoformat()
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        for category, articles in results.items():
            self.collection_stats["by_category"][category] = len(articles)
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        source_counts = {}
        sentiment_counts = {"positive": 0, "negative": 0, "neutral": 0}
        
        for articles in results.values():
            for article in articles:
                source_counts[article.source] = source_counts.get(article.source, 0) + 1
                
                if article.sentiment_label:
                    if article.sentiment_label.value > 0:
                        sentiment_counts["positive"] += 1
                    elif article.sentiment_label.value < 0:
                        sentiment_counts["negative"] += 1
                    else:
                        sentiment_counts["neutral"] += 1
        
        self.collection_stats["by_source"] = source_counts
        self.collection_stats["sentiment_distribution"] = sentiment_counts
    
    async def get_system_health(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        health_data = {
            "status": "healthy",
            "collectors": {},
            "total_collectors": len(self.collectors),
            "active_collectors": 0,
            "last_collection": self.collection_stats.get("last_collection"),
            "collection_stats": self.collection_stats.copy()
        }
        
        # ê° ìˆ˜ì§‘ê¸° ìƒíƒœ í™•ì¸
        for name, collector in self.collectors.items():
            try:
                collector_health = await collector.health_check()
                health_data["collectors"][name] = collector_health
                
                if collector_health.get("status") == "healthy":
                    health_data["active_collectors"] += 1
                    
            except Exception as e:
                health_data["collectors"][name] = {
                    "status": "error",
                    "error": str(e)
                }
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        if health_data["active_collectors"] == 0:
            health_data["status"] = "critical"
        elif health_data["active_collectors"] < health_data["total_collectors"] / 2:
            health_data["status"] = "degraded"
        
        return health_data
    
    async def export_news_data(self, format: str = "json", 
                              since: Optional[datetime] = None) -> str:
        """ë‰´ìŠ¤ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        if since is None:
            since = datetime.now() - timedelta(hours=24)
        
        # ìµœê·¼ ë‰´ìŠ¤ ìˆ˜ì§‘
        all_news = await self.collect_comprehensive_news(hours_back=24)
        
        if format.lower() == "json":
            import json
            return json.dumps({
                "export_time": datetime.now().isoformat(),
                "since": since.isoformat(),
                "total_articles": sum(len(articles) for articles in all_news.values()),
                "categories": {
                    category: [article.to_dict() for article in articles]
                    for category, articles in all_news.items()
                }
            }, indent=2, default=str)
        else:
            raise ValueError(f"Unsupported format: {format}")
    
    async def get_bitcoin_futures_impact_news(self, hours_back: int = 6, 
                                            min_impact_score: float = 0.6) -> Dict[str, Any]:
        """ë¹„íŠ¸ì½”ì¸ ì„ ë¬¼ì‹œì¥ ì˜í–¥ ë‰´ìŠ¤ ë¶„ì„"""
        self.logger.info(f"ğŸ¯ Bitcoin futures impact analysis (last {hours_back} hours, min score: {min_impact_score})")
        
        # 1. í¬ê´„ì  ë‰´ìŠ¤ ìˆ˜ì§‘
        all_categories = [
            NewsCategory.CRYPTO, NewsCategory.FINANCE, 
            NewsCategory.MACRO, NewsCategory.HEADLINE, 
            NewsCategory.BREAKING
        ]
        
        comprehensive_news = await self.collect_comprehensive_news(
            categories=all_categories,
            hours_back=hours_back,
            articles_per_category=20
        )
        
        # 2. ëª¨ë“  ê¸°ì‚¬ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©
        all_articles = []
        for articles_list in comprehensive_news.values():
            all_articles.extend(articles_list)
        
        # 3. ë¹„íŠ¸ì½”ì¸ ì„ ë¬¼ì‹œì¥ íŠ¹í™” í•„í„°ë§
        market_analysis = self.btc_futures_filter.get_market_moving_news(all_articles)
        
        # 4. ê³ ì˜í–¥ ê¸°ì‚¬ë§Œ ì¶”ì¶œ
        high_impact_articles = [
            article_data for article_data in market_analysis['top_articles']
            if article_data['composite_score'] >= min_impact_score
        ]
        
        # 5. í†µê³„ ì—…ë°ì´íŠ¸
        self.collection_stats["high_impact_articles"] = len(high_impact_articles)
        self.collection_stats["market_moving_events"] = market_analysis['event_type_distribution']
        
        return {
            "analysis_time": datetime.now().isoformat(),
            "period_hours": hours_back,
            "min_impact_score": min_impact_score,
            "total_articles_analyzed": len(all_articles),
            "high_impact_count": len(high_impact_articles),
            "market_sentiment": market_analysis['market_sentiment'],
            "impact_distribution": market_analysis['impact_distribution'],
            "event_type_distribution": market_analysis['event_type_distribution'],
            "high_impact_articles": high_impact_articles,
            "trading_signals": self._generate_trading_signals(market_analysis),
            "summary": self._generate_market_summary(market_analysis, high_impact_articles)
        }
    
    def _generate_trading_signals(self, market_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹œì¥ ë¶„ì„ ê¸°ë°˜ ê±°ë˜ ì‹ í˜¸ ìƒì„±"""
        market_sentiment = market_analysis['market_sentiment']
        impact_dist = market_analysis['impact_distribution']
        
        # ê¸°ë³¸ ì‹ í˜¸ ê³„ì‚°
        sentiment_score = market_sentiment['score']
        
        # ê¸´ê¸‰ì„± ê°€ì¤‘ì¹˜ ê³„ì‚°
        urgency_weight = 0.0
        if 'critical' in impact_dist and impact_dist['critical']:
            urgency_weight += len(impact_dist['critical']) * 0.4
        if 'high' in impact_dist and impact_dist['high']:
            urgency_weight += len(impact_dist['high']) * 0.3
        
        # ìµœì¢… ì‹ í˜¸ ì ìˆ˜ (-1.0 to +1.0)
        signal_strength = max(-1.0, min(1.0, sentiment_score + urgency_weight * 0.2))
        
        # ì‹ í˜¸ ë¶„ë¥˜
        if signal_strength > 0.6:
            signal = "STRONG_BUY"
        elif signal_strength > 0.3:
            signal = "BUY"
        elif signal_strength > -0.3:
            signal = "NEUTRAL"
        elif signal_strength > -0.6:
            signal = "SELL"
        else:
            signal = "STRONG_SELL"
        
        # ì‹œê°„ ë¯¼ê°ì„± (ë¶„ ë‹¨ìœ„)
        time_sensitivity = 240  # ê¸°ë³¸ 4ì‹œê°„
        if 'critical' in impact_dist and impact_dist['critical']:
            time_sensitivity = min([article['time_sensitivity'] for article in impact_dist['critical']])
        elif 'high' in impact_dist and impact_dist['high']:
            time_sensitivity = min([article['time_sensitivity'] for article in impact_dist['high']])
        
        return {
            "signal": signal,
            "strength": abs(signal_strength),
            "direction": "LONG" if signal_strength > 0 else "SHORT" if signal_strength < 0 else "FLAT",
            "confidence": min(1.0, abs(signal_strength) * 1.2),
            "time_sensitivity_minutes": time_sensitivity,
            "recommended_position_size": self._calculate_position_size(signal_strength),
            "risk_level": self._assess_risk_level(market_analysis),
            "validity_minutes": time_sensitivity * 2  # ì‹ í˜¸ ìœ íš¨ì„±
        }
    
    def _calculate_position_size(self, signal_strength: float) -> float:
        """ì‹ í˜¸ ê°•ë„ ê¸°ë°˜ í¬ì§€ì…˜ í¬ê¸° ê³„ì‚° (0.0-1.0)"""
        base_size = abs(signal_strength) * 0.5  # ìµœëŒ€ 50% í¬ì§€ì…˜
        
        # ë³´ìˆ˜ì  ì ‘ê·¼
        if abs(signal_strength) > 0.8:
            return min(0.3, base_size)  # ê°•í•œ ì‹ í˜¸ë„ 30% ì œí•œ
        elif abs(signal_strength) > 0.5:
            return min(0.2, base_size)  # ì¤‘ê°„ ì‹ í˜¸ëŠ” 20% ì œí•œ
        else:
            return min(0.1, base_size)  # ì•½í•œ ì‹ í˜¸ëŠ” 10% ì œí•œ
    
    def _assess_risk_level(self, market_analysis: Dict[str, Any]) -> str:
        """ìœ„í—˜ ìˆ˜ì¤€ í‰ê°€"""
        impact_dist = market_analysis['impact_distribution']
        event_dist = market_analysis['event_type_distribution']
        
        # ìœ„í—˜ ì ìˆ˜ ê³„ì‚°
        risk_score = 0.0
        
        # ì„íŒ©íŠ¸ ê¸°ë°˜ ìœ„í—˜ë„
        if 'critical' in impact_dist:
            risk_score += len(impact_dist['critical']) * 0.4
        if 'high' in impact_dist:
            risk_score += len(impact_dist['high']) * 0.3
        
        # ì´ë²¤íŠ¸ ìœ í˜• ê¸°ë°˜ ìœ„í—˜ë„
        high_risk_events = ['regulatory', 'monetary_policy', 'geopolitical']
        for event_type in high_risk_events:
            if event_type in event_dist:
                risk_score += event_dist[event_type] * 0.2
        
        # ìœ„í—˜ ìˆ˜ì¤€ ë¶„ë¥˜
        if risk_score > 2.0:
            return "VERY_HIGH"
        elif risk_score > 1.5:
            return "HIGH"
        elif risk_score > 1.0:
            return "MEDIUM"
        elif risk_score > 0.5:
            return "LOW"
        else:
            return "VERY_LOW"
    
    def _generate_market_summary(self, market_analysis: Dict[str, Any], 
                               high_impact_articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì‹œì¥ ìš”ì•½ ìƒì„±"""
        
        # ì£¼ìš” í…Œë§ˆ ì¶”ì¶œ
        themes = {}
        for article in high_impact_articles:
            event_type = article['event_type']
            if event_type not in themes:
                themes[event_type] = []
            themes[event_type].append(article['title'])
        
        # ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” í…Œë§ˆ
        dominant_theme = max(themes.keys(), key=lambda k: len(themes[k])) if themes else "none"
        
        # ì‹œê°„ ë¯¼ê°ë„ ë¶„ì„
        urgent_articles = [
            article for article in high_impact_articles 
            if article['time_sensitivity_minutes'] <= 60
        ]
        
        return {
            "dominant_theme": dominant_theme,
            "theme_distribution": {theme: len(articles) for theme, articles in themes.items()},
            "urgent_news_count": len(urgent_articles),
            "market_sentiment_label": market_analysis['market_sentiment']['label'],
            "key_concerns": [
                article['title'] for article in high_impact_articles[:3]
            ],
            "recommendation": self._generate_recommendation(market_analysis, high_impact_articles)
        }
    
    def _generate_recommendation(self, market_analysis: Dict[str, Any], 
                               high_impact_articles: List[Dict[str, Any]]) -> str:
        """ì¢…í•© ì¶”ì²œì‚¬í•­ ìƒì„±"""
        sentiment = market_analysis['market_sentiment']['label']
        urgent_count = len([a for a in high_impact_articles if a['time_sensitivity_minutes'] <= 60])
        total_impact = len(high_impact_articles)
        
        if urgent_count > 3:
            return f"âš ï¸ ê¸´ê¸‰ ë‰´ìŠ¤ {urgent_count}ê°œ ë°œìƒ. ì¦‰ì‹œ í¬ì§€ì…˜ ê²€í†  í•„ìš”"
        elif total_impact > 5 and sentiment == "bearish":
            return f"ğŸ“‰ {total_impact}ê°œ ë¶€ì •ì  ê³ ì˜í–¥ ë‰´ìŠ¤. ë³´ìˆ˜ì  ì ‘ê·¼ ê¶Œì¥"
        elif total_impact > 5 and sentiment == "bullish":
            return f"ğŸ“ˆ {total_impact}ê°œ ê¸ì •ì  ê³ ì˜í–¥ ë‰´ìŠ¤. ì ê·¹ì  í¬ì§€ì…˜ ê³ ë ¤"
        elif sentiment == "neutral":
            return f"âš–ï¸ ì¤‘ë¦½ì  ì‹œì¥ ê°ì •. ê´€ë§ ë˜ëŠ” ì†ŒëŸ‰ í¬ì§€ì…˜ ê¶Œì¥"
        else:
            return f"ğŸ“Š ì¼ë°˜ì ì¸ ì‹œì¥ ìƒí™©. ì •ìƒì ì¸ ê±°ë˜ ì „ëµ ìœ ì§€"
    
    async def get_real_time_market_alerts(self, alert_threshold: float = 0.8) -> List[Dict[str, Any]]:
        """ì‹¤ì‹œê°„ ì‹œì¥ ì•Œë¦¼ ìƒì„±"""
        
        # ìµœê·¼ 30ë¶„ ë‰´ìŠ¤ ìˆ˜ì§‘
        recent_news = await self.get_breaking_news_all(minutes=30)
        
        if not recent_news:
            return []
        
        # ê³ ì˜í–¥ ê¸°ì‚¬ í•„í„°ë§
        high_impact_articles = self.btc_futures_filter.filter_high_impact_articles(
            recent_news, min_composite_score=alert_threshold
        )
        
        alerts = []
        for article, relevance in high_impact_articles:
            alert = {
                "timestamp": datetime.now().isoformat(),
                "alert_level": relevance.impact_level.value.upper(),
                "event_type": relevance.event_type.value,
                "title": article.title,
                "source": article.source,
                "composite_score": relevance.composite_score,
                "time_sensitive_minutes": relevance.time_sensitivity,
                "sentiment_impact": relevance.sentiment_multiplier,
                "recommended_action": self._get_alert_action(relevance),
                "article_url": article.url
            }
            alerts.append(alert)
        
        return alerts
    
    def _get_alert_action(self, relevance: MarketRelevanceScore) -> str:
        """ì•Œë¦¼ë³„ ê¶Œì¥ ì•¡ì…˜"""
        if relevance.impact_level.value == "critical":
            return "IMMEDIATE_REVIEW"
        elif relevance.impact_level.value == "high":
            return "MONITOR_CLOSELY"
        elif relevance.time_sensitivity <= 60:
            return "CHECK_POSITION"
        else:
            return "STAY_INFORMED"
    
    async def search_with_advanced_query(self, query: str, 
                                       hours_back: int = 24,
                                       min_score: float = 2.0,
                                       max_results: int = 50) -> Dict[str, Any]:
        """Feedly Power Search ìŠ¤íƒ€ì¼ ê³ ê¸‰ ê²€ìƒ‰"""
        self.logger.info(f"ğŸ” Advanced query search: '{query}' (last {hours_back}h)")
        
        # 1. ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘
        all_categories = [
            NewsCategory.CRYPTO, NewsCategory.FINANCE, 
            NewsCategory.MACRO, NewsCategory.HEADLINE, 
            NewsCategory.BREAKING
        ]
        
        comprehensive_news = await self.collect_comprehensive_news(
            categories=all_categories,
            hours_back=hours_back,
            articles_per_category=30
        )
        
        # 2. ëª¨ë“  ê¸°ì‚¬ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©
        all_articles = []
        for articles_list in comprehensive_news.values():
            all_articles.extend(articles_list)
        
        # 3. ê³ ê¸‰ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        matched_articles = self.query_engine.search_articles_with_advanced_query(
            articles=all_articles,
            query=query,
            min_score=min_score
        )
        
        # 4. ê²°ê³¼ ì œí•œ
        limited_results = matched_articles[:max_results]
        
        # 5. ê²°ê³¼ ë¶„ì„
        if limited_results:
            avg_score = sum(score for _, score in limited_results) / len(limited_results)
            top_score = limited_results[0][1]
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
            category_dist = {}
            for article, score in limited_results:
                cat = article.category.value
                if cat not in category_dist:
                    category_dist[cat] = []
                category_dist[cat].append({"title": article.title, "score": score})
            
            # ì†ŒìŠ¤ë³„ ë¶„í¬
            source_dist = {}
            for article, score in limited_results:
                source = article.source
                if source not in source_dist:
                    source_dist[source] = 0
                source_dist[source] += 1
        else:
            avg_score = 0.0
            top_score = 0.0
            category_dist = {}
            source_dist = {}
        
        return {
            "query": query,
            "search_time": datetime.now().isoformat(),
            "period_hours": hours_back,
            "min_score": min_score,
            "total_articles_searched": len(all_articles),
            "matched_articles": len(matched_articles),
            "returned_results": len(limited_results),
            "avg_match_score": avg_score,
            "top_match_score": top_score,
            "category_distribution": category_dist,
            "source_distribution": source_dist,
            "results": [
                {
                    "title": article.title,
                    "source": article.source,
                    "url": article.url,
                    "published": article.published_date.isoformat(),
                    "category": article.category.value,
                    "match_score": score,
                    "sentiment_score": article.sentiment_score,
                    "summary": article.summary[:200] + "..." if len(article.summary) > 200 else article.summary
                }
                for article, score in limited_results
            ]
        }
    
    async def get_smart_market_queries(self, market_condition: str = "normal") -> List[Dict[str, Any]]:
        """ì‹œì¥ ìƒí™©ë³„ ìŠ¤ë§ˆíŠ¸ ì¿¼ë¦¬ ì¶”ì²œ"""
        self.logger.info(f"ğŸ“Š Generating smart queries for market condition: {market_condition}")
        
        # ì‹œì¥ ìƒí™©ë³„ ì¿¼ë¦¬ ìƒì„±
        if market_condition.lower() == "volatile":
            categories = [
                MarketImpactCategory.PRICE_MOVEMENT,
                MarketImpactCategory.MACRO_ECONOMIC,
                MarketImpactCategory.REGULATORY_POLICY
            ]
        elif market_condition.lower() == "bullish":
            categories = [
                MarketImpactCategory.INSTITUTIONAL_FLOW,
                MarketImpactCategory.REGULATORY_POLICY,
                MarketImpactCategory.CELEBRITY_INFLUENCE
            ]
        elif market_condition.lower() == "bearish":
            categories = [
                MarketImpactCategory.REGULATORY_POLICY,
                MarketImpactCategory.TECHNOLOGY_SECURITY,
                MarketImpactCategory.GEOPOLITICAL
            ]
        else:  # normal
            categories = [
                MarketImpactCategory.INSTITUTIONAL_FLOW,
                MarketImpactCategory.MACRO_ECONOMIC,
                MarketImpactCategory.PRICE_MOVEMENT
            ]
        
        smart_queries = []
        
        for category in categories:
            category_queries = self.query_engine.build_smart_queries_for_category(
                category=category,
                impact_level="HIGH"
            )
            
            for query in category_queries:
                smart_queries.append({
                    "query": query.raw_query,
                    "category": category.value,
                    "estimated_impact": query.estimated_impact,
                    "priority_score": query.priority_score,
                    "description": self._get_query_description(query, category)
                })
        
        # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        smart_queries.sort(key=lambda x: x["priority_score"], reverse=True)
        
        return smart_queries[:10]  # ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
    
    def _get_query_description(self, query: SearchQuery, category: MarketImpactCategory) -> str:
        """ì¿¼ë¦¬ ì„¤ëª… ìƒì„±"""
        descriptions = {
            MarketImpactCategory.REGULATORY_POLICY: "ê·œì œ ê¸°ê´€ ë° ì •ì±… ë³€í™” ëª¨ë‹ˆí„°ë§",
            MarketImpactCategory.INSTITUTIONAL_FLOW: "ê¸°ê´€ íˆ¬ìì ë™í–¥ ë° ìê¸ˆ íë¦„ ì¶”ì ",
            MarketImpactCategory.MACRO_ECONOMIC: "ê±°ì‹œê²½ì œ ì§€í‘œ ë° í†µí™”ì •ì±… ì˜í–¥ ë¶„ì„",
            MarketImpactCategory.PRICE_MOVEMENT: "ê°€ê²© ë³€ë™ ë° ì‹œì¥ ê°ì • ì¶”ì ",
            MarketImpactCategory.TECHNOLOGY_SECURITY: "ê¸°ìˆ ì  ë°œì „ ë° ë³´ì•ˆ ì´ìŠˆ ëª¨ë‹ˆí„°ë§",
            MarketImpactCategory.CELEBRITY_INFLUENCE: "ì£¼ìš” ì¸ë¬¼ ë°œì–¸ ë° ì˜í–¥ë ¥ ì¶”ì ",
            MarketImpactCategory.GEOPOLITICAL: "ì§€ì •í•™ì  ë¦¬ìŠ¤í¬ ë° êµ­ì œ ì •ì„¸ ëª¨ë‹ˆí„°ë§"
        }
        return descriptions.get(category, "ì¼ë°˜ì ì¸ ì‹œì¥ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§")
    
    async def execute_trending_queries(self, max_results_per_query: int = 10) -> Dict[str, Any]:
        """íŠ¸ë Œë”© ì¿¼ë¦¬ ì‹¤í–‰"""
        self.logger.info("ğŸ“ˆ Executing trending queries")
        
        # í˜„ì¬ ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë”© ì¿¼ë¦¬ ê°€ì ¸ì˜¤ê¸°
        trending_queries = self.query_engine.get_trending_queries(timeframe_hours=24)
        
        # ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ (í•œ ë²ˆë§Œ)
        all_categories = [
            NewsCategory.CRYPTO, NewsCategory.FINANCE, 
            NewsCategory.MACRO, NewsCategory.HEADLINE, 
            NewsCategory.BREAKING
        ]
        
        comprehensive_news = await self.collect_comprehensive_news(
            categories=all_categories,
            hours_back=12,
            articles_per_category=25
        )
        
        all_articles = []
        for articles_list in comprehensive_news.values():
            all_articles.extend(articles_list)
        
        # ê° íŠ¸ë Œë”© ì¿¼ë¦¬ ì‹¤í–‰
        trending_results = {}
        total_high_impact = 0
        
        for i, query in enumerate(trending_queries):
            matched_articles = self.query_engine.search_articles_with_advanced_query(
                articles=all_articles,
                query=query,
                min_score=2.5  # ë†’ì€ ì„ê³„ê°’
            )
            
            limited_results = matched_articles[:max_results_per_query]
            
            if limited_results:
                avg_score = sum(score for _, score in limited_results) / len(limited_results)
                high_impact_count = len([s for _, s in limited_results if s >= 4.0])
                total_high_impact += high_impact_count
                
                trending_results[f"query_{i}"] = {
                    "query": query.raw_query,
                    "category": query.categories[0].value if query.categories else "unknown",
                    "estimated_impact": query.estimated_impact,
                    "priority_score": query.priority_score,
                    "matched_count": len(matched_articles),
                    "returned_count": len(limited_results),
                    "avg_score": avg_score,
                    "high_impact_count": high_impact_count,
                    "top_articles": [
                        {
                            "title": article.title,
                            "source": article.source,
                            "match_score": score,
                            "published": article.published_date.isoformat()
                        }
                        for article, score in limited_results[:3]
                    ]
                }
        
        return {
            "execution_time": datetime.now().isoformat(),
            "total_articles_analyzed": len(all_articles),
            "trending_queries_count": len(trending_queries),
            "total_high_impact_articles": total_high_impact,
            "queries_with_results": len([r for r in trending_results.values() if r["matched_count"] > 0]),
            "market_alert_level": self._assess_market_alert_level(total_high_impact, len(trending_queries)),
            "trending_results": trending_results
        }
    
    def _assess_market_alert_level(self, high_impact_count: int, total_queries: int) -> str:
        """ì‹œì¥ ê²½ë³´ ìˆ˜ì¤€ í‰ê°€"""
        if total_queries == 0:
            return "UNKNOWN"
        
        impact_ratio = high_impact_count / total_queries
        
        if impact_ratio >= 0.8:
            return "CRITICAL"
        elif impact_ratio >= 0.6:
            return "HIGH"
        elif impact_ratio >= 0.4:
            return "MEDIUM"
        elif impact_ratio >= 0.2:
            return "LOW"
        else:
            return "MINIMAL"


# ì‚¬ìš© ì˜ˆì œ
async def main():
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    aggregator = AuroraQNewsAggregator()
    
    try:
        # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        print("ğŸ¥ Checking system health...")
        health = await aggregator.get_system_health()
        print(f"Status: {health['status']}")
        print(f"Active collectors: {health['active_collectors']}/{health['total_collectors']}")
        
        # í¬ê´„ì  ë‰´ìŠ¤ ìˆ˜ì§‘
        print("\nğŸ“° Collecting comprehensive news...")
        news_data = await aggregator.collect_comprehensive_news(
            categories=[NewsCategory.CRYPTO, NewsCategory.FINANCE],
            hours_back=6,
            articles_per_category=10
        )
        
        for category, articles in news_data.items():
            print(f"\n{category.upper()}: {len(articles)} articles")
            for article in articles[:3]:
                print(f"  - {article.title[:60]}...")
                print(f"    Source: {article.source}, Sentiment: {article.sentiment_label.name if article.sentiment_label else 'N/A'}")
        
        # ì‹œì¥ ì˜í–¥ ë‰´ìŠ¤
        print("\nğŸ“ˆ Market-moving news analysis...")
        market_news = await aggregator.get_market_moving_news(minutes=60)
        print(f"Market sentiment: {market_news['market_sentiment']['label']}")
        print(f"High impact articles: {market_news['high_impact_count']}")
        
        # ê°œì¸í™” í”¼ë“œ
        print("\nğŸ¯ Personalized feed...")
        feed = await aggregator.get_personalized_feed(
            interests=["bitcoin", "federal reserve"],
            count=10
        )
        print(f"Personalized feed: {len(feed)} articles")
        
    finally:
        await aggregator.close_all()


if __name__ == "__main__":
    asyncio.run(main())