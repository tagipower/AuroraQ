#!/usr/bin/env python3
"""
Google News RSS Collector
ë¬´ë£Œë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ Google News RSS í”¼ë“œ ìˆ˜ì§‘ê¸°
"""

import asyncio
import aiohttp
import feedparser
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
import hashlib
from urllib.parse import quote_plus
import logging

from ..base_collector import (
    BaseNewsCollector, NewsArticle, NewsCategory, 
    CollectorConfig, SentimentScore
)


class GoogleNewsCollector(BaseNewsCollector):
    """Google News RSS í”¼ë“œ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, config: Optional[CollectorConfig] = None):
        if not config:
            config = CollectorConfig(
                rate_limit=1000,  # RSSëŠ” ì œí•œì´ ëŠìŠ¨í•¨
                timeout=30.0,
                cache_ttl=600  # 10ë¶„ ìºì‹œ
            )
        super().__init__(config)
        
        self.base_url = "https://news.google.com/rss"
        self.session: Optional[aiohttp.ClientSession] = None
        
        # ì§€ì› ì–¸ì–´ ë° ì§€ì—­
        self.language = "en"
        self.country = "US"
        
        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        self.category_mapping = {
            NewsCategory.HEADLINE: "NATION",
            NewsCategory.FINANCE: "BUSINESS", 
            NewsCategory.CRYPTO: "TECHNOLOGY",
            NewsCategory.BREAKING: "NATION"
        }
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """HTTP ì„¸ì…˜ ê´€ë¦¬"""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=self.config.timeout)
            self.session = aiohttp.ClientSession(timeout=timeout)
        return self.session
    
    async def _fetch_rss(self, url: str) -> Optional[str]:
        """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            session = await self._get_session()
            async with session.get(url) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    self.logger.error(f"RSS fetch failed: {response.status}")
                    return None
        except Exception as e:
            self.logger.error(f"Error fetching RSS: {e}")
            self.stats["errors"] += 1
            return None
    
    def _parse_google_news_item(self, item: Dict[str, Any]) -> Optional[NewsArticle]:
        """Google News RSS ì•„ì´í…œ íŒŒì‹±"""
        try:
            # ì œëª©ì—ì„œ ì†ŒìŠ¤ ë¶„ë¦¬
            title_parts = item.get('title', '').split(' - ')
            if len(title_parts) >= 2:
                title = ' - '.join(title_parts[:-1])
                source = title_parts[-1]
            else:
                title = item.get('title', '')
                source = 'Google News'
            
            # ë°œí–‰ ì‹œê°„ íŒŒì‹±
            pub_date_str = item.get('published', '')
            try:
                pub_date = datetime.strptime(pub_date_str, '%a, %d %b %Y %H:%M:%S %Z')
            except:
                pub_date = datetime.now()
            
            # ID ìƒì„± (URL í•´ì‹œ)
            url = item.get('link', '')
            article_id = hashlib.md5(url.encode()).hexdigest()
            
            # ìš”ì•½ ì •ë¦¬
            summary = item.get('description', '')
            if summary.startswith('<'):
                # HTML íƒœê·¸ ì œê±° (ê°„ë‹¨í•œ ë°©ë²•)
                import re
                summary = re.sub('<[^<]+?>', '', summary)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ì œëª©ê³¼ ìš”ì•½ì—ì„œ)
            keywords = self._extract_keywords(title + " " + summary)
            
            return NewsArticle(
                id=article_id,
                title=title,
                content=summary,  # RSSëŠ” ì „ì²´ ë‚´ìš©ì„ ì œê³µí•˜ì§€ ì•ŠìŒ
                summary=summary[:500] if summary else "",
                url=url,
                source=source,
                author=None,
                published_date=pub_date,
                collected_date=datetime.now(),
                category=NewsCategory.HEADLINE,
                keywords=keywords,
                entities=self.extract_entities(title + " " + summary),
                metadata={"rss_source": "google_news"}
            )
            
        except Exception as e:
            self.logger.error(f"Error parsing Google News item: {e}")
            return None
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì¤‘ìš” ë‹¨ì–´ë“¤
        important_words = [
            "bitcoin", "ethereum", "crypto", "blockchain", "defi", "nft",
            "stock", "market", "economy", "inflation", "recession",
            "fed", "fomc", "cpi", "gdp", "rate", "dollar",
            "breaking", "urgent", "alert", "crash", "surge"
        ]
        
        text_lower = text.lower()
        keywords = []
        
        for word in important_words:
            if word in text_lower:
                keywords.append(word)
        
        return keywords
    
    async def collect_headlines(self, count: int = 20) -> List[NewsArticle]:
        """ì£¼ìš” í—¤ë“œë¼ì¸ ìˆ˜ì§‘"""
        url = f"{self.base_url}?hl={self.language}-{self.country}&gl={self.country}&ceid={self.country}:{self.language}"
        
        rss_content = await self._fetch_rss(url)
        if not rss_content:
            return []
        
        feed = feedparser.parse(rss_content)
        articles = []
        
        for item in feed.entries[:count]:
            article = self._parse_google_news_item(item)
            if article:
                # ê°ì • ë¶„ì„
                article = await self.analyze_sentiment(article)
                articles.append(article)
                self.stats["articles_collected"] += 1
        
        return articles
    
    async def search_news(self, keywords: List[str], 
                         since: Optional[datetime] = None,
                         until: Optional[datetime] = None,
                         count: int = 20) -> List[NewsArticle]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰"""
        # Google News RSS ê²€ìƒ‰ URL ìƒì„±
        query = " ".join(keywords)
        encoded_query = quote_plus(query)
        
        url = f"{self.base_url}/search?q={encoded_query}&hl={self.language}-{self.country}&gl={self.country}&ceid={self.country}:{self.language}"
        
        rss_content = await self._fetch_rss(url)
        if not rss_content:
            return []
        
        feed = feedparser.parse(rss_content)
        articles = []
        
        for item in feed.entries[:count]:
            article = self._parse_google_news_item(item)
            if article:
                # ì‹œê°„ í•„í„°ë§
                if since and article.published_date < since:
                    continue
                if until and article.published_date > until:
                    continue
                
                # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                article.relevance_score = self.calculate_relevance(article, keywords)
                
                # ê°ì • ë¶„ì„
                article = await self.analyze_sentiment(article)
                
                articles.append(article)
                self.stats["articles_collected"] += 1
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        articles.sort(key=lambda x: x.relevance_score or 0, reverse=True)
        
        return articles
    
    async def get_breaking_news(self, minutes: int = 30) -> List[NewsArticle]:
        """ì†ë³´ ìˆ˜ì§‘"""
        # Google NewsëŠ” ë³„ë„ì˜ ì†ë³´ í”¼ë“œê°€ ì—†ìœ¼ë¯€ë¡œ ìµœì‹  ë‰´ìŠ¤ì—ì„œ í•„í„°ë§
        headlines = await self.collect_headlines(count=50)
        
        cutoff_time = datetime.now() - timedelta(minutes=minutes)
        breaking_news = []
        
        for article in headlines:
            if article.published_date >= cutoff_time:
                # ì†ë³´ í‚¤ì›Œë“œ í™•ì¸
                breaking_keywords = ["breaking", "urgent", "alert", "just in", "update"]
                title_lower = article.title.lower()
                
                if any(kw in title_lower for kw in breaking_keywords):
                    article.category = NewsCategory.BREAKING
                    breaking_news.append(article)
        
        return breaking_news
    
    async def collect_by_category(self, category: NewsCategory, 
                                 count: int = 20) -> List[NewsArticle]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        # Google News ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        google_category = self.category_mapping.get(category, "NATION")
        
        url = f"{self.base_url}/headlines/section/topic/{google_category}?hl={self.language}-{self.country}&gl={self.country}&ceid={self.country}:{self.language}"
        
        rss_content = await self._fetch_rss(url)
        if not rss_content:
            return []
        
        feed = feedparser.parse(rss_content)
        articles = []
        
        for item in feed.entries[:count]:
            article = self._parse_google_news_item(item)
            if article:
                article.category = category
                article = await self.analyze_sentiment(article)
                articles.append(article)
                self.stats["articles_collected"] += 1
        
        return articles
    
    async def collect_crypto_news(self, count: int = 20) -> List[NewsArticle]:
        """ì•”í˜¸í™”í ê´€ë ¨ ë‰´ìŠ¤ íŠ¹í™” ìˆ˜ì§‘"""
        crypto_keywords = [
            "bitcoin", "ethereum", "cryptocurrency", "blockchain",
            "defi", "nft", "crypto market", "btc", "eth"
        ]
        
        articles = await self.search_news(crypto_keywords, count=count)
        
        # ì¹´í…Œê³ ë¦¬ ì„¤ì •
        for article in articles:
            article.category = NewsCategory.CRYPTO
        
        return articles
    
    async def collect_macro_news(self, count: int = 20) -> List[NewsArticle]:
        """ê±°ì‹œê²½ì œ ë‰´ìŠ¤ íŠ¹í™” ìˆ˜ì§‘"""
        macro_keywords = [
            "federal reserve", "fomc", "inflation", "cpi", "gdp",
            "interest rate", "central bank", "economic data", "job report"
        ]
        
        articles = await self.search_news(macro_keywords, count=count)
        
        # ì¹´í…Œê³ ë¦¬ ì„¤ì •
        for article in articles:
            article.category = NewsCategory.MACRO
        
        return articles
    
    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.session and not self.session.closed:
            await self.session.close()
        await super().close()


# ì‚¬ìš© ì˜ˆì œ
async def main():
    """Google News Collector í…ŒìŠ¤íŠ¸"""
    collector = GoogleNewsCollector()
    
    try:
        # í—¤ë“œë¼ì¸ ìˆ˜ì§‘
        print("ğŸ“° Collecting headlines...")
        headlines = await collector.collect_headlines(count=5)
        print(f"Found {len(headlines)} headlines")
        
        for article in headlines[:3]:
            print(f"\n- {article.title}")
            print(f"  Source: {article.source}")
            print(f"  Published: {article.published_date}")
            print(f"  Sentiment: {article.sentiment_label.name if article.sentiment_label else 'N/A'}")
        
        # ì•”í˜¸í™”í ë‰´ìŠ¤ ê²€ìƒ‰
        print("\n\nğŸ” Searching crypto news...")
        crypto_news = await collector.collect_crypto_news(count=5)
        print(f"Found {len(crypto_news)} crypto articles")
        
        for article in crypto_news[:3]:
            print(f"\n- {article.title}")
            print(f"  Relevance: {article.relevance_score:.2f}")
            print(f"  Keywords: {', '.join(article.keywords[:5])}")
        
        # ì†ë³´ í™•ì¸
        print("\n\nğŸš¨ Checking breaking news...")
        breaking = await collector.get_breaking_news(minutes=60)
        print(f"Found {len(breaking)} breaking news in last hour")
        
        # í†µê³„
        stats = collector.get_stats()
        print(f"\nğŸ“Š Stats: {stats}")
        
    finally:
        await collector.close()


if __name__ == "__main__":
    asyncio.run(main())