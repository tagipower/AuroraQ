#!/usr/bin/env python3
"""
Yahoo Finance News Collector
ë¬´ë£Œ Yahoo Finance RSS í”¼ë“œë¥¼ í†µí•œ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°
"""

import asyncio
import aiohttp
import feedparser
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
import hashlib
import json
import re
from urllib.parse import quote_plus

from ..base_collector import (
    BaseNewsCollector, NewsArticle, NewsCategory,
    CollectorConfig, SentimentScore
)


class YahooFinanceCollector(BaseNewsCollector):
    """Yahoo Finance ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, config: Optional[CollectorConfig] = None):
        if not config:
            config = CollectorConfig(
                rate_limit=500,  # Yahoo Finance RSSëŠ” ê´€ëŒ€í•¨
                timeout=30.0,
                cache_ttl=300  # 5ë¶„ ìºì‹œ
            )
        super().__init__(config)
        
        self.base_rss_url = "https://finance.yahoo.com/rss"
        self.session: Optional[aiohttp.ClientSession] = None
        
        # ì£¼ìš” RSS í”¼ë“œë“¤
        self.feeds = {
            "headlines": "https://finance.yahoo.com/news/rssindex",
            "crypto": "https://finance.yahoo.com/news/rss/category-cryptocurrency",
            "markets": "https://finance.yahoo.com/news/rss/category-stock-market-news",
            "economy": "https://finance.yahoo.com/news/rss/category-economy",
            "currencies": "https://finance.yahoo.com/news/rss/category-currencies",
            "commodities": "https://finance.yahoo.com/news/rss/category-commodities"
        }
        
        # í‹°ì»¤ ì‹¬ë³¼ ë§¤í•‘
        self.crypto_symbols = {
            "BTC-USD": "Bitcoin",
            "ETH-USD": "Ethereum",
            "BNB-USD": "Binance Coin",
            "SOL-USD": "Solana",
            "ADA-USD": "Cardano"
        }
        
        self.market_symbols = {
            "^GSPC": "S&P 500",
            "^DJI": "Dow Jones",
            "^IXIC": "NASDAQ",
            "^VIX": "VIX",
            "GC=F": "Gold",
            "CL=F": "Oil"
        }
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """HTTP ì„¸ì…˜ ê´€ë¦¬"""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=self.config.timeout)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            self.session = aiohttp.ClientSession(timeout=timeout, headers=headers)
        return self.session
    
    async def _fetch_rss(self, url: str) -> Optional[str]:
        """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            session = await self._get_session()
            async with session.get(url) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    self.logger.error(f"RSS fetch failed: {response.status}")
                    return None
        except Exception as e:
            self.logger.error(f"Error fetching RSS: {e}")
            self.stats["errors"] += 1
            return None
    
    def _parse_yahoo_item(self, item: Dict[str, Any], 
                         category: NewsCategory = NewsCategory.FINANCE) -> Optional[NewsArticle]:
        """Yahoo Finance RSS ì•„ì´í…œ íŒŒì‹±"""
        try:
            # ì•„ì´í…œì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
            if not isinstance(item, dict):
                self.logger.warning(f"Yahoo RSS item is not a dict: {type(item)}")
                return None
            
            title = item.get('title', '')
            url = item.get('link', '')
            
            if not title or not url:
                return None
            
            # ID ìƒì„±
            article_id = hashlib.md5(url.encode()).hexdigest()
            
            # ë°œí–‰ ì‹œê°„ íŒŒì‹±
            pub_date_str = item.get('published', '')
            try:
                pub_date = datetime.strptime(pub_date_str, '%a, %d %b %Y %H:%M:%S %z')
                # timezone ì œê±° (naive datetimeìœ¼ë¡œ ë³€í™˜)
                pub_date = pub_date.replace(tzinfo=None)
            except:
                pub_date = datetime.now()
            
            # ìš”ì•½ ì¶”ì¶œ
            description = item.get('description', '')
            summary = self._clean_html(description)[:500] if description else ''
            
            # ì†ŒìŠ¤ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
            source = 'Yahoo Finance'
            if 'source' in item:
                source_data = item['source']
                if isinstance(source_data, dict):
                    source = source_data.get('content', 'Yahoo Finance')
                elif isinstance(source_data, str):
                    source = source_data
            
            # í‚¤ì›Œë“œ ë° ì—”í‹°í‹° ì¶”ì¶œ
            text_content = title + " " + summary
            keywords = self._extract_finance_keywords(text_content)
            entities = self._extract_tickers(text_content)
            
            # GUID ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
            guid = ''
            if 'guid' in item:
                guid_data = item['guid']
                if isinstance(guid_data, dict):
                    guid = guid_data.get('content', '')
                elif isinstance(guid_data, str):
                    guid = guid_data
            
            return NewsArticle(
                id=article_id,
                title=title,
                content=summary,  # RSSëŠ” ì „ì²´ ë‚´ìš© ë¯¸ì œê³µ
                summary=summary,
                url=url,
                source=source,
                author=item.get('author'),
                published_date=pub_date,
                collected_date=datetime.now(),
                category=category,
                keywords=keywords,
                entities=entities,
                metadata={
                    "rss_source": "yahoo_finance",
                    "guid": guid
                }
            )
            
        except Exception as e:
            self.logger.error(f"Error parsing Yahoo item: {e}")
            return None
    
    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        # ê°„ë‹¨í•œ HTML íƒœê·¸ ì œê±°
        text = re.sub('<[^<]+?>', '', text)
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def _extract_finance_keywords(self, text: str) -> List[str]:
        """ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        text_lower = text.lower()
        
        # ê¸ˆìœµ í‚¤ì›Œë“œ
        finance_terms = [
            "stock", "market", "trading", "investor", "earnings",
            "revenue", "profit", "loss", "growth", "recession",
            "inflation", "rate", "bond", "yield", "volatility",
            "bull", "bear", "rally", "crash", "correction"
        ]
        
        # ì•”í˜¸í™”í í‚¤ì›Œë“œ
        crypto_terms = [
            "bitcoin", "ethereum", "crypto", "blockchain", "defi",
            "nft", "altcoin", "mining", "wallet", "exchange"
        ]
        
        # ê²½ì œ ì§€í‘œ
        economic_terms = [
            "gdp", "cpi", "unemployment", "fomc", "fed", "ecb",
            "inflation", "deflation", "stimulus", "tapering"
        ]
        
        all_terms = finance_terms + crypto_terms + economic_terms
        
        for term in all_terms:
            if term in text_lower:
                keywords.append(term)
        
        return list(set(keywords))
    
    def _extract_tickers(self, text: str) -> List[str]:
        """ì£¼ì‹/ì•”í˜¸í™”í í‹°ì»¤ ì¶”ì¶œ"""
        tickers = []
        
        # ì¼ë°˜ì ì¸ í‹°ì»¤ íŒ¨í„´ (ëŒ€ë¬¸ì 2-5ì)
        ticker_pattern = r'\b[A-Z]{2,5}\b'
        potential_tickers = re.findall(ticker_pattern, text)
        
        # ì•Œë ¤ì§„ í‹°ì»¤ í™•ì¸
        known_tickers = list(self.crypto_symbols.keys()) + list(self.market_symbols.keys())
        
        for ticker in potential_tickers:
            # ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸
            common_words = ["CEO", "CFO", "IPO", "ETF", "NYSE", "NASDAQ", "USD", "EUR"]
            if ticker not in common_words and len(ticker) >= 2:
                tickers.append(ticker)
        
        # ì•”í˜¸í™”í ì´ë¦„ë„ ì¶”ê°€
        text_lower = text.lower()
        for symbol, name in self.crypto_symbols.items():
            if name.lower() in text_lower:
                tickers.append(symbol)
        
        return list(set(tickers))
    
    async def collect_headlines(self, count: int = 20) -> List[NewsArticle]:
        """ì£¼ìš” ê¸ˆìœµ í—¤ë“œë¼ì¸ ìˆ˜ì§‘"""
        url = self.feeds["headlines"]
        
        rss_content = await self._fetch_rss(url)
        if not rss_content:
            return []
        
        feed = feedparser.parse(rss_content)
        articles = []
        
        for item in feed.entries[:count]:
            article = self._parse_yahoo_item(item, NewsCategory.HEADLINE)
            if article:
                article = await self.analyze_sentiment(article)
                articles.append(article)
                self.stats["articles_collected"] += 1
        
        return articles
    
    async def search_news(self, keywords: List[str],
                         since: Optional[datetime] = None,
                         until: Optional[datetime] = None,
                         count: int = 20) -> List[NewsArticle]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰"""
        # Yahoo FinanceëŠ” RSS ê²€ìƒ‰ì„ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ
        # ëŒ€ì‹  ì—¬ëŸ¬ í”¼ë“œì—ì„œ ìˆ˜ì§‘ í›„ í•„í„°ë§
        all_articles = []
        
        # ëª¨ë“  í”¼ë“œì—ì„œ ìˆ˜ì§‘
        for feed_name, feed_url in self.feeds.items():
            rss_content = await self._fetch_rss(feed_url)
            if not rss_content:
                continue
            
            feed = feedparser.parse(rss_content)
            
            for item in feed.entries:
                article = self._parse_yahoo_item(item)
                if article:
                    # í‚¤ì›Œë“œ ë§¤ì¹­
                    text = (article.title + " " + article.summary).lower()
                    if any(kw.lower() in text for kw in keywords):
                        # ì‹œê°„ í•„í„°ë§
                        if since and article.published_date < since:
                            continue
                        if until and article.published_date > until:
                            continue
                        
                        article.relevance_score = self.calculate_relevance(article, keywords)
                        all_articles.append(article)
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        unique_articles = {}
        for article in all_articles:
            if article.url not in unique_articles:
                unique_articles[article.url] = article
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        sorted_articles = sorted(
            unique_articles.values(),
            key=lambda x: x.relevance_score or 0,
            reverse=True
        )[:count]
        
        # ê°ì • ë¶„ì„
        for article in sorted_articles:
            article = await self.analyze_sentiment(article)
            self.stats["articles_collected"] += 1
        
        return sorted_articles
    
    async def get_breaking_news(self, minutes: int = 30) -> List[NewsArticle]:
        """ê¸ˆìœµ ì†ë³´ ìˆ˜ì§‘"""
        cutoff_time = datetime.now() - timedelta(minutes=minutes)
        breaking_news = []
        
        # ì£¼ìš” í”¼ë“œì—ì„œ ìµœì‹  ë‰´ìŠ¤ í™•ì¸
        for feed_name in ["headlines", "markets", "crypto"]:
            feed_url = self.feeds[feed_name]
            rss_content = await self._fetch_rss(feed_url)
            
            if not rss_content:
                continue
            
            feed = feedparser.parse(rss_content)
            
            for item in feed.entries[:10]:  # ê° í”¼ë“œì—ì„œ ìµœì‹  10ê°œë§Œ
                article = self._parse_yahoo_item(item, NewsCategory.BREAKING)
                
                if article and article.published_date >= cutoff_time:
                    # ì†ë³´ í‚¤ì›Œë“œ í™•ì¸
                    breaking_keywords = [
                        "breaking", "alert", "urgent", "flash",
                        "plunge", "surge", "crash", "spike", "halt"
                    ]
                    
                    text_lower = article.title.lower()
                    if any(kw in text_lower for kw in breaking_keywords):
                        article = await self.analyze_sentiment(article)
                        breaking_news.append(article)
        
        # ì‹œê°„ìˆœ ì •ë ¬
        breaking_news.sort(key=lambda x: x.published_date, reverse=True)
        
        return breaking_news
    
    async def collect_crypto_news(self, count: int = 20) -> List[NewsArticle]:
        """ì•”í˜¸í™”í ë‰´ìŠ¤ íŠ¹í™” ìˆ˜ì§‘"""
        url = self.feeds["crypto"]
        
        rss_content = await self._fetch_rss(url)
        if not rss_content:
            return []
        
        feed = feedparser.parse(rss_content)
        articles = []
        
        for item in feed.entries[:count]:
            article = self._parse_yahoo_item(item, NewsCategory.CRYPTO)
            if article:
                # ì•”í˜¸í™”í íŠ¹í™” ê°ì • ë¶„ì„
                article = await self.analyze_crypto_sentiment(article)
                articles.append(article)
                self.stats["articles_collected"] += 1
        
        return articles
    
    async def collect_market_news(self, count: int = 20) -> List[NewsArticle]:
        """ì£¼ì‹ ì‹œì¥ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        url = self.feeds["markets"]
        
        rss_content = await self._fetch_rss(url)
        if not rss_content:
            return []
        
        feed = feedparser.parse(rss_content)
        articles = []
        
        for item in feed.entries[:count]:
            article = self._parse_yahoo_item(item, NewsCategory.FINANCE)
            if article:
                article = await self.analyze_sentiment(article)
                articles.append(article)
                self.stats["articles_collected"] += 1
        
        return articles
    
    async def collect_economic_news(self, count: int = 20) -> List[NewsArticle]:
        """ê²½ì œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        url = self.feeds["economy"]
        
        rss_content = await self._fetch_rss(url)
        if not rss_content:
            return []
        
        feed = feedparser.parse(rss_content)
        articles = []
        
        for item in feed.entries[:count]:
            article = self._parse_yahoo_item(item, NewsCategory.MACRO)
            if article:
                article = await self.analyze_sentiment(article)
                articles.append(article)
                self.stats["articles_collected"] += 1
        
        return articles
    
    async def analyze_crypto_sentiment(self, article: NewsArticle) -> NewsArticle:
        """ì•”í˜¸í™”í íŠ¹í™” ê°ì • ë¶„ì„"""
        # ì•”í˜¸í™”í íŠ¹í™” ê°ì • í‚¤ì›Œë“œ
        crypto_positive = [
            "adoption", "institutional", "bullish", "ath", "moon",
            "accumulation", "breakout", "upgrade", "halving", "defi growth"
        ]
        
        crypto_negative = [
            "ban", "regulation", "bearish", "crash", "hack", "scam",
            "rug pull", "sec lawsuit", "crackdown", "bubble"
        ]
        
        text = (article.title + " " + article.summary).lower()
        
        positive_count = sum(1 for word in crypto_positive if word in text)
        negative_count = sum(1 for word in crypto_negative if word in text)
        
        # ê°€ê²© ë³€ë™ íŒ¨í„´ í™•ì¸
        price_patterns = {
            "surge": 0.3, "soar": 0.3, "rally": 0.2, "jump": 0.2,
            "plunge": -0.3, "crash": -0.4, "tumble": -0.3, "drop": -0.2
        }
        
        sentiment_adjustment = 0
        for pattern, weight in price_patterns.items():
            if pattern in text:
                sentiment_adjustment += weight
        
        # ìµœì¢… ê°ì • ì ìˆ˜ ê³„ì‚°
        base_score = (positive_count - negative_count) / max(1, positive_count + negative_count)
        final_score = max(-1, min(1, base_score + sentiment_adjustment))
        
        article.sentiment_score = final_score
        
        if final_score > 0.3:
            article.sentiment_label = SentimentScore.POSITIVE
        elif final_score < -0.3:
            article.sentiment_label = SentimentScore.NEGATIVE
        else:
            article.sentiment_label = SentimentScore.NEUTRAL
        
        return article
    
    async def get_ticker_news(self, ticker: str, count: int = 10) -> List[NewsArticle]:
        """íŠ¹ì • í‹°ì»¤ ê´€ë ¨ ë‰´ìŠ¤"""
        # Yahoo FinanceëŠ” í‹°ì»¤ë³„ RSSë„ ì œê³µ
        ticker_url = f"https://finance.yahoo.com/rss/headline?s={ticker}"
        
        rss_content = await self._fetch_rss(ticker_url)
        if not rss_content:
            return []
        
        feed = feedparser.parse(rss_content)
        articles = []
        
        for item in feed.entries[:count]:
            article = self._parse_yahoo_item(item)
            if article:
                article.metadata["ticker"] = ticker
                article = await self.analyze_sentiment(article)
                articles.append(article)
        
        return articles
    
    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.session and not self.session.closed:
            await self.session.close()
        await super().close()


# ì‚¬ìš© ì˜ˆì œ
async def main():
    """Yahoo Finance Collector í…ŒìŠ¤íŠ¸"""
    collector = YahooFinanceCollector()
    
    try:
        # ê¸ˆìœµ í—¤ë“œë¼ì¸
        print("ğŸ“ˆ Collecting financial headlines...")
        headlines = await collector.collect_headlines(count=5)
        print(f"Found {len(headlines)} headlines")
        
        for article in headlines[:3]:
            print(f"\n- {article.title}")
            print(f"  Source: {article.source}")
            print(f"  Keywords: {', '.join(article.keywords[:5])}")
        
        # ì•”í˜¸í™”í ë‰´ìŠ¤
        print("\n\nğŸª™ Collecting crypto news...")
        crypto_news = await collector.collect_crypto_news(count=5)
        print(f"Found {len(crypto_news)} crypto articles")
        
        for article in crypto_news[:3]:
            print(f"\n- {article.title}")
            print(f"  Sentiment: {article.sentiment_label.name if article.sentiment_label else 'N/A'}")
            print(f"  Score: {article.sentiment_score:.2f}" if article.sentiment_score else "")
        
        # BTC ë‰´ìŠ¤
        print("\n\nâ‚¿ Getting Bitcoin news...")
        btc_news = await collector.get_ticker_news("BTC-USD", count=3)
        print(f"Found {len(btc_news)} BTC articles")
        
        for article in btc_news:
            print(f"\n- {article.title}")
            print(f"  Published: {article.published_date}")
        
    finally:
        await collector.close()


if __name__ == "__main__":
    asyncio.run(main())