#!/usr/bin/env python3
"""
NewsAPI Collector
NewsAPI.orgë¥¼ í†µí•œ ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°
"""

import asyncio
import aiohttp
import os
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
import hashlib
from urllib.parse import urlencode

from ..base_collector import (
    BaseNewsCollector, NewsArticle, NewsCategory,
    CollectorConfig, SentimentScore
)


class NewsAPICollector(BaseNewsCollector):
    """NewsAPI.org ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, config: Optional[CollectorConfig] = None):
        # API í‚¤ í™•ì¸
        api_key = config.api_key if config else os.getenv("NEWSAPI_KEY")
        
        if not api_key:
            self.logger.warning("NEWSAPI_KEY not found. Get free key at https://newsapi.org/")
        
        if not config:
            config = CollectorConfig(
                api_key=api_key,
                rate_limit=100,  # ë¬´ë£Œ í‹°ì–´ëŠ” ì‹œê°„ë‹¹ 100 ìš”ì²­
                timeout=30.0,
                cache_ttl=900  # 15ë¶„ ìºì‹œ (ë” ê¸¸ê²Œ)
            )
        else:
            config.api_key = api_key
            
        super().__init__(config)
        
        self.base_url = "https://newsapi.org/v2"
        self.session: Optional[aiohttp.ClientSession] = None
        
        # ì£¼ìš” ë‰´ìŠ¤ ì†ŒìŠ¤ë“¤
        self.crypto_sources = [
            "coindesk", "cointelegraph", "crypto-coins-news"
        ]
        
        self.finance_sources = [
            "bloomberg", "reuters", "cnbc", "financial-times",
            "the-wall-street-journal", "marketwatch"
        ]
        
        self.general_sources = [
            "bbc-news", "cnn", "reuters", "associated-press",
            "the-guardian-uk", "usa-today"
        ]
        
        # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ
        self.category_keywords = {
            NewsCategory.CRYPTO: [
                "bitcoin", "ethereum", "cryptocurrency", "blockchain",
                "defi", "nft", "crypto market", "digital currency"
            ],
            NewsCategory.FINANCE: [
                "stock market", "wall street", "nasdaq", "dow jones",
                "s&p 500", "earnings", "ipo", "merger"
            ],
            NewsCategory.MACRO: [
                "federal reserve", "inflation", "gdp", "unemployment",
                "interest rates", "economic policy", "recession"
            ],
            NewsCategory.PERSON: [
                "ceo", "president", "chairman", "executive",
                "jerome powell", "elon musk", "warren buffett"
            ]
        }
        
        # ì–¸ì–´ ë° êµ­ê°€ ì„¤ì •
        self.language = "en"
        self.country = "us"
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """HTTP ì„¸ì…˜ ê´€ë¦¬"""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=self.config.timeout)
            headers = {
                'X-API-Key': self.config.api_key,
                'User-Agent': 'AuroraQ/1.0'
            }
            self.session = aiohttp.ClientSession(timeout=timeout, headers=headers)
        return self.session
    
    async def _make_request(self, endpoint: str, params: Dict[str, Any] = None) -> Optional[Dict]:
        """NewsAPI ìš”ì²­"""
        if not self.config.api_key:
            self.logger.error("API key required for NewsAPI")
            return None
        
        try:
            session = await self._get_session()
            url = f"{self.base_url}{endpoint}"
            
            async with session.get(url, params=params) as response:
                self.stats["requests_made"] += 1
                
                if response.status == 200:
                    return await response.json()
                elif response.status == 429:
                    self.logger.error("Rate limit exceeded")
                    self.stats["errors"] += 1
                    return None
                elif response.status == 426:
                    self.logger.error("Upgrade required - free tier limitations")
                    self.stats["errors"] += 1
                    return None
                else:
                    self.logger.error(f"API request failed: {response.status}")
                    text = await response.text()
                    self.logger.error(f"Response: {text}")
                    self.stats["errors"] += 1
                    return None
                    
        except Exception as e:
            self.logger.error(f"Error making request: {e}")
            self.stats["errors"] += 1
            return None
    
    def _parse_newsapi_article(self, item: Dict[str, Any], 
                              category: NewsCategory = NewsCategory.HEADLINE) -> Optional[NewsArticle]:
        """NewsAPI ê¸°ì‚¬ íŒŒì‹±"""
        try:
            # ê¸°ë³¸ ì •ë³´
            title = item.get('title', '')
            description = item.get('description', '')
            content = item.get('content', description)  # contentëŠ” ë³´í†µ ì˜ë¦¼
            url = item.get('url', '')
            
            # ID ìƒì„±
            article_id = hashlib.md5(url.encode()).hexdigest()
            
            # ì‹œê°„ íŒŒì‹±
            published_at = item.get('publishedAt', '')
            try:
                published_date = datetime.strptime(published_at, '%Y-%m-%dT%H:%M:%SZ')
            except:
                published_date = datetime.now()
            
            # ì†ŒìŠ¤ ì •ë³´
            source_info = item.get('source', {})
            source_name = source_info.get('name', 'Unknown')
            
            # ì‘ì„±ì
            author = item.get('author', 'Unknown')
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            text_content = f"{title} {description} {content}"
            keywords = self._extract_newsapi_keywords(text_content)
            
            # ì¹´í…Œê³ ë¦¬ ìë™ ê²°ì • (í‚¤ì›Œë“œ ê¸°ë°˜)
            if category == NewsCategory.HEADLINE:
                category = self._determine_category_from_keywords(keywords, text_content)
            
            return NewsArticle(
                id=article_id,
                title=title,
                content=content or description,
                summary=description[:500] if description else "",
                url=url,
                source=source_name,
                author=author,
                published_date=published_date,
                collected_date=datetime.now(),
                category=category,
                keywords=keywords,
                entities=self.extract_entities(text_content),
                metadata={
                    "source_id": source_info.get('id', ''),
                    "url_to_image": item.get('urlToImage', ''),
                    "newsapi_category": category.value
                }
            )
            
        except Exception as e:
            self.logger.error(f"Error parsing NewsAPI article: {e}")
            return None
    
    def _extract_newsapi_keywords(self, text: str) -> List[str]:
        """NewsAPI ê¸°ì‚¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        text_lower = text.lower()
        
        # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ í‚¤ì›Œë“œ í™•ì¸
        for category, category_keywords in self.category_keywords.items():
            for keyword in category_keywords:
                if keyword.lower() in text_lower:
                    keywords.append(keyword)
        
        # ì¶”ê°€ ê¸ˆìœµ/ê²½ì œ í‚¤ì›Œë“œ
        additional_keywords = [
            "market", "trading", "investor", "price", "value",
            "revenue", "profit", "loss", "growth", "decline",
            "announcement", "report", "data", "forecast"
        ]
        
        for keyword in additional_keywords:
            if keyword in text_lower:
                keywords.append(keyword)
        
        return list(set(keywords))
    
    def _determine_category_from_keywords(self, keywords: List[str], text: str) -> NewsCategory:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ê²°ì •"""
        text_lower = text.lower()
        
        # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë¶„ë¥˜
        if any(kw in keywords for kw in self.category_keywords[NewsCategory.CRYPTO]):
            return NewsCategory.CRYPTO
        elif any(kw in keywords for kw in self.category_keywords[NewsCategory.MACRO]):
            return NewsCategory.MACRO
        elif any(kw in keywords for kw in self.category_keywords[NewsCategory.FINANCE]):
            return NewsCategory.FINANCE
        elif any(kw in keywords for kw in self.category_keywords[NewsCategory.PERSON]):
            return NewsCategory.PERSON
        else:
            # ì†ë³´ í‚¤ì›Œë“œ í™•ì¸
            breaking_keywords = ["breaking", "urgent", "alert", "just in"]
            if any(kw in text_lower for kw in breaking_keywords):
                return NewsCategory.BREAKING
            return NewsCategory.HEADLINE
    
    async def collect_headlines(self, count: int = 20) -> List[NewsArticle]:
        """ì£¼ìš” í—¤ë“œë¼ì¸ ìˆ˜ì§‘"""
        params = {
            "country": self.country,
            "pageSize": min(count, 100),  # API ì œí•œ
            "page": 1
        }
        
        data = await self._make_request("/top-headlines", params)
        
        if not data or 'articles' not in data:
            return []
        
        articles = []
        for item in data['articles'][:count]:
            article = self._parse_newsapi_article(item, NewsCategory.HEADLINE)
            if article:
                article = await self.analyze_sentiment(article)
                articles.append(article)
                self.stats["articles_collected"] += 1
        
        return articles
    
    async def search_news(self, keywords: List[str],
                         since: Optional[datetime] = None,
                         until: Optional[datetime] = None,
                         count: int = 20) -> List[NewsArticle]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰"""
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        query = " OR ".join(f'"{keyword}"' for keyword in keywords[:5])  # ìµœëŒ€ 5ê°œ
        
        params = {
            "q": query,
            "language": self.language,
            "sortBy": "relevancy",
            "pageSize": min(count, 100),
            "page": 1
        }
        
        # ì‹œê°„ ë²”ìœ„ ì„¤ì •
        if since:
            params["from"] = since.strftime('%Y-%m-%dT%H:%M:%SZ')
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œ ì§€ë‚œ 30ì¼
            params["from"] = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%dT%H:%M:%SZ')
        
        if until:
            params["to"] = until.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        data = await self._make_request("/everything", params)
        
        if not data or 'articles' not in data:
            return []
        
        articles = []
        for item in data['articles'][:count]:
            article = self._parse_newsapi_article(item)
            if article:
                article.relevance_score = self.calculate_relevance(article, keywords)
                article = await self.analyze_sentiment(article)
                articles.append(article)
                self.stats["articles_collected"] += 1
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        articles.sort(key=lambda x: x.relevance_score or 0, reverse=True)
        
        return articles
    
    async def get_breaking_news(self, minutes: int = 30) -> List[NewsArticle]:
        """ì†ë³´ ìˆ˜ì§‘"""
        # ìµœì‹  í—¤ë“œë¼ì¸ì—ì„œ ì†ë³´ í‚¤ì›Œë“œ í•„í„°ë§
        params = {
            "country": self.country,
            "pageSize": 50,
            "page": 1
        }
        
        data = await self._make_request("/top-headlines", params)
        
        if not data or 'articles' not in data:
            return []
        
        breaking_news = []
        cutoff_time = datetime.now() - timedelta(minutes=minutes)
        
        for item in data['articles']:
            article = self._parse_newsapi_article(item, NewsCategory.BREAKING)
            if article and article.published_date >= cutoff_time:
                # ì†ë³´ í‚¤ì›Œë“œ í™•ì¸
                breaking_keywords = ["breaking", "urgent", "alert", "just in", "update"]
                title_lower = article.title.lower()
                
                if any(kw in title_lower for kw in breaking_keywords):
                    article = await self.analyze_sentiment(article)
                    breaking_news.append(article)
        
        return breaking_news
    
    async def collect_crypto_news(self, count: int = 20) -> List[NewsArticle]:
        """ì•”í˜¸í™”í ë‰´ìŠ¤ íŠ¹í™” ìˆ˜ì§‘"""
        crypto_keywords = ["bitcoin", "ethereum", "cryptocurrency", "blockchain"]
        
        articles = await self.search_news(
            keywords=crypto_keywords,
            since=datetime.now() - timedelta(days=7),
            count=count
        )
        
        # ì¹´í…Œê³ ë¦¬ ì„¤ì •
        for article in articles:
            article.category = NewsCategory.CRYPTO
        
        return articles
    
    async def collect_finance_news(self, count: int = 20) -> List[NewsArticle]:
        """ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        finance_keywords = ["stock market", "wall street", "nasdaq", "s&p 500"]
        
        articles = await self.search_news(
            keywords=finance_keywords,
            since=datetime.now() - timedelta(days=3),
            count=count
        )
        
        for article in articles:
            article.category = NewsCategory.FINANCE
        
        return articles
    
    async def collect_macro_news(self, count: int = 20) -> List[NewsArticle]:
        """ê±°ì‹œê²½ì œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        macro_keywords = ["federal reserve", "inflation", "gdp", "unemployment"]
        
        articles = await self.search_news(
            keywords=macro_keywords,
            since=datetime.now() - timedelta(days=7),
            count=count
        )
        
        for article in articles:
            article.category = NewsCategory.MACRO
        
        return articles
    
    async def collect_person_news(self, person: str, count: int = 10) -> List[NewsArticle]:
        """íŠ¹ì • ì¸ë¬¼ ê´€ë ¨ ë‰´ìŠ¤"""
        articles = await self.search_news(
            keywords=[person],
            since=datetime.now() - timedelta(days=30),
            count=count
        )
        
        for article in articles:
            article.category = NewsCategory.PERSON
        
        return articles
    
    async def collect_by_source(self, sources: List[str], count: int = 20) -> List[NewsArticle]:
        """íŠ¹ì • ì†ŒìŠ¤ë“¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        source_string = ",".join(sources[:20])  # API ì œí•œ
        
        params = {
            "sources": source_string,
            "pageSize": min(count, 100),
            "page": 1
        }
        
        data = await self._make_request("/top-headlines", params)
        
        if not data or 'articles' not in data:
            return []
        
        articles = []
        for item in data['articles'][:count]:
            article = self._parse_newsapi_article(item)
            if article:
                article = await self.analyze_sentiment(article)
                articles.append(article)
                self.stats["articles_collected"] += 1
        
        return articles
    
    async def get_tech_giants_news(self, count: int = 15) -> List[NewsArticle]:
        """ë¹…í…Œí¬ ê´€ë ¨ ë‰´ìŠ¤"""
        tech_keywords = [
            "Apple", "Google", "Microsoft", "Amazon", "Tesla",
            "Meta", "Netflix", "NVIDIA"
        ]
        
        articles = await self.search_news(
            keywords=tech_keywords,
            since=datetime.now() - timedelta(days=7),
            count=count
        )
        
        return articles
    
    async def get_fed_news(self, count: int = 10) -> List[NewsArticle]:
        """ì—°ì¤€ ê´€ë ¨ ë‰´ìŠ¤"""
        fed_keywords = ["Federal Reserve", "Jerome Powell", "FOMC", "Fed meeting"]
        
        articles = await self.search_news(
            keywords=fed_keywords,
            since=datetime.now() - timedelta(days=14),
            count=count
        )
        
        for article in articles:
            article.category = NewsCategory.MACRO
        
        return articles
    
    async def analyze_news_sentiment_by_category(self, hours: int = 24) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ê°ì • ë¶„ì„"""
        since = datetime.now() - timedelta(hours=hours)
        
        categories_data = {}
        
        # ì£¼ìš” ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„
        category_methods = {
            "crypto": self.collect_crypto_news,
            "finance": self.collect_finance_news,
            "macro": self.collect_macro_news
        }
        
        for category_name, method in category_methods.items():
            try:
                articles = await method(count=20)
                
                # ê°ì • ë¶„ì„
                sentiments = {"positive": 0, "negative": 0, "neutral": 0}
                total_score = 0
                
                for article in articles:
                    if article.sentiment_label:
                        if article.sentiment_label.value > 0:
                            sentiments["positive"] += 1
                        elif article.sentiment_label.value < 0:
                            sentiments["negative"] += 1
                        else:
                            sentiments["neutral"] += 1
                    
                    if article.sentiment_score:
                        total_score += article.sentiment_score
                
                article_count = len(articles)
                avg_sentiment = total_score / article_count if article_count > 0 else 0
                
                categories_data[category_name] = {
                    "article_count": article_count,
                    "sentiment_distribution": sentiments,
                    "average_sentiment": avg_sentiment,
                    "top_sources": self._get_top_sources(articles)
                }
                
            except Exception as e:
                self.logger.error(f"Error analyzing {category_name}: {e}")
                categories_data[category_name] = {"error": str(e)}
        
        return {
            "analysis_period_hours": hours,
            "categories": categories_data,
            "timestamp": datetime.now().isoformat()
        }
    
    def _get_top_sources(self, articles: List[NewsArticle], top_n: int = 5) -> List[Dict[str, int]]:
        """ìƒìœ„ ì†ŒìŠ¤ ì¶”ì¶œ"""
        source_counts = {}
        for article in articles:
            source_counts[article.source] = source_counts.get(article.source, 0) + 1
        
        sorted_sources = sorted(source_counts.items(), key=lambda x: x[1], reverse=True)
        return [{"source": source, "count": count} for source, count in sorted_sources[:top_n]]
    
    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.session and not self.session.closed:
            await self.session.close()
        await super().close()


# ì‚¬ìš© ì˜ˆì œ
async def main():
    """NewsAPI Collector í…ŒìŠ¤íŠ¸"""
    if not os.getenv("NEWSAPI_KEY"):
        print("âŒ NEWSAPI_KEY environment variable required")
        print("Get free API key at: https://newsapi.org/register")
        return
    
    collector = NewsAPICollector()
    
    try:
        # í—¤ë“œë¼ì¸ ìˆ˜ì§‘
        print("ğŸ“° Collecting headlines...")
        headlines = await collector.collect_headlines(count=5)
        print(f"Found {len(headlines)} headlines")
        
        for article in headlines[:3]:
            print(f"\n- {article.title}")
            print(f"  Source: {article.source}")
            print(f"  Category: {article.category.value}")
            print(f"  Sentiment: {article.sentiment_label.name if article.sentiment_label else 'N/A'}")
        
        # ì•”í˜¸í™”í ë‰´ìŠ¤
        print("\n\nğŸ” Searching crypto news...")
        crypto_news = await collector.collect_crypto_news(count=5)
        print(f"Found {len(crypto_news)} crypto articles")
        
        for article in crypto_news[:3]:
            print(f"\n- {article.title}")
            print(f"  Relevance: {article.relevance_score:.2f}")
        
        # ì—°ì¤€ ë‰´ìŠ¤
        print("\n\nğŸ›ï¸ Fed news...")
        fed_news = await collector.get_fed_news(count=3)
        print(f"Found {len(fed_news)} Fed articles")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê°ì • ë¶„ì„
        print("\n\nğŸ“Š Category sentiment analysis...")
        sentiment_analysis = await collector.analyze_news_sentiment_by_category(hours=48)
        
        for category, data in sentiment_analysis["categories"].items():
            if "error" not in data:
                print(f"\n{category.upper()}:")
                print(f"  Articles: {data['article_count']}")
                print(f"  Avg Sentiment: {data['average_sentiment']:.2f}")
                print(f"  Distribution: {data['sentiment_distribution']}")
        
    finally:
        await collector.close()


if __name__ == "__main__":
    asyncio.run(main())