# ğŸ—ï¸ AuroraQ Architecture V3.0

**Advanced AI Trading System with Microservices Architecture**

ìµœì‹  ì—…ë°ì´íŠ¸: 2025ë…„ 7ì›” 28ì¼
ë²„ì „: 3.0 (FinBERT + Microservices + Docker)

---

## ğŸ“‹ ëª©ì°¨

1. [ì•„í‚¤í…ì²˜ ê°œìš”](#-ì•„í‚¤í…ì²˜-ê°œìš”)
2. [ì‹œìŠ¤í…œ êµ¬ì„±ìš”ì†Œ](#-ì‹œìŠ¤í…œ-êµ¬ì„±ìš”ì†Œ)
3. [ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜](#-ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤-ì•„í‚¤í…ì²˜)
4. [ê°ì •ë¶„ì„ ì„œë¹„ìŠ¤ ë…ë¦½í™”](#-ê°ì •ë¶„ì„-ì„œë¹„ìŠ¤-ë…ë¦½í™”)
5. [ë°ì´í„° í”Œë¡œìš°](#-ë°ì´í„°-í”Œë¡œìš°)
6. [API ì„¤ê³„](#-api-ì„¤ê³„)
7. [ë°°í¬ ë° í™•ì¥ì„±](#-ë°°í¬-ë°-í™•ì¥ì„±)
8. [ì„±ëŠ¥ ìµœì í™”](#-ì„±ëŠ¥-ìµœì í™”)

---

## ğŸ¯ ì•„í‚¤í…ì²˜ ê°œìš”

### **V3.0 ì£¼ìš” ë³€í™”ì **
- **ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜**: ëª¨ë…¸ë¦¬ì‹ â†’ ì„œë¹„ìŠ¤ ì§€í–¥ ì•„í‚¤í…ì²˜
- **ê°ì •ë¶„ì„ ì„œë¹„ìŠ¤ ë…ë¦½í™”**: FinBERT ê¸°ë°˜ ì „ìš© ì„œë¹„ìŠ¤
- **API ê¸°ë°˜ í†µì‹ **: REST API + gRPC í•˜ì´ë¸Œë¦¬ë“œ
- **ì»¨í…Œì´ë„ˆí™”**: Docker + Kubernetes ì§€ì›
- **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨í™”**: GPU/CPU ìì› ìµœì í™”

### **ì„¤ê³„ ì›ì¹™**
- **ë¶„ë¦¬ì™€ ë…ë¦½ì„±**: ê° ì„œë¹„ìŠ¤ëŠ” ë…ë¦½ì ìœ¼ë¡œ ë°°í¬/í™•ì¥ ê°€ëŠ¥
- **ì„±ëŠ¥ ìµœì í™”**: ëª¨ë¸ ë¡œë”©ê³¼ ìºì‹±ì„ ì¤‘ì•™ì§‘ì¤‘í™”
- **í™•ì¥ì„±**: ìˆ˜í‰ì  í™•ì¥ ì§€ì›
- **ì•ˆì •ì„±**: ì„œë¹„ìŠ¤ë³„ ì¥ì•  ê²©ë¦¬

---

## ğŸ¢ ì‹œìŠ¤í…œ êµ¬ì„±ìš”ì†Œ

### **Core AI Agents**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    AuroraQ      â”‚    â”‚     MacroQ      â”‚
â”‚  (Crypto Bot)   â”‚    â”‚  (Stock Bot)    â”‚
â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ PPO RL        â”‚    â”‚ â€¢ PPO RL        â”‚
â”‚ â€¢ Risk Mgmt     â”‚    â”‚ â€¢ Macro Anal    â”‚
â”‚ â€¢ Execution     â”‚    â”‚ â€¢ Portfolio     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   API Gateway         â”‚
         â”‚ (FastAPI + Load Bal)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Microservices Layer**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Microservices Ecosystem                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Sentiment  â”‚   Market    â”‚   Risk      â”‚   Execution     â”‚
â”‚  Service    â”‚   Data      â”‚   Service   â”‚   Service       â”‚
â”‚             â”‚   Service   â”‚             â”‚                 â”‚
â”‚ â€¢ FinBERT   â”‚ â€¢ Binance   â”‚ â€¢ VaR Calc  â”‚ â€¢ Order Mgmt    â”‚
â”‚ â€¢ Fusion    â”‚ â€¢ Feedly    â”‚ â€¢ Position  â”‚ â€¢ Portfolio     â”‚
â”‚ â€¢ Router    â”‚ â€¢ Cache     â”‚ â€¢ Limits    â”‚ â€¢ Reporting     â”‚
â”‚ â€¢ History   â”‚ â€¢ Redis     â”‚ â€¢ Alert     â”‚ â€¢ Monitoring    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Infrastructure Layer**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Infrastructure & DevOps                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Docker    â”‚ Kubernetes  â”‚   Monitoringâ”‚     Storage     â”‚
â”‚ Containers  â”‚   Cluster   â”‚   Stack     â”‚    Systems      â”‚
â”‚             â”‚             â”‚             â”‚                 â”‚
â”‚ â€¢ Services  â”‚ â€¢ Auto      â”‚ â€¢ Prometheusâ”‚ â€¢ PostgreSQL    â”‚
â”‚ â€¢ Images    â”‚   Scaling   â”‚ â€¢ Grafana   â”‚ â€¢ Redis         â”‚
â”‚ â€¢ Networks  â”‚ â€¢ Load Bal  â”‚ â€¢ Logging   â”‚ â€¢ MinIO         â”‚
â”‚ â€¢ Volumes   â”‚ â€¢ Service   â”‚ â€¢ Alerts    â”‚ â€¢ Backup        â”‚
â”‚             â”‚   Mesh      â”‚             â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜

### **Service Architecture Overview**
```mermaid
graph TB
    A[AuroraQ Agent] --> G[API Gateway]
    B[MacroQ Agent] --> G
    
    G --> S1[Sentiment Service]
    G --> S2[Market Data Service]
    G --> S3[Risk Service]
    G --> S4[Execution Service]
    
    S1 --> D1[(Redis Cache)]
    S1 --> D2[(PostgreSQL)]
    S2 --> D1
    S2 --> E1[Binance API]
    S2 --> E2[Feedly API]
    
    S3 --> D2
    S4 --> D2
    S4 --> E1
```

### **Service Responsibilities**

#### **1. Sentiment Service** ğŸ§ 
```yaml
responsibilities:
  - FinBERT ê¸°ë°˜ ê¸ˆìœµ ê°ì • ë¶„ì„
  - ë‹¤ì¤‘ ì†ŒìŠ¤ ê°ì • ìœµí•©
  - Live/Backtest ëª¨ë“œ ë¼ìš°íŒ…
  - ê°ì • íˆìŠ¤í† ë¦¬ ê´€ë¦¬
  
tech_stack:
  - FastAPI + uvicorn
  - FinBERT (ProsusAI/finbert)
  - PyTorch + transformers
  - Redis (ìºì‹±)
  - PostgreSQL (íˆìŠ¤í† ë¦¬)
  
resources:
  - CPU: 4 cores
  - Memory: 8GB
  - GPU: Optional (ì„±ëŠ¥ í–¥ìƒ)
  
endpoints:
  - POST /analyze/text
  - POST /analyze/batch
  - GET /sentiment/{asset}
  - POST /fusion/multi-source
  - GET /health
```

#### **2. Market Data Service** ğŸ“Š
```yaml
responsibilities:
  - ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘
  - ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ (Feedly)
  - ë°ì´í„° ì •ê·œí™” ë° ìºì‹±
  - íˆìŠ¤í† ë¦¬ ë°ì´í„° ê´€ë¦¬
  
tech_stack:
  - FastAPI + asyncio
  - aiohttp (API í˜¸ì¶œ)
  - pandas + numpy
  - Redis (ì‹¤ì‹œê°„ ìºì‹±)
  - TimescaleDB (ì‹œê³„ì—´ ë°ì´í„°)
  
endpoints:
  - GET /market/{symbol}/ohlcv
  - GET /news/{asset}/latest
  - GET /market/multiple
  - WebSocket /ws/market/{symbol}
```

#### **3. Risk Service** âš ï¸
```yaml
responsibilities:
  - ë¦¬ìŠ¤í¬ ë©”íŠ¸ë¦­ ê³„ì‚°
  - í¬ì§€ì…˜ í¬ê¸° ê²°ì •
  - ì†ì‹¤ í•œë„ ëª¨ë‹ˆí„°ë§
  - ë¦¬ìŠ¤í¬ ì•Œë¦¼
  
endpoints:
  - POST /risk/calculate
  - GET /risk/limits/{agent}
  - POST /risk/validate-order
  - GET /risk/metrics
```

#### **4. Execution Service** ğŸ¯
```yaml
responsibilities:
  - ì£¼ë¬¸ ì‹¤í–‰ ë° ê´€ë¦¬
  - í¬íŠ¸í´ë¦¬ì˜¤ ì¶”ì 
  - ì„±ê³¼ ë¶„ì„
  - ë³´ê³ ì„œ ìƒì„±
  
endpoints:
  - POST /orders/place
  - GET /portfolio/{agent}
  - GET /performance/metrics
  - GET /orders/history
```

---

## ğŸ§  ê°ì •ë¶„ì„ ì„œë¹„ìŠ¤ ë…ë¦½í™”

### **Architecture Design**

#### **Service Structure**
```
sentiment-service/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ analyze.py      # ê°ì • ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸
â”‚   â”‚   â”‚   â”œâ”€â”€ fusion.py       # ìœµí•© ì—”ë“œí¬ì¸íŠ¸
â”‚   â”‚   â”‚   â”œâ”€â”€ history.py      # íˆìŠ¤í† ë¦¬ ì—”ë“œí¬ì¸íŠ¸
â”‚   â”‚   â”‚   â””â”€â”€ health.py       # í—¬ìŠ¤ì²´í¬
â”‚   â”‚   â””â”€â”€ middleware/
â”‚   â”‚       â”œâ”€â”€ auth.py         # ì¸ì¦ ë¯¸ë“¤ì›¨ì–´
â”‚   â”‚       â”œâ”€â”€ rate_limit.py   # ì†ë„ ì œí•œ
â”‚   â”‚       â””â”€â”€ logging.py      # ë¡œê¹…
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sentiment/
â”‚   â”‚   â”‚   â”œâ”€â”€ finbert_analyzer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ fusion_manager.py
â”‚   â”‚   â”‚   â”œâ”€â”€ sentiment_router.py
â”‚   â”‚   â”‚   â””â”€â”€ history_loader.py
â”‚   â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â”‚   â”œâ”€â”€ redis_client.py
â”‚   â”‚   â”‚   â””â”€â”€ memory_cache.py
â”‚   â”‚   â””â”€â”€ config/
â”‚   â”‚       â”œâ”€â”€ settings.py
â”‚   â”‚       â””â”€â”€ logging.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ requests.py         # Pydantic ìš”ì²­ ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ responses.py        # Pydantic ì‘ë‹µ ëª¨ë¸
â”‚   â”‚   â””â”€â”€ database.py         # DB ëª¨ë¸
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ model_loader.py     # FinBERT ëª¨ë¸ ë¡œë”
â”‚       â””â”€â”€ validators.py       # ì…ë ¥ ê²€ì¦
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_sentiment.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ deploy/
    â”œâ”€â”€ k8s/                    # Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸
    â”œâ”€â”€ helm/                   # Helm ì°¨íŠ¸
    â””â”€â”€ scripts/
```

#### **API Schema Design**
```python
# Pydantic Models
class SentimentAnalyzeRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=10000)
    asset: Optional[str] = "CRYPTO"
    mode: Literal["live", "backtest"] = "live"
    timestamp: Optional[datetime] = None

class SentimentAnalyzeResponse(BaseModel):
    sentiment_score: float = Field(..., ge=0.0, le=1.0)
    label: Literal["positive", "negative", "neutral"]
    confidence: float = Field(..., ge=0.0, le=1.0)
    keywords: List[str] = []
    scenario_tag: str
    processing_time: float
    model_version: str

class BatchAnalyzeRequest(BaseModel):
    texts: List[str] = Field(..., min_items=1, max_items=100)
    asset: Optional[str] = "CRYPTO"
    mode: Literal["live", "backtest"] = "live"

class FusionRequest(BaseModel):
    sentiment_scores: Dict[str, float] = Field(..., min_items=1)
    symbol: str = "BTCUSDT"
    timestamp: Optional[datetime] = None

class FusionResponse(BaseModel):
    fused_score: float = Field(..., ge=0.0, le=1.0)
    weights_used: Dict[str, float]
    confidence: float
    trend: str
    metadata: Dict[str, Any] = {}
```

### **Docker Configuration**

#### **Dockerfile**
```dockerfile
# sentiment-service/docker/Dockerfile
FROM python:3.11-slim as base

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# FinBERT ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ (ì„ íƒì )
RUN python -c "from transformers import AutoTokenizer, AutoModelForSequenceClassification; AutoTokenizer.from_pretrained('ProsusAI/finbert'); AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')"

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY app/ ./app/

# ë¹„ë£¨íŠ¸ ìœ ì € ìƒì„±
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8001

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8001/health || exit 1

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8001", "--workers", "1"]
```

#### **Docker Compose**
```yaml
# sentiment-service/docker/docker-compose.yml
version: '3.8'

services:
  sentiment-service:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    ports:
      - "8001:8001"
    environment:
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://user:pass@postgres:5432/sentiment
      - LOG_LEVEL=INFO
      - MODEL_CACHE_DIR=/app/models
    volumes:
      - model_cache:/app/models
      - ./logs:/app/logs
    depends_on:
      - redis
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          memory: 4G
          cpus: '2'

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    restart: unless-stopped

  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: sentiment
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

volumes:
  model_cache:
  redis_data:
  postgres_data:
```

---

## ğŸŒŠ ë°ì´í„° í”Œë¡œìš°

### **Sentiment Analysis Flow**
```mermaid
sequenceDiagram
    participant A as AuroraQ
    participant G as API Gateway
    participant S as Sentiment Service
    participant R as Redis Cache
    participant F as FinBERT
    participant P as PostgreSQL

    A->>G: POST /sentiment/analyze
    G->>S: Forward request
    
    S->>R: Check cache
    alt Cache Hit
        R->>S: Return cached result
    else Cache Miss
        S->>F: Analyze text
        F->>S: Return sentiment
        S->>R: Cache result
        S->>P: Store history
    end
    
    S->>G: Return response
    G->>A: Forward response
```

### **Multi-Source Fusion Flow**
```mermaid
sequenceDiagram
    participant A as AuroraQ
    participant S as Sentiment Service
    participant N as News Collector
    participant T as Social Tracker
    participant F as Fusion Manager

    A->>S: Request BTC sentiment
    
    par Collect from multiple sources
        S->>N: Get news sentiment
        S->>T: Get social sentiment
    end
    
    S->>F: Fuse sentiment scores
    F->>F: Apply outlier detection
    F->>F: Calculate weighted average
    F->>S: Return fused score
    
    S->>A: Return comprehensive sentiment
```

---

## ğŸ“¡ API ì„¤ê³„

### **REST API Endpoints**

#### **ê°ì • ë¶„ì„ API**
```yaml
POST /api/v1/sentiment/analyze:
  summary: "ë‹¨ì¼ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„"
  parameters:
    - text: string (required)
    - asset: string (optional, default: "CRYPTO")
    - mode: string (optional, "live" | "backtest")
  responses:
    200:
      sentiment_score: float [0.0-1.0]
      label: string
      confidence: float
      keywords: array[string]
      scenario_tag: string
      processing_time: float

POST /api/v1/sentiment/batch:
  summary: "ë°°ì¹˜ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„"
  parameters:
    - texts: array[string] (max 100)
    - asset: string (optional)
  responses:
    200:
      results: array[SentimentResult]
      total_processed: integer
      avg_processing_time: float

GET /api/v1/sentiment/{asset}/realtime:
  summary: "ì‹¤ì‹œê°„ ìì‚° ê°ì • ì¡°íšŒ"
  parameters:
    - asset: string (path parameter)
    - hours_back: integer (optional, default: 6)
  responses:
    200:
      sentiment_score: float
      confidence: float
      article_count: integer
      trend: string
      sources: array[string]

POST /api/v1/sentiment/fusion:
  summary: "ë‹¤ì¤‘ ì†ŒìŠ¤ ê°ì • ìœµí•©"
  parameters:
    - sentiment_scores: object
    - symbol: string
    - timestamp: datetime (optional)
  responses:
    200:
      fused_score: float
      weights_used: object
      confidence: float
      trend: string
```

#### **gRPC ì¸í„°í˜ì´ìŠ¤** (ê³ ì„±ëŠ¥ ìš”êµ¬ì‹œ)
```protobuf
syntax = "proto3";

service SentimentService {
  rpc AnalyzeText(AnalyzeRequest) returns (AnalyzeResponse);
  rpc AnalyzeBatch(BatchRequest) returns (BatchResponse);
  rpc GetRealtimeSentiment(RealtimeRequest) returns (RealtimeResponse);
  rpc FuseSentiments(FusionRequest) returns (FusionResponse);
}

message AnalyzeRequest {
  string text = 1;
  string asset = 2;
  string mode = 3;
  google.protobuf.Timestamp timestamp = 4;
}

message AnalyzeResponse {
  float sentiment_score = 1;
  string label = 2;
  float confidence = 3;
  repeated string keywords = 4;
  string scenario_tag = 5;
  float processing_time = 6;
}
```

### **Client Integration**

#### **AuroraQ Integration**
```python
# AuroraQ/services/sentiment_client.py
import aiohttp
import asyncio
from typing import Dict, List, Optional
from datetime import datetime

class SentimentClient:
    def __init__(self, base_url: str = "http://sentiment-service:8001"):
        self.base_url = base_url
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def analyze_sentiment(
        self, 
        text: str, 
        asset: str = "BTC",
        mode: str = "live"
    ) -> Dict:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„"""
        async with self.session.post(
            f"{self.base_url}/api/v1/sentiment/analyze",
            json={
                "text": text,
                "asset": asset,
                "mode": mode
            }
        ) as response:
            return await response.json()
    
    async def get_realtime_sentiment(
        self, 
        asset: str = "BTC",
        hours_back: int = 6
    ) -> Dict:
        """ì‹¤ì‹œê°„ ê°ì • ë¶„ì„"""
        async with self.session.get(
            f"{self.base_url}/api/v1/sentiment/{asset}/realtime",
            params={"hours_back": hours_back}
        ) as response:
            return await response.json()
    
    async def batch_analyze(
        self, 
        texts: List[str], 
        asset: str = "BTC"
    ) -> Dict:
        """ë°°ì¹˜ ê°ì • ë¶„ì„"""
        async with self.session.post(
            f"{self.base_url}/api/v1/sentiment/batch",
            json={
                "texts": texts,
                "asset": asset
            }
        ) as response:
            return await response.json()

# Usage in AuroraQ
class AuroraQAgent:
    def __init__(self):
        self.sentiment_client = SentimentClient()
    
    async def make_decision(self, market_data: Dict) -> Dict:
        async with self.sentiment_client as client:
            # ì‹¤ì‹œê°„ BTC ê°ì • ë¶„ì„
            sentiment = await client.get_realtime_sentiment("BTC")
            
            # ì˜ì‚¬ê²°ì • ë¡œì§
            if sentiment['sentiment_score'] > 0.7 and sentiment['confidence'] > 0.8:
                action = "BUY"
            elif sentiment['sentiment_score'] < 0.3 and sentiment['confidence'] > 0.8:
                action = "SELL"
            else:
                action = "HOLD"
            
            return {
                "action": action,
                "sentiment_score": sentiment['sentiment_score'],
                "confidence": sentiment['confidence']
            }
```

---

## ğŸš€ ë°°í¬ ë° í™•ì¥ì„±

### **Kubernetes Deployment**

#### **Sentiment Service Deployment**
```yaml
# deploy/k8s/sentiment-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sentiment-service
  labels:
    app: sentiment-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: sentiment-service
  template:
    metadata:
      labels:
        app: sentiment-service
    spec:
      containers:
      - name: sentiment-service
        image: auroraq/sentiment-service:v3.0
        ports:
        - containerPort: 8001
        env:
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
        livenessProbe:
          httpGet:
            path: /health
            port: 8001
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8001
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: sentiment-service
spec:
  selector:
    app: sentiment-service
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8001
  type: ClusterIP

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: sentiment-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: sentiment-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

#### **Ingress Configuration**
```yaml
# deploy/k8s/sentiment-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: sentiment-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/rate-limit: "100"
spec:
  rules:
  - host: sentiment.auroraq.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: sentiment-service
            port:
              number: 80
```

### **Helm Chart**
```yaml
# deploy/helm/sentiment-service/Chart.yaml
apiVersion: v2
name: sentiment-service
description: AI-powered financial sentiment analysis service
version: 3.0.0
appVersion: "3.0.0"

# deploy/helm/sentiment-service/values.yaml
replicaCount: 3

image:
  repository: auroraq/sentiment-service
  tag: "v3.0"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 80
  targetPort: 8001

ingress:
  enabled: true
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/rate-limit: "100"
  hosts:
    - host: sentiment.auroraq.local
      paths:
        - path: /
          pathType: Prefix

resources:
  limits:
    cpu: 4
    memory: 8Gi
  requests:
    cpu: 2
    memory: 4Gi

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

env:
  REDIS_URL: "redis://redis-service:6379"
  LOG_LEVEL: "INFO"
  MODEL_CACHE_SIZE: "1000"

redis:
  enabled: true
  architecture: standalone
  auth:
    enabled: false

postgresql:
  enabled: true
  auth:
    postgresPassword: "your-secure-password"
  primary:
    persistence:
      enabled: true
      size: 20Gi
```

---

## ğŸ¯ ì„±ëŠ¥ ìµœì í™”

### **ëª¨ë¸ ë¡œë”© ìµœì í™”**
```python
# app/utils/model_loader.py
import os
import logging
from typing import Optional
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import torch
from functools import lru_cache

logger = logging.getLogger(__name__)

class FinBERTModelManager:
    """FinBERT ëª¨ë¸ ê´€ë¦¬ì - ì‹±ê¸€í†¤ íŒ¨í„´"""
    
    _instance: Optional['FinBERTModelManager'] = None
    _initialized: bool = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self.model = None
            self.tokenizer = None
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self._model_cache = {}
            self._initialized = True
    
    async def load_model(self, model_name: str = "ProsusAI/finbert"):
        """ëª¨ë¸ ë¡œë”© (ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ì‹œ í•œ ë²ˆë§Œ)"""
        if self.model is not None:
            return
        
        logger.info(f"Loading FinBERT model: {model_name} on {self.device}")
        
        try:
            # ìºì‹œëœ ëª¨ë¸ ê²½ë¡œ í™•ì¸
            cache_dir = os.getenv("MODEL_CACHE_DIR", "/app/models")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                cache_dir=cache_dir
            )
            
            model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                cache_dir=cache_dir
            )
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ ëª¨ë¸ì„ GPUë¡œ ì´ë™
            if self.device == "cuda":
                model = model.cuda()
            
            self.model = pipeline(
                "sentiment-analysis",
                model=model,
                tokenizer=self.tokenizer,
                device=0 if self.device == "cuda" else -1,
                return_all_scores=True
            )
            
            logger.info("FinBERT model loaded successfully âœ…")
            
        except Exception as e:
            logger.error(f"Failed to load FinBERT model: {e}")
            raise

# ì „ì—­ ëª¨ë¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
model_manager = FinBERTModelManager()

@lru_cache(maxsize=1000)
def get_cached_sentiment(text_hash: str, text: str) -> dict:
    """LRU ìºì‹œë¥¼ ì‚¬ìš©í•œ ê°ì • ë¶„ì„"""
    if model_manager.model is None:
        raise RuntimeError("Model not loaded")
    
    result = model_manager.model(text)[0]
    
    # ê²°ê³¼ ì •ê·œí™”
    positive_score = next(r['score'] for r in result if r['label'] == 'positive')
    negative_score = next(r['score'] for r in result if r['label'] == 'negative')
    neutral_score = next(r['score'] for r in result if r['label'] == 'neutral')
    
    # ìµœê³  ì ìˆ˜ ë ˆì´ë¸” ê²°ì •
    max_score = max(positive_score, negative_score, neutral_score)
    if max_score == positive_score:
        label = "positive"
        normalized_score = 0.5 + (positive_score - 0.5)
    elif max_score == negative_score:
        label = "negative"
        normalized_score = 0.5 - (negative_score - 0.5)
    else:
        label = "neutral"
        normalized_score = 0.5
    
    return {
        "sentiment_score": min(max(normalized_score, 0.0), 1.0),
        "label": label,
        "confidence": max_score,
        "raw_scores": {
            "positive": positive_score,
            "negative": negative_score,
            "neutral": neutral_score
        }
    }
```

### **Redis ìºì‹± ì „ëµ**
```python
# app/core/cache/redis_client.py
import json
import hashlib
from typing import Optional, Any, Dict
import aioredis
from datetime import datetime, timedelta

class RedisCacheManager:
    def __init__(self, redis_url: str):
        self.redis_url = redis_url
        self.redis: Optional[aioredis.Redis] = None
    
    async def connect(self):
        """Redis ì—°ê²°"""
        self.redis = aioredis.from_url(self.redis_url)
    
    async def close(self):
        """Redis ì—°ê²° ì¢…ë£Œ"""
        if self.redis:
            await self.redis.close()
    
    def _get_cache_key(self, prefix: str, data: Any) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        data_str = json.dumps(data, sort_keys=True, default=str)
        hash_key = hashlib.md5(data_str.encode()).hexdigest()
        return f"{prefix}:{hash_key}"
    
    async def get_sentiment_cache(
        self, 
        text: str, 
        asset: str = "CRYPTO"
    ) -> Optional[Dict]:
        """ê°ì • ë¶„ì„ ìºì‹œ ì¡°íšŒ"""
        if not self.redis:
            return None
        
        cache_key = self._get_cache_key("sentiment", {"text": text[:100], "asset": asset})
        
        try:
            cached_data = await self.redis.get(cache_key)
            if cached_data:
                return json.loads(cached_data)
        except Exception as e:
            logger.error(f"Cache get error: {e}")
        
        return None
    
    async def set_sentiment_cache(
        self, 
        text: str, 
        asset: str, 
        result: Dict,
        ttl_seconds: int = 3600
    ):
        """ê°ì • ë¶„ì„ ê²°ê³¼ ìºì‹±"""
        if not self.redis:
            return
        
        cache_key = self._get_cache_key("sentiment", {"text": text[:100], "asset": asset})
        
        try:
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            cache_data = {
                **result,
                "cached_at": datetime.utcnow().isoformat(),
                "cache_key": cache_key
            }
            
            await self.redis.setex(
                cache_key, 
                ttl_seconds, 
                json.dumps(cache_data, default=str)
            )
        except Exception as e:
            logger.error(f"Cache set error: {e}")
    
    async def get_realtime_sentiment_cache(self, asset: str) -> Optional[Dict]:
        """ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ ìºì‹œ ì¡°íšŒ"""
        cache_key = f"realtime_sentiment:{asset}"
        
        try:
            cached_data = await self.redis.get(cache_key)
            if cached_data:
                data = json.loads(cached_data)
                # 10ë¶„ ì´ë‚´ ë°ì´í„°ë§Œ ìœ íš¨
                cached_time = datetime.fromisoformat(data['timestamp'])
                if datetime.utcnow() - cached_time < timedelta(minutes=10):
                    return data
        except Exception as e:
            logger.error(f"Realtime cache get error: {e}")
        
        return None
    
    async def set_realtime_sentiment_cache(
        self, 
        asset: str, 
        result: Dict,
        ttl_seconds: int = 600  # 10ë¶„
    ):
        """ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ ê²°ê³¼ ìºì‹±"""
        cache_key = f"realtime_sentiment:{asset}"
        
        try:
            cache_data = {
                **result,
                "timestamp": datetime.utcnow().isoformat()
            }
            
            await self.redis.setex(
                cache_key, 
                ttl_seconds, 
                json.dumps(cache_data, default=str)
            )
        except Exception as e:
            logger.error(f"Realtime cache set error: {e}")
```

### **ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”**
```python
# app/api/routes/analyze.py
from fastapi import APIRouter, HTTPException, BackgroundTasks
from typing import List
import asyncio
from concurrent.futures import ThreadPoolExecutor

router = APIRouter()

class BatchProcessor:
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def process_batch(self, texts: List[str], asset: str) -> List[Dict]:
        """ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”"""
        # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í•  (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        chunk_size = 16
        chunks = [texts[i:i + chunk_size] for i in range(0, len(texts), chunk_size)]
        
        # ë³‘ë ¬ ì²˜ë¦¬
        tasks = []
        for chunk in chunks:
            task = asyncio.create_task(self._process_chunk(chunk, asset))
            tasks.append(task)
        
        # ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸°
        results = await asyncio.gather(*tasks)
        
        # ê²°ê³¼ ë³‘í•©
        all_results = []
        for chunk_results in results:
            all_results.extend(chunk_results)
        
        return all_results
    
    async def _process_chunk(self, texts: List[str], asset: str) -> List[Dict]:
        """ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬"""
        loop = asyncio.get_event_loop()
        
        # CPU ì§‘ì•½ì  ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        results = await loop.run_in_executor(
            self.executor,
            self._analyze_texts_sync,
            texts,
            asset
        )
        
        return results
    
    def _analyze_texts_sync(self, texts: List[str], asset: str) -> List[Dict]:
        """ë™ê¸° í…ìŠ¤íŠ¸ ë¶„ì„ (ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰)"""
        results = []
        
        for text in texts:
            try:
                # ìºì‹œëœ ê°ì • ë¶„ì„ ì‚¬ìš©
                text_hash = hashlib.md5(text.encode()).hexdigest()
                result = get_cached_sentiment(text_hash, text)
                
                results.append({
                    "text": text[:100] + "..." if len(text) > 100 else text,
                    "sentiment_score": result["sentiment_score"],
                    "label": result["label"],
                    "confidence": result["confidence"],
                    "asset": asset
                })
                
            except Exception as e:
                logger.error(f"Batch analysis error for text: {e}")
                results.append({
                    "text": text[:100] + "..." if len(text) > 100 else text,
                    "sentiment_score": 0.5,
                    "label": "neutral",
                    "confidence": 0.0,
                    "asset": asset,
                    "error": str(e)
                })
        
        return results

# ì „ì—­ ë°°ì¹˜ í”„ë¡œì„¸ì„œ
batch_processor = BatchProcessor()

@router.post("/batch", response_model=BatchAnalyzeResponse)
async def analyze_batch(
    request: BatchAnalyzeRequest,
    background_tasks: BackgroundTasks
):
    """ë°°ì¹˜ ê°ì • ë¶„ì„"""
    if len(request.texts) > 100:
        raise HTTPException(
            status_code=400, 
            detail="Maximum 100 texts allowed per batch"
        )
    
    start_time = time.time()
    
    try:
        # ë°°ì¹˜ ì²˜ë¦¬
        results = await batch_processor.process_batch(request.texts, request.asset)
        
        processing_time = time.time() - start_time
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìºì‹±
        background_tasks.add_task(
            cache_batch_results,
            request.texts,
            request.asset,
            results
        )
        
        return BatchAnalyzeResponse(
            results=results,
            total_processed=len(results),
            avg_processing_time=processing_time / len(results),
            total_processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"Batch analysis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„±

### **Prometheus Metrics**
```python
# app/core/metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
from functools import wraps

# ë©”íŠ¸ë¦­ ì •ì˜
SENTIMENT_REQUESTS = Counter(
    'sentiment_requests_total',
    'Total sentiment analysis requests',
    ['method', 'asset', 'status']
)

SENTIMENT_DURATION = Histogram(
    'sentiment_analysis_duration_seconds',
    'Sentiment analysis duration',
    ['method', 'asset']
)

MODEL_LOADING_TIME = Histogram(
    'model_loading_duration_seconds',
    'Model loading duration'
)

CACHE_HITS = Counter(
    'cache_hits_total',
    'Cache hits',
    ['cache_type']
)

ACTIVE_CONNECTIONS = Gauge(
    'active_connections',
    'Active connections'
)

def track_sentiment_metrics(asset: str = "unknown"):
    """ê°ì • ë¶„ì„ ë©”íŠ¸ë¦­ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            
            try:
                result = await func(*args, **kwargs)
                SENTIMENT_REQUESTS.labels(
                    method=func.__name__,
                    asset=asset,
                    status='success'
                ).inc()
                return result
                
            except Exception as e:
                SENTIMENT_REQUESTS.labels(
                    method=func.__name__,
                    asset=asset,
                    status='error'
                ).inc()
                raise
                
            finally:
                duration = time.time() - start_time
                SENTIMENT_DURATION.labels(
                    method=func.__name__,
                    asset=asset
                ).observe(duration)
        
        return wrapper
    return decorator
```

### **Structured Logging**
```python
# app/core/config/logging.py
import logging
import json
from datetime import datetime
from typing import Dict, Any

class StructuredFormatter(logging.Formatter):
    """êµ¬ì¡°í™”ëœ ë¡œê·¸ í¬ë§·í„°"""
    
    def format(self, record: logging.LogRecord) -> str:
        log_data = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        
        # ì¶”ê°€ í•„ë“œê°€ ìˆìœ¼ë©´ í¬í•¨
        if hasattr(record, 'extra_fields'):
            log_data.update(record.extra_fields)
        
        # ì˜ˆì™¸ ì •ë³´ í¬í•¨
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)
        
        return json.dumps(log_data, ensure_ascii=False)

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    handler = logging.StreamHandler()
    handler.setFormatter(StructuredFormatter())
    
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    root_logger.addHandler(handler)
    
    # íŠ¹ì • ë¡œê±° ë ˆë²¨ ì„¤ì •
    logging.getLogger("uvicorn").setLevel(logging.WARNING)
    logging.getLogger("transformers").setLevel(logging.WARNING)

class SentimentLogger:
    """ê°ì • ë¶„ì„ ì „ìš© ë¡œê±°"""
    
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
    
    def log_analysis(
        self, 
        text: str, 
        result: Dict[str, Any],
        processing_time: float,
        cached: bool = False
    ):
        """ê°ì • ë¶„ì„ ë¡œê·¸"""
        self.logger.info(
            "Sentiment analysis completed",
            extra={
                'extra_fields': {
                    'text_length': len(text),
                    'sentiment_score': result.get('sentiment_score'),
                    'label': result.get('label'),
                    'confidence': result.get('confidence'),
                    'processing_time': processing_time,
                    'cached': cached,
                    'operation': 'sentiment_analysis'
                }
            }
        )
    
    def log_error(self, error: Exception, context: Dict[str, Any]):
        """ì—ëŸ¬ ë¡œê·¸"""
        self.logger.error(
            f"Error occurred: {str(error)}",
            extra={
                'extra_fields': {
                    **context,
                    'error_type': type(error).__name__,
                    'operation': 'error'
                }
            },
            exc_info=True
        )
```

---

## ğŸ”’ ë³´ì•ˆ ë° ì¸ì¦

### **API í‚¤ ê¸°ë°˜ ì¸ì¦**
```python
# app/api/middleware/auth.py
from fastapi import HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt
from datetime import datetime, timedelta
import os

security = HTTPBearer()

class AuthManager:
    def __init__(self):
        self.secret_key = os.getenv("JWT_SECRET_KEY", "your-secret-key")
        self.algorithm = "HS256"
        self.api_keys = set(os.getenv("API_KEYS", "").split(","))
    
    def verify_api_key(self, api_key: str) -> bool:
        """API í‚¤ ê²€ì¦"""
        return api_key in self.api_keys
    
    def create_access_token(self, data: dict) -> str:
        """ì•¡ì„¸ìŠ¤ í† í° ìƒì„±"""
        to_encode = data.copy()
        expire = datetime.utcnow() + timedelta(hours=24)
        to_encode.update({"exp": expire})
        
        return jwt.encode(to_encode, self.secret_key, algorithm=self.algorithm)
    
    def verify_token(self, token: str) -> dict:
        """í† í° ê²€ì¦"""
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])
            return payload
        except jwt.ExpiredSignatureError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Token expired"
            )
        except jwt.JWTError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token"
            )

auth_manager = AuthManager()

async def verify_authentication(
    credentials: HTTPAuthorizationCredentials = Depends(security)
) -> dict:
    """ì¸ì¦ í™•ì¸"""
    token = credentials.credentials
    
    # API í‚¤ ë°©ì‹ í™•ì¸
    if auth_manager.verify_api_key(token):
        return {"type": "api_key", "key": token}
    
    # JWT í† í° ë°©ì‹ í™•ì¸
    try:
        payload = auth_manager.verify_token(token)
        return {"type": "jwt", "payload": payload}
    except HTTPException:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials"
        )
```

### **Rate Limiting**
```python
# app/api/middleware/rate_limit.py
from fastapi import HTTPException, Request
import time
from collections import defaultdict, deque
from typing import Dict, Deque
import asyncio

class RateLimiter:
    def __init__(self):
        self.requests: Dict[str, Deque[float]] = defaultdict(deque)
        self.lock = asyncio.Lock()
    
    async def is_allowed(
        self, 
        identifier: str, 
        max_requests: int = 100, 
        window_seconds: int = 60
    ) -> bool:
        """ìš”ì²­ í—ˆìš© ì—¬ë¶€ í™•ì¸"""
        async with self.lock:
            now = time.time()
            window_start = now - window_seconds
            
            # ìœˆë„ìš° ë°–ì˜ ìš”ì²­ ì œê±°
            while (self.requests[identifier] and 
                   self.requests[identifier][0] < window_start):
                self.requests[identifier].popleft()
            
            # í˜„ì¬ ìš”ì²­ ìˆ˜ í™•ì¸
            if len(self.requests[identifier]) >= max_requests:
                return False
            
            # í˜„ì¬ ìš”ì²­ ì¶”ê°€
            self.requests[identifier].append(now)
            return True

rate_limiter = RateLimiter()

async def rate_limit_middleware(request: Request, call_next):
    """Rate limiting ë¯¸ë“¤ì›¨ì–´"""
    # í´ë¼ì´ì–¸íŠ¸ ì‹ë³„ (IP ì£¼ì†Œ ë˜ëŠ” API í‚¤)
    client_ip = request.client.host
    auth_header = request.headers.get("Authorization")
    
    identifier = f"ip:{client_ip}"
    if auth_header:
        identifier = f"auth:{auth_header[:20]}"  # API í‚¤ì˜ ì• 20ìë¦¬
    
    # Rate limit í™•ì¸
    allowed = await rate_limiter.is_allowed(identifier, max_requests=100, window_seconds=60)
    
    if not allowed:
        raise HTTPException(
            status_code=429,
            detail="Rate limit exceeded. Please try again later."
        )
    
    response = await call_next(request)
    return response
```

---

## ğŸ¯ ê²°ë¡ 

**AuroraQ Architecture V3.0**ì€ ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤:

### **âœ… ì£¼ìš” ë‹¬ì„± ëª©í‘œ**
1. **ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜**: ê° ì„œë¹„ìŠ¤ ë…ë¦½ì  ê°œë°œ/ë°°í¬/í™•ì¥
2. **FinBERT ê¸°ë°˜ ê³ ê¸‰ ê°ì •ë¶„ì„**: ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” AI ëª¨ë¸
3. **ì»¨í…Œì´ë„ˆí™” ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**: Docker + Kubernetes ì™„ì „ ì§€ì›
4. **API ê¸°ë°˜ í†µì‹ **: REST + gRPC í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜
5. **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨í™”**: ëª¨ë¸ ë¡œë”© ì¤‘ì•™ì§‘ì¤‘í™” ë° ìºì‹± ìµœì í™”

### **ğŸš€ í™•ì¥ì„± ë° ì„±ëŠ¥**
- **ìˆ˜í‰ì  í™•ì¥**: HPAë¥¼ í†µí•œ ìë™ ìŠ¤ì¼€ì¼ë§
- **ë¡œë“œ ë°¸ëŸ°ì‹±**: íŠ¸ë˜í”½ ë¶„ì‚° ë° ê³ ê°€ìš©ì„±
- **ìºì‹± ì „ëµ**: Redis ê¸°ë°˜ ë‹¤ì¸µ ìºì‹±
- **ë°°ì¹˜ ì²˜ë¦¬**: ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìµœì í™”

### **ğŸ”’ ë³´ì•ˆ ë° ìš´ì˜**
- **API í‚¤ + JWT** ì´ì¤‘ ì¸ì¦
- **Rate Limiting** ë° DDoS ë°©ì–´
- **êµ¬ì¡°í™”ëœ ë¡œê¹…** ë° ë©”íŠ¸ë¦­
- **í—¬ìŠ¤ì²´í¬** ë° ëª¨ë‹ˆí„°ë§

### **ğŸ¨ ê°œë°œì ê²½í—˜**
- **ê°„ë‹¨í•œ API ì¸í„°í˜ì´ìŠ¤**: RESTful + ì§ê´€ì  ì—”ë“œí¬ì¸íŠ¸
- **ì™„ì „í•œ Docker ì§€ì›**: ë¡œì»¬ ê°œë°œë¶€í„° í”„ë¡œë•ì…˜ê¹Œì§€
- **í¬ê´„ì ì¸ ë¬¸ì„œí™”**: API ìŠ¤í‚¤ë§ˆ + ì‚¬ìš© ì˜ˆì œ
- **í…ŒìŠ¤íŠ¸ ìë™í™”**: ë‹¨ìœ„/í†µí•©/E2E í…ŒìŠ¤íŠ¸

**ì´ì œ AuroraQì™€ MacroQëŠ” ê³ ì„±ëŠ¥, í™•ì¥ ê°€ëŠ¥í•œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê¸°ë°˜ ê°ì •ë¶„ì„ì„ í†µí•´ ë”ìš± ì •í™•í•œ AI ê±°ë˜ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!** ğŸ‰