#!/usr/bin/env python3
"""
VPS PPO Strategy - ë…ë¦½ì ì¸ PPO ê°•í™”í•™ìŠµ ì „ëµ
ì„ íƒì  ì˜ì¡´ì„±ìœ¼ë¡œ VPS deploymentì—ì„œ ì•ˆì „í•˜ê²Œ ë™ì‘
"""


# VPS ë°°í¬ ì‹œìŠ¤í…œ ê²½ë¡œ ì„¤ì •
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import os
import sys
import logging
import numpy as np
import pandas as pd
import time
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple
from datetime import datetime
from dataclasses import dataclass
import asyncio

# Sentiment Integration
try:
    from trading.sentiment_integration import get_sentiment_client, SentimentScore, MarketSentiment
    SENTIMENT_AVAILABLE = True
except ImportError:
    SENTIMENT_AVAILABLE = False
    # Fallback classes
    class SentimentScore:
        def __init__(self):
            self.value = 0.0
            self.confidence = 0.0
            self.weighted_score = 0.0
        
    class MarketSentiment:
        def to_feature_vector(self):
            return [0.0] * 6
    
    def get_sentiment_client():
        return None

# BaseRuleStrategy import (ê°™ì€ ë””ë ‰í† ë¦¬)
try:
    from trading.rule_strategies import BaseRuleStrategy
except ImportError:
    # ë…ë¦½ ì‹¤í–‰ ì‹œ
    sys.path.append(os.path.dirname(__file__)) 
    from rule_strategies import BaseRuleStrategy

# PPO Agent and Trainer modules
PPO_MODULES_AVAILABLE = False
try:
    from ppo_agent import PPOAgent, PPOAgentConfig, ActionResult
    from ppo_trainer import PPOTrainer, TrainingConfig
    PPO_MODULES_AVAILABLE = True
except ImportError:
    try:
        from trading.ppo_agent import PPOAgent, PPOAgentConfig, ActionResult
        from trading.ppo_trainer import PPOTrainer, TrainingConfig
        PPO_MODULES_AVAILABLE = True
    except ImportError:
        PPO_MODULES_AVAILABLE = False

# ì„ íƒì  PPO ì˜ì¡´ì„±
PPO_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    from stable_baselines3 import PPO
    from stable_baselines3.common.env_util import make_vec_env
    PPO_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("PPO dependencies available")
except ImportError as e:
    logger = logging.getLogger(__name__)
    logger.warning(f"PPO dependencies not available: {e}")

@dataclass
class PPOConfig:
    """PPO ì„¤ì •"""
    model_path: str = os.getenv('PPO_MODEL_PATH', '/app/models/ppo_model.zip')
    confidence_threshold: float = 0.7
    max_positions: int = 2
    state_features: int = 26  # ê¸°ì¡´ 20 + ê°ì • íŠ¹ì„± 6ê°œ
    action_space_size: int = 3  # BUY, SELL, HOLD
    learning_rate: float = 3e-4
    n_steps: int = 2048
    batch_size: int = 64
    n_epochs: int = 10
    gamma: float = 0.99
    gae_lambda: float = 0.95
    clip_range: float = 0.2
    ent_coef: float = 0.01
    vf_coef: float = 0.5
    max_grad_norm: float = 0.5
    use_sde: bool = False
    sde_sample_freq: int = -1
    
    # P1-3: ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì • ì„¤ì •
    enable_dynamic_batching: bool = True
    min_batch_size: int = 16
    max_batch_size: int = 256
    target_processing_time_s: float = 2.0
    target_memory_mb: float = 1000.0

class PPOStrategy(BaseRuleStrategy):
    """VPSìš© PPO ê°•í™”í•™ìŠµ ì „ëµ"""
    
    def __init__(self, config: PPOConfig = None):
        super().__init__(name="PPOStrategy")
        
        self.config = config or PPOConfig()
        self.model = None
        self.model_loaded = False
        self.last_prediction = None
        self.prediction_cache = {}
        
        # PPO ì „ìš© ì„±ê³¼ ì¶”ì 
        self.ppo_predictions = 0
        self.ppo_successes = 0
        self.model_confidence_history = []
        
        # BaseRuleStrategy í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±ë“¤
        self.total_trades = 0
        self.winning_trades = 0
        self.losing_trades = 0
        self.total_profit = 0.0
        self.total_loss = 0.0
        self.profit_factor = 1.0
        
        # ìƒíƒœ ì „ì²˜ë¦¬ ì„¤ì • (ê°ì • íŠ¹ì„± í¬í•¨)
        self.feature_columns = [
            'close', 'volume', 'rsi_14', 'macd', 'bb_upper', 'bb_lower',
            'ema_12', 'ema_26', 'adx', 'stoch_k', 'stoch_d',
            # ê°ì • ë¶„ì„ íŠ¹ì„±
            'sentiment_overall', 'sentiment_fear_greed', 'sentiment_volatility',
            'sentiment_trend', 'sentiment_social', 'sentiment_news'
        ]
        
        # PPO Agent ì´ˆê¸°í™”
        if PPO_MODULES_AVAILABLE:
            agent_config = PPOAgentConfig(
                model_path=self.config.model_path,
                confidence_threshold=self.config.confidence_threshold,
                state_features=self.config.state_features
            )
            self.agent = PPOAgent(agent_config)
            self.model_loaded = self.agent.is_ready()
        else:
            self.agent = None
            self.model_loaded = False

        # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±ë“¤
        self.model = self.agent.model if self.agent else None

        # Trainer ì´ˆê¸°í™” (í•„ìš” ì‹œ)
        self.trainer = None
        if PPO_MODULES_AVAILABLE:
            training_config = TrainingConfig(
                state_features=self.config.state_features,
                model_save_dir=str(Path(self.config.model_path).parent)
            )
            self.trainer = PPOTrainer(training_config)

        # ê°ì • ë¶„ì„ í´ë¼ì´ì–¸íŠ¸
        self.sentiment_client = get_sentiment_client() if SENTIMENT_AVAILABLE else None
        self.sentiment_cache = {}
        self.last_sentiment_update = 0
        
        # P1-3: ë™ì  ë°°ì¹˜ í¬ê¸° ê´€ë¦¬ì ì´ˆê¸°í™”
        self.batch_manager = None
        if self.config.enable_dynamic_batching:
            try:
                from core.performance.dynamic_batch_manager import DynamicBatchManager, BatchConfig
                batch_config = BatchConfig(
                    initial_batch_size=self.config.batch_size,
                    min_batch_size=self.config.min_batch_size,
                    max_batch_size=self.config.max_batch_size,
                    target_processing_time_s=self.config.target_processing_time_s,
                    target_memory_mb=self.config.target_memory_mb
                )
                self.batch_manager = DynamicBatchManager(batch_config)
                logger.info(f"Dynamic batch manager initialized: {self.config.batch_size} -> {self.batch_manager.get_current_batch_size()}")
            except ImportError as e:
                logger.warning(f"Dynamic batch manager not available: {e}")
                self.batch_manager = None
        
        # Agent ê¸°ë°˜ ëª¨ë¸ ìƒíƒœ í™•ì¸ (Agent ìì²´ì ìœ¼ë¡œ ë¡œë“œë¨)
        if self.agent:
            self.model_loaded = self.agent.is_ready()
            if self.model_loaded:
                self.model = self.agent.model
        
        logger.info(f"PPOStrategy ì´ˆê¸°í™” ì™„ë£Œ - PPO ì‚¬ìš©ê°€ëŠ¥: {PPO_AVAILABLE}, Agent ì¤€ë¹„: {self.model_loaded}, ê°ì •ë¶„ì„: {SENTIMENT_AVAILABLE}, Modules: {PPO_MODULES_AVAILABLE}")
    
    def _load_model(self) -> bool:
        """PPO ëª¨ë¸ ë¡œë“œ (Agent ê¸°ë°˜)"""
        if not PPO_MODULES_AVAILABLE:
            logger.warning("PPO modules not available - using fallback mode")
            return False
        
        try:
            if self.agent and self.agent.is_ready():
                self.model_loaded = True
                # í˜¸í™˜ì„±ì„ ìœ„í•´ model ì°¸ì¡°ë„ ì„¤ì •
                self.model = self.agent.model
                logger.info(f"PPO Agent loaded successfully from {self.config.model_path}")
                return True
            else:
                logger.warning(f"PPO Agent failed to initialize - using fallback mode")
                return False
                
        except Exception as e:
            logger.error(f"PPO Agent loading failed: {e} - using fallback mode")
            return False
    
    async def _get_sentiment_features(self, symbol: str = "BTCUSDT") -> List[float]:
        """ê°ì • ë¶„ì„ íŠ¹ì„± ìˆ˜ì§‘"""
        try:
            # ìºì‹œ í™•ì¸ (2ë¶„ ìºì‹œ)
            current_time = time.time()
            cache_key = f"{symbol}_sentiment"
            
            if (cache_key in self.sentiment_cache and 
                current_time - self.last_sentiment_update < 120):
                return self.sentiment_cache[cache_key]
            
            if self.sentiment_client:
                # ì‹œì¥ ê°ì • ìƒíƒœ ìˆ˜ì§‘
                market_sentiment = await self.sentiment_client.get_market_sentiment(symbol)
                sentiment_features = market_sentiment.to_feature_vector()
                
                # ìºì‹œ ì—…ë°ì´íŠ¸
                self.sentiment_cache[cache_key] = sentiment_features
                self.last_sentiment_update = current_time
                
                return sentiment_features
            else:
                # Fallback: ì¤‘ë¦½ ê°ì •
                return [0.0, 0.5, 0.0, 0.0, 0.0, 0.0]
                
        except Exception as e:
            logger.warning(f"Sentiment feature collection failed: {e}")
            return [0.0, 0.5, 0.0, 0.0, 0.0, 0.0]  # Fallback
    
    def _extract_features(self, price_data: pd.DataFrame, sentiment_features: List[float] = None) -> Optional[np.ndarray]:
        """ê°€ê²© ë°ì´í„°ì—ì„œ íŠ¹ì„± ì¶”ì¶œ (ê°ì • íŠ¹ì„± í¬í•¨)"""
        try:
            logger.debug(f"íŠ¹ì„± ì¶”ì¶œ ì‹œì‘ - ë°ì´í„° ê¸¸ì´: {len(price_data)}, ê°ì • íŠ¹ì„± ìˆ˜: {len(sentiment_features) if sentiment_features else 0}")
            
            if len(price_data) < 50:
                logger.debug("ë°ì´í„° ê¸¸ì´ ë¶€ì¡± (< 50)")
                return None
            
            # ê¸°ë³¸ ì§€í‘œ ê³„ì‚° (BaseRuleStrategyì˜ ë©”ì„œë“œ í™œìš©)
            indicators = self.calculate_indicators(price_data)
            
            if indicators is None or len(indicators) == 0:
                return None
            
            # íŠ¹ì„± ë²¡í„° êµ¬ì„±
            features = []
            
            # ê°€ê²© ì •ë³´
            current_close = self.safe_last(price_data, "close")
            features.append(current_close / 50000.0)  # ì •ê·œí™” (BTC ê¸°ì¤€)
            
            # ê±°ë˜ëŸ‰ ì •ë³´
            current_volume = self.safe_last(price_data, "volume", 0)
            if "volume" in price_data.columns:
                avg_volume = price_data["volume"].tail(20).mean()
            else:
                avg_volume = 1
            volume_ratio = current_volume / avg_volume if avg_volume > 0 else 1
            features.append(min(volume_ratio, 5.0) / 5.0)  # ì •ê·œí™”
            
            # ê¸°ìˆ ì  ì§€í‘œ
            features.append(indicators.get('rsi_14', 50) / 100.0)  # RSI
            features.append(indicators.get('macd', 0) / 1000.0)    # MACD
            features.append(indicators.get('adx', 25) / 100.0)     # ADX
            
            # ì´ë™í‰ê· 
            ema_12 = indicators.get('ema_12', current_close)
            ema_26 = indicators.get('ema_26', current_close)
            features.append((current_close - ema_12) / current_close)  # EMA12 ë¹„ìœ¨
            features.append((current_close - ema_26) / current_close)  # EMA26 ë¹„ìœ¨
            features.append((ema_12 - ema_26) / current_close)         # EMA ì°¨ì´
            
            # ë³¼ë¦°ì € ë°´ë“œ
            bb_upper = indicators.get('bb_upper', current_close * 1.02)
            bb_lower = indicators.get('bb_lower', current_close * 0.98)
            
            # Seriesë¥¼ floatë¡œ ë³€í™˜
            if hasattr(bb_upper, 'iloc'):  # Seriesì¸ ê²½ìš°
                bb_upper = float(bb_upper.iloc[-1])
            if hasattr(bb_lower, 'iloc'):  # Seriesì¸ ê²½ìš°
                bb_lower = float(bb_lower.iloc[-1])
            
            bb_diff = bb_upper - bb_lower
            bb_position = (current_close - bb_lower) / bb_diff if abs(bb_diff) > 1e-8 else 0.5
            features.append(bb_position)
            
            # ìŠ¤í† ìºìŠ¤í‹±
            stoch_k = indicators.get('stoch_k', 50)
            stoch_d = indicators.get('stoch_d', 50)
            features.append(stoch_k / 100.0)
            features.append(stoch_d / 100.0)
            
            # ê°€ê²© ë³€í™”ìœ¨ (ì—¬ëŸ¬ ê¸°ê°„)
            if len(price_data) >= 5:
                price_5_ago = price_data['close'].iloc[-5]
                features.append((current_close - price_5_ago) / price_5_ago)
            else:
                features.append(0.0)
            
            if len(price_data) >= 20:
                price_20_ago = price_data['close'].iloc[-20]
                features.append((current_close - price_20_ago) / price_20_ago)
            else:
                features.append(0.0)
            
            # ë³€ë™ì„±
            if len(price_data) >= 14:
                returns = price_data['close'].pct_change().tail(14).dropna()
                volatility = returns.std()
                features.append(min(volatility * 100, 10.0) / 10.0)  # ì •ê·œí™”
            else:
                features.append(0.02)  # ê¸°ë³¸ê°’
            
            # íŠ¸ë Œë“œ ê°•ë„
            if len(price_data) >= 10:
                close_values = price_data['close'].tail(10).values  # Series â†’ array ë³€í™˜
                trend_slope = np.polyfit(range(10), close_values, 1)[0]
                features.append(trend_slope / current_close * 1000)  # ì •ê·œí™”
            else:
                features.append(0.0)
            
            # ê°ì • ë¶„ì„ íŠ¹ì„± ì¶”ê°€ (6ê°œ)
            if sentiment_features and len(sentiment_features) >= 6:
                # sentiment_featuresì˜ ê° ìš”ì†Œë¥¼ floatë¡œ ë³€í™˜
                for i in range(6):
                    sentiment_val = sentiment_features[i] if i < len(sentiment_features) else 0.0
                    features.append(float(sentiment_val))
            else:
                # Fallback: ì¤‘ë¦½ ê°ì • íŠ¹ì„±
                features.extend([0.0, 0.5, 0.0, 0.0, 0.0, 0.0])
            
            # ëª¨ë“  íŠ¹ì„±ì„ floatë¡œ ë³€í™˜í•˜ì—¬ 26ê°œë¡œ ë§ì¶”ê¸°
            final_features = []
            for i, feat in enumerate(features):
                try:
                    final_features.append(float(feat))
                except (ValueError, TypeError) as e:
                    logger.debug(f"íŠ¹ì„± {i} ë³€í™˜ ì‹¤íŒ¨: {feat} -> 0.0 ({e})")
                    final_features.append(0.0)
            
            logger.debug(f"ë³€í™˜ëœ íŠ¹ì„± ìˆ˜: {len(final_features)}")
            
            # 26ê°œë¡œ ë§ì¶”ê¸°
            while len(final_features) < self.config.state_features:
                final_features.append(0.0)
            
            # 26ê°œë¡œ ì œí•œ
            final_features = final_features[:self.config.state_features]
            
            logger.debug(f"ìµœì¢… íŠ¹ì„± ë°°ì—´ í¬ê¸°: {len(final_features)}")
            return np.array(final_features, dtype=np.float32)
            
        except Exception as e:
            import traceback
            logger.error(f"Feature extraction error: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return None
    
    def _predict_action(self, features: np.ndarray) -> Tuple[int, float]:
        """PPO ëª¨ë¸ë¡œ ì•¡ì…˜ ì˜ˆì¸¡ (Agent ì‚¬ìš©)"""
        try:
            if not PPO_MODULES_AVAILABLE or not self.model_loaded or not self.agent:
                return 2, 0.5  # HOLD, ì¤‘ë¦½ ì‹ ë¢°ë„
            
            # ìƒˆë¡œìš´ Agentë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡
            action_result = self.agent.predict(features)
            
            if action_result:
                self.ppo_predictions += 1
                return action_result.action, action_result.confidence
            else:
                logger.warning("Agent prediction returned None")
                return 2, 0.3  # HOLD, ë‚®ì€ ì‹ ë¢°ë„
            
        except Exception as e:
            logger.error(f"PPO Agent prediction error: {e}")
            return 2, 0.3  # HOLD, ë‚®ì€ ì‹ ë¢°ë„
    
    async def should_enter(self, price_data: pd.DataFrame, symbol: str = "BTCUSDT") -> Optional[Dict[str, Any]]:
        """ì§„ì… ì¡°ê±´ í™•ì¸ (PPO ê¸°ë°˜, ê°ì • ë¶„ì„ í¬í•¨)"""
        try:
            # ê°ì • ë¶„ì„ íŠ¹ì„± ìˆ˜ì§‘
            sentiment_features = await self._get_sentiment_features(symbol)
            
            # íŠ¹ì„± ì¶”ì¶œ (ê°ì • í¬í•¨)
            features = self._extract_features(price_data, sentiment_features)
            if features is None:
                return None
            
            # PPO ì˜ˆì¸¡
            action, confidence = self._predict_action(features)
            
            # ì‹ ë¢°ë„ ì„ê³„ê°’ í™•ì¸
            if confidence < self.config.confidence_threshold:
                return None
            
            current_price = self.safe_last(price_data, "close")
            
            # ì•¡ì…˜ì— ë”°ë¥¸ ì‹ í˜¸ ìƒì„±
            if action == 0:  # BUY
                return {
                    "side": "LONG",
                    "confidence": confidence,
                    "reason": f"PPO BUY signal (confidence: {confidence:.3f}, sentiment: {sentiment_features[0]:.2f})",
                    "stop_loss": current_price * 0.98,
                    "take_profit": current_price * 1.04,
                    "ppo_action": action,
                    "features_count": len(features),
                    "sentiment_boost": sentiment_features[0] > 0.1,  # ê¸ì •ì  ê°ì • ì‹œ ì‹ í˜¸ ê°•í™”
                    "sentiment_features": sentiment_features
                }
            elif action == 1:  # SELL
                return {
                    "side": "SHORT", 
                    "confidence": confidence,
                    "reason": f"PPO SELL signal (confidence: {confidence:.3f}, sentiment: {sentiment_features[0]:.2f})",
                    "stop_loss": current_price * 1.02,
                    "take_profit": current_price * 0.96,
                    "ppo_action": action,
                    "features_count": len(features),
                    "sentiment_boost": sentiment_features[0] < -0.1,  # ë¶€ì •ì  ê°ì • ì‹œ ì‹ í˜¸ ê°•í™”
                    "sentiment_features": sentiment_features
                }
            else:  # HOLD
                return None
                
        except Exception as e:
            logger.error(f"PPO should_enter error: {e}")
            return None
    
    def should_exit(self, position, price_data: pd.DataFrame) -> Optional[str]:
        """ì²­ì‚° ì¡°ê±´ í™•ì¸"""
        try:
            # ê¸°ë³¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬
            current_price = self.safe_last(price_data, "close")
            
            if hasattr(position, 'entry_price') and hasattr(position, 'side'):
                entry_price = position.entry_price
                side = position.side
                
                # ì†ì ˆ/ìµì ˆ í™•ì¸
                if side == "LONG":
                    if current_price <= entry_price * 0.97:  # 3% ì†ì ˆ
                        return "stop_loss"
                    elif current_price >= entry_price * 1.05:  # 5% ìµì ˆ
                        return "take_profit"
                elif side == "SHORT":
                    if current_price >= entry_price * 1.03:  # 3% ì†ì ˆ
                        return "stop_loss"
                    elif current_price <= entry_price * 0.95:  # 5% ìµì ˆ
                        return "take_profit"
            
            # PPO ì¬í‰ê°€
            features = self._extract_features(price_data)
            if features is not None:
                action, confidence = self._predict_action(features)
                
                # ë°˜ëŒ€ ì‹ í˜¸ê°€ ê°•í•˜ê²Œ ë‚˜ì˜¬ ë•Œ ì²­ì‚°
                if confidence > 0.8:
                    if hasattr(position, 'side'):
                        if position.side == "LONG" and action == 1:  # SELL signal
                            return "ppo_reversal"
                        elif position.side == "SHORT" and action == 0:  # BUY signal
                            return "ppo_reversal"
            
            return None
            
        except Exception as e:
            logger.error(f"PPO should_exit error: {e}")
            return None
    
    def score(self, price_data: pd.DataFrame) -> Tuple[float, Dict[str, float]]:
        """PPO ì „ëµ ì ìˆ˜ ê³„ì‚° (Profit Factor í¬í•¨)"""
        try:
            scores = {}
            
            if len(price_data) < 50:
                return 0.0, scores
            
            # 1. PPO ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ì„± ì ìˆ˜
            if self.model_loaded and PPO_AVAILABLE:
                scores['model_availability'] = 1.0
            else:
                scores['model_availability'] = 0.0
                # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë‚®ì€ ì ìˆ˜ ë°˜í™˜
                return 0.2, scores
            
            # 2. íŠ¹ì„± ì¶”ì¶œ í’ˆì§ˆ ì ìˆ˜
            features = self._extract_features(price_data)
            if features is not None:
                # íŠ¹ì„± ê°’ë“¤ì˜ ë¶„ì‚°ìœ¼ë¡œ í’ˆì§ˆ ì¸¡ì •
                feature_variance = np.var(features)
                scores['feature_quality'] = min(1.0, feature_variance * 10)
            else:
                scores['feature_quality'] = 0.0
            
            # 3. PPO ì˜ˆì¸¡ ì‹ ë¢°ë„ ì ìˆ˜
            if features is not None:
                action, confidence = self._predict_action(features)
                scores['prediction_confidence'] = confidence
                scores['action_decisiveness'] = 1.0 if action != 2 else 0.5  # HOLDì´ ì•„ë‹ˆë©´ ê²°ì •ì 
            else:
                scores['prediction_confidence'] = 0.0
                scores['action_decisiveness'] = 0.0
            
            # 4. ëª¨ë¸ ì„±ê³¼ ê¸°ë¡ ì ìˆ˜
            if self.ppo_predictions > 0:
                success_rate = self.ppo_successes / self.ppo_predictions
                scores['historical_performance'] = success_rate
            else:
                scores['historical_performance'] = 0.5  # ì¤‘ë¦½
            
            # 5. Profit Factor ë³´ë„ˆìŠ¤ ì ìˆ˜ (BaseRuleStrategy ìƒì†)
            if hasattr(self, 'profit_factor') and self.total_trades > 5:
                if self.profit_factor == float('inf'):
                    scores['profit_factor'] = 1.0
                elif self.profit_factor > 2.0:
                    scores['profit_factor'] = 1.0
                elif self.profit_factor > 1.5:
                    scores['profit_factor'] = 0.8 + (self.profit_factor - 1.5) * 0.4
                elif self.profit_factor > 1.0:
                    scores['profit_factor'] = 0.5 + (self.profit_factor - 1.0) * 0.6
                else:
                    scores['profit_factor'] = max(0.0, self.profit_factor * 0.5)
            else:
                scores['profit_factor'] = 0.5
            
            # 6. ì‹œì¥ ì¡°ê±´ ì í•©ì„± ì ìˆ˜
            current_price = self.safe_last(price_data, "close")
            if len(price_data) >= 20:
                price_volatility = price_data['close'].pct_change().tail(20).std()
                # PPOëŠ” ë³€ë™ì„±ì´ ìˆëŠ” ì‹œì¥ì—ì„œ ë” íš¨ê³¼ì 
                scores['market_suitability'] = min(1.0, price_volatility * 50)
            else:
                scores['market_suitability'] = 0.5
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            weights = {
                'model_availability': 0.25,    # ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ì„±
                'feature_quality': 0.15,       # íŠ¹ì„± í’ˆì§ˆ
                'prediction_confidence': 0.20, # ì˜ˆì¸¡ ì‹ ë¢°ë„
                'action_decisiveness': 0.10,   # ì•¡ì…˜ ê²°ì •ì„±
                'historical_performance': 0.15, # ê³¼ê±° ì„±ê³¼
                'profit_factor': 0.10,         # Profit Factor ë³´ë„ˆìŠ¤
                'market_suitability': 0.05     # ì‹œì¥ ì í•©ì„±
            }
            
            composite_score = sum(scores.get(key, 0) * weight for key, weight in weights.items())
            
            return composite_score, scores
            
        except Exception as e:
            logger.error(f"PPO score calculation error: {e}")
            return 0.0, {'error': 1.0}
    
    async def generate_signal(self, price_data: pd.DataFrame, symbol: str = "BTCUSDT") -> Dict[str, Any]:
        """VPSìš© ì‹ í˜¸ ìƒì„± ë©”ì„œë“œ (ê°ì • ë¶„ì„ í†µí•©, P1-3: ë™ì  ë°°ì¹˜ ì„±ëŠ¥ ì¶”ì )"""
        start_time = time.time()
        
        try:
            if len(price_data) < 50:
                return {
                    "action": "HOLD",
                    "strength": 0.0,
                    "price": self.safe_last(price_data, "close"),
                    "metadata": {
                        "strategy": "PPOStrategy",
                        "reason": "Insufficient data",
                        "confidence": 0.0,
                        "composite_score": 0.0,
                        "detail_scores": {},
                        "sentiment_integrated": SENTIMENT_AVAILABLE,
                        "dynamic_batching": self.batch_manager is not None
                    }
                }
            
            # P1-3: í˜„ì¬ ë°°ì¹˜ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
            current_batch_size = self.batch_manager.get_current_batch_size() if self.batch_manager else self.config.batch_size
            
            # ì§„ì… ì¡°ê±´ í™•ì¸ (ê°ì • ë¶„ì„ í¬í•¨)
            entry_signal = await self.should_enter(price_data, symbol) 
            current_price = self.safe_last(price_data, "close")
            
            if entry_signal:
                # ì ìˆ˜ ê³„ì‚°
                composite_score, detail_scores = self.score(price_data)
                
                # ê°ì • ë¶€ìŠ¤íŠ¸ ì ìš©
                strength = entry_signal.get("confidence", 0.5) * composite_score
                if entry_signal.get("sentiment_boost", False):
                    strength = min(strength * 1.1, 1.0)  # 10% ë¶€ìŠ¤íŠ¸
                
                # P1-3: ì„±ê³µì ì¸ ì‹ í˜¸ ìƒì„± ì‹œ ë°°ì¹˜ ì„±ëŠ¥ ì—…ë°ì´íŠ¸
                processing_time = time.time() - start_time
                if self.batch_manager:
                    self.batch_manager.update_batch_performance(
                        processing_time=processing_time,
                        items_processed=current_batch_size,
                        success_rate=1.0,  # ì‹ í˜¸ ìƒì„± ì„±ê³µ
                        custom_metrics={"confidence": entry_signal.get("confidence", 0.5)}
                    )
                
                return {
                    "action": "BUY" if entry_signal["side"] == "LONG" else "SELL",
                    "strength": strength,
                    "price": current_price,
                    "metadata": {
                        "strategy": "PPOStrategy",
                        "reason": entry_signal.get("reason", ""),
                        "confidence": entry_signal.get("confidence", 0.5),
                        "composite_score": composite_score,
                        "detail_scores": detail_scores,
                        "stop_loss": entry_signal.get("stop_loss"),
                        "take_profit": entry_signal.get("take_profit"),
                        "ppo_action": entry_signal.get("ppo_action"),
                        "features_count": entry_signal.get("features_count", 0),
                        "model_loaded": self.model_loaded,
                        "ppo_available": PPO_AVAILABLE,
                        "sentiment_integrated": SENTIMENT_AVAILABLE,
                        "sentiment_boost": entry_signal.get("sentiment_boost", False),
                        "sentiment_features": entry_signal.get("sentiment_features", []),
                        "dynamic_batching": self.batch_manager is not None,
                        "current_batch_size": current_batch_size,
                        "processing_time_ms": round(processing_time * 1000, 2)
                    }
                }
            
            # HOLD ì‹ í˜¸
            composite_score, detail_scores = self.score(price_data)
            
            # P1-3: HOLD ì‹ í˜¸ ì‹œì—ë„ ë°°ì¹˜ ì„±ëŠ¥ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            if self.batch_manager:
                self.batch_manager.update_batch_performance(
                    processing_time=processing_time,
                    items_processed=current_batch_size,
                    success_rate=0.5,  # HOLDëŠ” ì¤‘ê°„ ì„±ê³µë¥ 
                    custom_metrics={"signal_type": "hold"}
                )
            
            # PPO ì ìˆ˜ ë¡œê¹… (HOLD ì‹ í˜¸)
            try:
                from trading.ppo_score_logger import get_ppo_score_logger
                ppo_logger = get_ppo_score_logger()
                ppo_logger.log_score(
                    strategy_score=composite_score,
                    confidence=0.0,
                    action="HOLD",
                    selected=False,  # HOLDëŠ” ì„ íƒë˜ì§€ ì•ŠìŒ
                    total_predictions=self.ppo_predictions,
                    success_rate=self.ppo_successes / self.ppo_predictions if self.ppo_predictions > 0 else 0.0
                )
            except ImportError:
                logger.warning("PPO ì ìˆ˜ ë¡œê±°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            return {
                "action": "HOLD",
                "strength": 0.0,
                "price": current_price,
                "metadata": {
                    "strategy": "PPOStrategy",
                    "reason": "No strong signal or low confidence",
                    "confidence": 0.0,
                    "composite_score": composite_score,
                    "detail_scores": detail_scores,
                    "model_loaded": self.model_loaded,
                    "ppo_available": PPO_AVAILABLE,
                    "sentiment_integrated": SENTIMENT_AVAILABLE,
                    "dynamic_batching": self.batch_manager is not None,
                    "current_batch_size": current_batch_size,
                    "processing_time_ms": round(processing_time * 1000, 2)
                }
            }
            
        except Exception as e:
            logger.error(f"PPO generate_signal error: {e}")
            current_price = self.safe_last(price_data, "close")
            
            # P1-3: ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë°°ì¹˜ ì„±ëŠ¥ ì—…ë°ì´íŠ¸ (ë‚®ì€ ì„±ê³µë¥ )
            processing_time = time.time() - start_time
            if self.batch_manager:
                self.batch_manager.update_batch_performance(
                    processing_time=processing_time,
                    items_processed=current_batch_size if 'current_batch_size' in locals() else self.config.batch_size,
                    success_rate=0.0,  # ì—ëŸ¬ ë°œìƒ ì‹œ 0% ì„±ê³µë¥ 
                    custom_metrics={"error": str(e)[:50]}  # ì—ëŸ¬ ë©”ì‹œì§€ ì¼ë¶€
                )
            
            return {
                "action": "HOLD",
                "strength": 0.0,
                "price": current_price,
                "metadata": {
                    "strategy": "PPOStrategy",
                    "reason": f"Error: {str(e)}",
                    "confidence": 0.0,
                    "composite_score": 0.0,
                    "detail_scores": {"error": 1.0},
                    "model_loaded": self.model_loaded,
                    "ppo_available": PPO_AVAILABLE,
                    "sentiment_integrated": SENTIMENT_AVAILABLE
                }
            }
    
    def update_prediction_result(self, success: bool):
        """PPO ì˜ˆì¸¡ ê²°ê³¼ ì—…ë°ì´íŠ¸"""
        if success:
            self.ppo_successes += 1
            self.model_confidence_history.append(1.0)
        else:
            self.model_confidence_history.append(0.0)
        
        # ìµœê·¼ 20ê°œë§Œ ìœ ì§€
        if len(self.model_confidence_history) > 20:
            self.model_confidence_history.pop(0)
    
    def get_ppo_statistics(self) -> Dict[str, Any]:
        """PPO ì „ìš© í†µê³„ (P1-3: ë™ì  ë°°ì¹˜ ì„±ëŠ¥ í¬í•¨)"""
        stats = {
            'ppo_available': PPO_AVAILABLE,
            'ppo_modules_available': PPO_MODULES_AVAILABLE,
            'model_loaded': self.model_loaded,
            'agent_ready': self.agent.is_ready() if self.agent else False,
            'model_path': self.config.model_path,
            'total_predictions': self.ppo_predictions,
            'successful_predictions': self.ppo_successes,
            'prediction_success_rate': self.ppo_successes / self.ppo_predictions if self.ppo_predictions > 0 else 0,
            'confidence_threshold': self.config.confidence_threshold,
            'recent_confidence_avg': np.mean(self.model_confidence_history) if self.model_confidence_history else 0.0,
            'deployment_mode': 'inference_only',  # VPSëŠ” ì¶”ë¡ ë§Œ ìˆ˜í–‰
            'training_available': self.trainer is not None  # Trainer ê°€ìš©ì„±
        }
        
        # P1-3: ë™ì  ë°°ì¹˜ ì„±ëŠ¥ í†µê³„ ì¶”ê°€
        if self.batch_manager:
            batch_summary = self.batch_manager.get_performance_summary()
            stats.update({
                'dynamic_batching_enabled': True,
                'batch_performance': batch_summary,
                'batch_recommendations': self.batch_manager.get_optimization_recommendations()
            })
        else:
            stats.update({
                'dynamic_batching_enabled': False,
                'static_batch_size': self.config.batch_size
            })
        
        return stats
    
    def add_training_experience(self, state: np.ndarray, action: int, reward: float, 
                               next_state: np.ndarray, done: bool):
        """íŠ¸ë ˆì´ë‹ ê²½í—˜ ì¶”ê°€ (Trainer ì‚¬ìš©)"""
        if self.trainer and PPO_MODULES_AVAILABLE:
            try:
                self.trainer.add_experience(state, action, reward, next_state, done)
                logger.debug(f"Training experience added: action={action}, reward={reward}")
            except Exception as e:
                logger.error(f"Failed to add training experience: {e}")
        else:
            logger.warning("Trainer not available for adding experience")
    
    def add_score_based_reward(self, state: np.ndarray, action: int, strategy_score: float, 
                              market_outcome: float, next_state: np.ndarray, done: bool = False,
                              confidence: float = 0.5, action_str: str = 'UNKNOWN') -> float:
        """
        ğŸ¯ ì •ë°€í™”ëœ ì „ëµ ì ìˆ˜ â†’ ë³´ìƒ ë³€í™˜ ì‹œìŠ¤í…œ
        Rule ì „ëµê³¼ ìœ ì‚¬í•œ ì„±ê³¼ ê¸°ë°˜ ë³´ìƒ ê°•í™”, PPOTrainerì— íš¨ê³¼ì  shaping í”¼ë“œë°± ì œê³µ
        
        Args:
            state: í˜„ì¬ ìƒíƒœ ë²¡í„°
            action: ì„ íƒëœ ì•¡ì…˜ (0=BUY, 1=SELL, 2=HOLD)
            strategy_score: PPO ì „ëµ ì ìˆ˜ (0.0~1.0)
            market_outcome: ì‹œì¥ ê²°ê³¼ (ì†ìµë¥ , -1.0~1.0)
            next_state: ë‹¤ìŒ ìƒíƒœ ë²¡í„°
            done: ì—í”¼ì†Œë“œ ì™„ë£Œ ì—¬ë¶€
            confidence: ì˜ˆì¸¡ ì‹ ë¢°ë„ (0.0~1.0)
            action_str: ì•¡ì…˜ ë¬¸ìì—´ (ë¡œê¹…ìš©)
            
        Returns:
            ìµœì¢… ê³„ì‚°ëœ ë³´ìƒê°’ (-1.0~1.0)
        """
        try:
            # 1. ê¸°ë³¸ ë³´ìƒ ì»´í¬ë„ŒíŠ¸ ê³„ì‚°
            base_reward = self._calculate_base_reward(strategy_score, confidence)
            market_reward = self._calculate_market_reward(market_outcome)
            consistency_reward = self._calculate_consistency_reward(action, market_outcome)
            risk_penalty = self._calculate_risk_penalty(state, action)
            exploration_bonus = self._calculate_exploration_bonus(action)
            
            # 2. ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° (Rule ì „ëµ ëŒ€ë¹„ ì„±ê³¼ ê¸°ë°˜)
            performance_weights = self._calculate_dynamic_weights()
            
            # 3. ë©€í‹°ì»´í¬ë„ŒíŠ¸ ë³´ìƒ ì¡°í•©
            final_reward = (
                base_reward * performance_weights['base'] +              # ì „ëµ ì ìˆ˜ & ì‹ ë¢°ë„
                market_reward * performance_weights['market'] +          # ì‹œì¥ ê²°ê³¼
                consistency_reward * performance_weights['consistency'] + # ì¼ê´€ì„± ë³´ë„ˆìŠ¤
                risk_penalty * performance_weights['risk'] +             # ë¦¬ìŠ¤í¬ í˜ë„í‹°
                exploration_bonus * performance_weights['exploration']   # íƒí—˜ ë³´ë„ˆìŠ¤
            )
            
            # 4. ì ì‘í˜• ë³´ìƒ í´ë¦¬í•‘ ë° ìŠ¤ì¼€ì¼ë§
            final_reward = self._adaptive_reward_scaling(final_reward, strategy_score, confidence)
            
            # 5. ê³ ê¸‰ ê²½í—˜ ë©”íƒ€ë°ì´í„° ìƒì„±
            experience_metadata = {
                'strategy_score': strategy_score,
                'market_outcome': market_outcome,
                'final_reward': final_reward,
                'confidence': confidence,
                'action_str': action_str,
                'reward_components': {
                    'base_reward': base_reward,
                    'market_reward': market_reward,
                    'consistency_reward': consistency_reward,
                    'risk_penalty': risk_penalty,
                    'exploration_bonus': exploration_bonus
                },
                'weights': performance_weights,
                'timestamp': datetime.now().isoformat()
            }
            
            # 6. í–¥ìƒëœ ê²½í—˜ ì¶”ê°€ (ë©”íƒ€ë°ì´í„° í¬í•¨)
            self.add_enhanced_training_experience(
                state, action, final_reward, next_state, done, experience_metadata
            )
            
            # 7. ì„±ê³¼ ì¶”ì  ì—…ë°ì´íŠ¸
            self._update_performance_tracking(final_reward, strategy_score, market_outcome, confidence)
            
            # 8. í–¥ìƒëœ ë¡œê¹…
            self._log_reward_shaping_details(experience_metadata)
            
            return final_reward
            
        except Exception as e:
            logger.error(f"ê³ ê¸‰ ì ìˆ˜ ê¸°ë°˜ ë³´ìƒ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ë³´ìƒ ê³„ì‚°
            fallback_reward = (strategy_score * 0.3) + (market_outcome * 0.7)
            return max(-1.0, min(1.0, fallback_reward))
    
    def _calculate_base_reward(self, strategy_score: float, confidence: float) -> float:
        """ê¸°ë³¸ ë³´ìƒ ê³„ì‚° (ì „ëµ ì ìˆ˜ + ì‹ ë¢°ë„)"""
        # ì „ëµ ì ìˆ˜ì™€ ì‹ ë¢°ë„ì˜ ì¡°í™” í‰ê·  ì‚¬ìš© (ê·¹ê°’ ë°©ì§€)
        if strategy_score + confidence > 0:
            harmonic_mean = 2 * strategy_score * confidence / (strategy_score + confidence)
        else:
            harmonic_mean = 0.0
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ ë³´ë„ˆìŠ¤
        confidence_bonus = 0.1 if confidence > 0.8 else 0.0
        
        return min(1.0, harmonic_mean + confidence_bonus)
    
    def _calculate_market_reward(self, market_outcome: float) -> float:
        """ì‹œì¥ ë³´ìƒ ê³„ì‚° (ë¹„ì„ í˜• ìŠ¤ì¼€ì¼ë§)"""
        # ì‘ì€ ìˆ˜ìµë„ í¬ì§€í‹°ë¸Œí•˜ê²Œ ë³´ìƒ, í° ì†ì‹¤ì€ ê°•í•˜ê²Œ í˜ë„í‹°
        if market_outcome > 0:
            # ìˆ˜ìµ: ì œê³±ê·¼ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì‘ì€ ìˆ˜ìµë„ ì¸ì„¼í‹°ë¸Œ
            return min(1.0, np.sqrt(abs(market_outcome)) * np.sign(market_outcome))
        else:
            # ì†ì‹¤: ì œê³± ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ í° ì†ì‹¤ ê°•ë ¥ í˜ë„í‹°
            return max(-1.0, -(abs(market_outcome) ** 1.5))
    
    def _calculate_consistency_reward(self, action: int, market_outcome: float) -> float:
        """ì¼ê´€ì„± ë³´ìƒ ê³„ì‚° (ì•¡ì…˜ê³¼ ê²°ê³¼ì˜ ì¼ì¹˜ë„)"""
        # BUY ì•¡ì…˜ì—ì„œ ìˆ˜ìµ, SELL ì•¡ì…˜ì—ì„œ ìˆ˜ìµ ì‹œ ë³´ë„ˆìŠ¤
        if action == 0 and market_outcome > 0:  # BUYì—ì„œ ìˆ˜ìµ
            return 0.15
        elif action == 1 and market_outcome > 0:  # SELLì—ì„œ ìˆ˜ìµ (ìˆ í¬ì§€ì…˜)
            return 0.15
        elif action == 2:  # HOLD ì•¡ì…˜
            return 0.05 if abs(market_outcome) < 0.01 else -0.05  # ì•ˆì •ì„± ë³´ìƒ/í˜ë„í‹°
        else:
            return -0.1  # ì•¡ì…˜ê³¼ ê²°ê³¼ ë¶ˆì¼ì¹˜ í˜ë„í‹°
    
    def _calculate_risk_penalty(self, state: np.ndarray, action: int) -> float:
        """ë¦¬ìŠ¤í¬ í˜ë„í‹° ê³„ì‚° (ì‹œì¥ ì¡°ê±´ ê¸°ë°˜)"""
        try:
            # ìƒíƒœ ë²¡í„°ì—ì„œ ë³€ë™ì„± ì¶”ì¶œ (ì¸ë±ìŠ¤ 12ëŠ” ë³€ë™ì„± íŠ¹ì„±)
            volatility = state[12] if len(state) > 12 else 0.02
            
            # ê³ ë³€ë™ì„±ì—ì„œ ê³µê²©ì  ì•¡ì…˜ í˜ë„í‹°
            if volatility > 0.8 and action != 2:  # ê³ ë³€ë™ì„±ì—ì„œ HOLDê°€ ì•„ë‹Œ ì•¡ì…˜
                return -0.1
            
            # RSI ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ì—ì„œ ì—­ë°©í–¥ ì•¡ì…˜ í˜ë„í‹°
            rsi = state[2] if len(state) > 2 else 0.5  # ì¸ë±ìŠ¤ 2ëŠ” RSI
            if rsi > 0.8 and action == 0:  # ê³¼ë§¤ìˆ˜ì—ì„œ BUY
                return -0.15
            elif rsi < 0.2 and action == 1:  # ê³¼ë§¤ë„ì—ì„œ SELL
                return -0.15
            
            return 0.0
            
        except Exception as e:
            logger.debug(f"ë¦¬ìŠ¤í¬ í˜ë„í‹° ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _calculate_exploration_bonus(self, action: int) -> float:
        """íƒí—˜ ë³´ë„ˆìŠ¤ ê³„ì‚° (ì•¡ì…˜ ë‹¤ì–‘ì„± ì¥ë ¤)"""
        if not hasattr(self, 'recent_actions'):
            self.recent_actions = []
        
        self.recent_actions.append(action)
        
        # ìµœê·¼ 10ê°œ ì•¡ì…˜ë§Œ ìœ ì§€
        if len(self.recent_actions) > 10:
            self.recent_actions.pop(0)
        
        # ì•¡ì…˜ ë‹¤ì–‘ì„± ê³„ì‚°
        if len(self.recent_actions) >= 5:
            unique_actions = len(set(self.recent_actions[-5:]))
            if unique_actions >= 3:  # 3ê°€ì§€ ì´ìƒ ì•¡ì…˜ ì‚¬ìš©
                return 0.05
            elif unique_actions == 1:  # ê°™ì€ ì•¡ì…˜ë§Œ ë°˜ë³µ
                return -0.03
        
        return 0.0
    
    def _calculate_dynamic_weights(self) -> Dict[str, float]:
        """Rule ì „ëµ ëŒ€ë¹„ ì„±ê³¼ ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° (ì‹¤ì œ ë°ì´í„° í™œìš©)"""
        try:
            # 1. Rule ì „ëµ ì„±ê³¼ ë°ì´í„° ìˆ˜ì§‘ ì‹œë„
            rule_avg_performance = self._get_rule_performance_benchmark()
            
            # 2. PPO ìì²´ ì„±ê³¼ ê³„ì‚°
            ppo_performance = self._calculate_ppo_performance_score()
            
            # 3. ì„±ê³¼ ë¹„êµ ë¹„ìœ¨ ê³„ì‚°
            performance_ratio = ppo_performance / rule_avg_performance if rule_avg_performance > 0 else 1.0
            
            # 4. ìµœê·¼ ì„±ê³¼ íŠ¸ë Œë“œ ë¶„ì„
            recent_trend = self._analyze_recent_performance_trend()
            
            # 5. ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì • (ì„±ê³¼ ê¸°ë°˜ + íŠ¸ë Œë“œ ê¸°ë°˜)
            base_weights = self._get_base_weights(performance_ratio)
            trend_adjusted_weights = self._apply_trend_adjustment(base_weights, recent_trend, performance_ratio)
            
            # 6. í•™ìŠµ ë‹¨ê³„ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •
            learning_adjusted_weights = self._apply_learning_stage_adjustment(trend_adjusted_weights)
            
            return learning_adjusted_weights
            
        except Exception as e:
            logger.debug(f"ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return self._get_fallback_weights()
    
    def _get_rule_performance_benchmark(self) -> float:
        """Rule ì „ëµ ì„±ê³¼ ë²¤ì¹˜ë§ˆí¬ ìˆ˜ì§‘"""
        try:
            # ì™¸ë¶€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ Rule ì„±ê³¼ ê°€ì ¸ì˜¤ê¸° ì‹œë„
            if hasattr(self, 'external_rule_performance'):
                return self.external_rule_performance
            
            # ê¸€ë¡œë²Œ ì„±ê³¼ ë©”íŠ¸ë¦­ ì ‘ê·¼ ì‹œë„ (vps_strategy_adapterì™€ì˜ ì—°ë™)
            import sys
            current_module = sys.modules.get(__name__)
            if hasattr(current_module, 'global_rule_performance'):
                return getattr(current_module, 'global_rule_performance')
            
            # Profit Factor ê¸°ë°˜ ì¶”ì •
            if hasattr(self, 'profit_factor') and self.total_trades > 5:
                # Rule ì „ëµë“¤ì˜ ì¼ë°˜ì ì¸ Profit Factor ë²”ìœ„: 1.0~2.5
                estimated_rule_pf = 1.5  # í‰ê· ì ì¸ Rule ì „ëµ PF
                ppo_pf = min(self.profit_factor, 5.0) if self.profit_factor != float('inf') else 2.0
                
                # PFë¥¼ 0~1 ì„±ê³¼ ì ìˆ˜ë¡œ ë³€í™˜
                rule_performance = min(1.0, (estimated_rule_pf - 1.0) / 1.5)  # 1.0~2.5 PFë¥¼ 0~1ë¡œ
                return max(0.3, min(0.9, rule_performance))
            
            # ê¸°ë³¸ê°’: ì¤‘ê°„ ìˆ˜ì¤€ ì„±ê³¼
            return 0.6
            
        except Exception as e:
            logger.debug(f"Rule ì„±ê³¼ ë²¤ì¹˜ë§ˆí¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return 0.6
    
    def _calculate_ppo_performance_score(self) -> float:
        """PPO ì„±ê³¼ ì ìˆ˜ ê³„ì‚° (ë‹¤ì°¨ì› í‰ê°€)"""
        try:
            if self.ppo_predictions == 0:
                return 0.5  # ì¤‘ë¦½
            
            # 1. ê¸°ë³¸ ì„±ê³µë¥ 
            success_rate = self.ppo_successes / self.ppo_predictions
            
            # 2. ìµœê·¼ ì‹ ë¢°ë„ í‰ê· 
            recent_confidence = np.mean(self.model_confidence_history) if self.model_confidence_history else 0.5
            
            # 3. ë³´ìƒ íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì„±ê³¼
            reward_performance = 0.5
            if hasattr(self, 'reward_history') and self.reward_history:
                recent_rewards = [r['final_reward'] for r in self.reward_history[-10:]]
                positive_ratio = sum(1 for r in recent_rewards if r > 0) / len(recent_rewards)
                avg_reward = np.mean(recent_rewards)
                reward_performance = (positive_ratio * 0.6) + (min(1.0, max(0.0, avg_reward + 0.5)) * 0.4)
            
            # 4. Profit Factor ê¸°ë°˜ ì„±ê³¼ (Rule ì „ëµê³¼ ë™ì¼í•œ ê¸°ì¤€)
            pf_performance = 0.5
            if hasattr(self, 'profit_factor') and self.total_trades > 0:
                if self.profit_factor == float('inf'):
                    pf_performance = 1.0
                elif self.profit_factor > 2.0:
                    pf_performance = 1.0
                elif self.profit_factor > 1.5:
                    pf_performance = 0.8 + (self.profit_factor - 1.5) * 0.4
                elif self.profit_factor > 1.0:
                    pf_performance = 0.5 + (self.profit_factor - 1.0) * 0.6
                else:
                    pf_performance = max(0.0, self.profit_factor * 0.5)
            
            # 5. ì¢…í•© ì„±ê³¼ ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
            composite_score = (
                success_rate * 0.25 +           # ì˜ˆì¸¡ ì„±ê³µë¥  25%
                recent_confidence * 0.20 +      # ì‹ ë¢°ë„ 20%
                reward_performance * 0.30 +     # ë³´ìƒ ì„±ê³¼ 30%
                pf_performance * 0.25           # Profit Factor ì„±ê³¼ 25%
            )
            
            return min(1.0, max(0.0, composite_score))
            
        except Exception as e:
            logger.debug(f"PPO ì„±ê³¼ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _analyze_recent_performance_trend(self) -> Dict[str, float]:
        """ìµœê·¼ ì„±ê³¼ íŠ¸ë Œë“œ ë¶„ì„"""
        try:
            trend_data = {
                'direction': 0.0,  # -1(í•˜ë½) ~ 1(ìƒìŠ¹)
                'strength': 0.0,   # 0(ì•½í•¨) ~ 1(ê°•í•¨)
                'stability': 0.0   # 0(ë¶ˆì•ˆì •) ~ 1(ì•ˆì •)
            }
            
            if not hasattr(self, 'reward_history') or len(self.reward_history) < 5:
                return trend_data
            
            # ìµœê·¼ ë³´ìƒ ë°ì´í„°
            recent_rewards = [r['final_reward'] for r in self.reward_history[-10:]]
            
            if len(recent_rewards) >= 5:
                # íŠ¸ë Œë“œ ë°©í–¥ ê³„ì‚° (ì„ í˜• íšŒê·€ ê¸°ìš¸ê¸°)
                x = np.arange(len(recent_rewards))
                slope = np.polyfit(x, recent_rewards, 1)[0]
                trend_data['direction'] = max(-1.0, min(1.0, slope * 10))  # ìŠ¤ì¼€ì¼ë§
                
                # íŠ¸ë Œë“œ ê°•ë„ ê³„ì‚° (RÂ² ê°’)
                correlation = np.corrcoef(x, recent_rewards)[0, 1] if len(recent_rewards) > 1 else 0
                trend_data['strength'] = abs(correlation)
                
                # ì•ˆì •ì„± ê³„ì‚° (ë³€ë™ì„±ì˜ ì—­ìˆ˜)
                volatility = np.std(recent_rewards)
                trend_data['stability'] = 1.0 / (1.0 + volatility * 5)  # ì •ê·œí™”
            
            return trend_data
            
        except Exception as e:
            logger.debug(f"ì„±ê³¼ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'direction': 0.0, 'strength': 0.0, 'stability': 0.0}
    
    def _get_base_weights(self, performance_ratio: float) -> Dict[str, float]:
        """ì„±ê³¼ ë¹„ìœ¨ ê¸°ë°˜ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        if performance_ratio > 1.3:  # PPOê°€ Ruleë³´ë‹¤ 30% ì´ìƒ ì¢‹ìŒ
            return {
                'base': 0.50,      # ì „ëµ ì ìˆ˜ ì‹ ë¢°ë„ ìµœëŒ€
                'market': 0.30,    # ì‹œì¥ ê²°ê³¼ ì˜ì¡´ë„ ìµœì†Œ
                'consistency': 0.12,
                'risk': 0.05,
                'exploration': 0.03
            }
        elif performance_ratio > 1.1:  # PPOê°€ Ruleë³´ë‹¤ 10% ì´ìƒ ì¢‹ìŒ
            return {
                'base': 0.45,
                'market': 0.35,
                'consistency': 0.12,
                'risk': 0.05,
                'exploration': 0.03
            }
        elif performance_ratio > 0.9:  # ë¹„ìŠ·í•œ ì„±ê³¼
            return {
                'base': 0.35,
                'market': 0.45,
                'consistency': 0.13,
                'risk': 0.04,
                'exploration': 0.03
            }
        elif performance_ratio > 0.7:  # PPOê°€ Ruleë³´ë‹¤ ì•½ê°„ ë‚˜ì¨
            return {
                'base': 0.28,
                'market': 0.52,
                'consistency': 0.15,
                'risk': 0.03,
                'exploration': 0.02
            }
        else:  # PPOê°€ Ruleë³´ë‹¤ 30% ì´ìƒ ë‚˜ì¨
            return {
                'base': 0.20,      # ì „ëµ ì ìˆ˜ ì‹ ë¢°ë„ ìµœì†Œ
                'market': 0.60,    # ì‹œì¥ ê²°ê³¼ ì˜ì¡´ë„ ìµœëŒ€
                'consistency': 0.15,
                'risk': 0.03,
                'exploration': 0.02
            }
    
    def _apply_trend_adjustment(self, base_weights: Dict[str, float], 
                              trend: Dict[str, float], performance_ratio: float) -> Dict[str, float]:
        """íŠ¸ë Œë“œ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •"""
        try:
            adjusted_weights = base_weights.copy()
            
            # ìƒìŠ¹ íŠ¸ë Œë“œì¼ ë•Œ
            if trend['direction'] > 0.3 and trend['strength'] > 0.5:
                # ì „ëµ ì ìˆ˜ ê°€ì¤‘ì¹˜ ì¦ê°€ (ëª¨ë¸ì´ í•™ìŠµí•˜ê³  ìˆìŒ)
                boost = min(0.1, trend['direction'] * trend['strength'] * 0.2)
                adjusted_weights['base'] = min(0.6, adjusted_weights['base'] + boost)
                adjusted_weights['market'] = max(0.2, adjusted_weights['market'] - boost)
            
            # í•˜ë½ íŠ¸ë Œë“œì¼ ë•Œ
            elif trend['direction'] < -0.3 and trend['strength'] > 0.5:
                # ì‹œì¥ ê²°ê³¼ ê°€ì¤‘ì¹˜ ì¦ê°€ (ëª¨ë¸ ì„±ê³¼ í•˜ë½)
                penalty = min(0.1, abs(trend['direction']) * trend['strength'] * 0.2)
                adjusted_weights['market'] = min(0.7, adjusted_weights['market'] + penalty)
                adjusted_weights['base'] = max(0.15, adjusted_weights['base'] - penalty)
            
            # ë¶ˆì•ˆì •í•œ ì„±ê³¼ì¼ ë•Œ
            if trend['stability'] < 0.3:
                # íƒí—˜ ë³´ë„ˆìŠ¤ ì¦ê°€ (ë” ë§ì€ íƒí—˜ í•„ìš”)
                adjusted_weights['exploration'] = min(0.1, adjusted_weights['exploration'] + 0.02)
                adjusted_weights['base'] = max(0.15, adjusted_weights['base'] - 0.02)
            
            return adjusted_weights
            
        except Exception as e:
            logger.debug(f"íŠ¸ë Œë“œ ì¡°ì • ì‹¤íŒ¨: {e}")
            return base_weights
    
    def _apply_learning_stage_adjustment(self, weights: Dict[str, float]) -> Dict[str, float]:
        """í•™ìŠµ ë‹¨ê³„ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •"""
        try:
            adjusted_weights = weights.copy()
            
            # ì´ˆê¸° í•™ìŠµ ë‹¨ê³„ (ì˜ˆì¸¡ ìˆ˜ < 50)
            if self.ppo_predictions < 50:
                # íƒí—˜ê³¼ ì¼ê´€ì„±ì„ ë” ì¤‘ì‹œ
                adjusted_weights['exploration'] = min(0.15, adjusted_weights['exploration'] + 0.05)
                adjusted_weights['consistency'] = min(0.2, adjusted_weights['consistency'] + 0.03)
                adjusted_weights['base'] = max(0.15, adjusted_weights['base'] - 0.04)
                adjusted_weights['market'] = max(0.2, adjusted_weights['market'] - 0.04)
            
            # ì¤‘ê°„ í•™ìŠµ ë‹¨ê³„ (50 <= ì˜ˆì¸¡ ìˆ˜ < 200)
            elif self.ppo_predictions < 200:
                # ê· í˜•ì¡íŒ í•™ìŠµ
                pass  # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ìœ ì§€
            
            # ì„±ìˆ™ í•™ìŠµ ë‹¨ê³„ (ì˜ˆì¸¡ ìˆ˜ >= 200)
            else:
                # ì „ëµ ì ìˆ˜ ì‹ ë¢°ë„ ì¦ê°€
                if self.ppo_successes / self.ppo_predictions > 0.6:  # ì„±ê³¼ê°€ ì¢‹ë‹¤ë©´
                    adjusted_weights['base'] = min(0.55, adjusted_weights['base'] + 0.05)
                    adjusted_weights['market'] = max(0.25, adjusted_weights['market'] - 0.05)
            
            # ê°€ì¤‘ì¹˜ ì •ê·œí™” (í•©ê³„ê°€ 1.0ì´ ë˜ë„ë¡)
            total = sum(adjusted_weights.values())
            if total > 0:
                for key in adjusted_weights:
                    adjusted_weights[key] /= total
            
            return adjusted_weights
            
        except Exception as e:
            logger.debug(f"í•™ìŠµ ë‹¨ê³„ ì¡°ì • ì‹¤íŒ¨: {e}")
            return weights
    
    def _get_fallback_weights(self) -> Dict[str, float]:
        """í´ë°± ê°€ì¤‘ì¹˜ ë°˜í™˜"""
        return {
            'base': 0.35,
            'market': 0.45,
            'consistency': 0.13,
            'risk': 0.04,
            'exploration': 0.03
        }
    
    def set_external_rule_performance(self, rule_performance: float):
        """ì™¸ë¶€ì—ì„œ Rule ì „ëµ ì„±ê³¼ ì„¤ì • (vps_strategy_adapterì—ì„œ í˜¸ì¶œ)"""
        self.external_rule_performance = max(0.1, min(1.0, rule_performance))
    
    def _adaptive_reward_scaling(self, reward: float, strategy_score: float, confidence: float) -> float:
        """ì ì‘í˜• ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ë° í´ë¦¬í•‘"""
        try:
            # 1. ì‹ ë¢°ë„ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
            confidence_scale = 0.5 + (confidence * 0.5)  # 0.5~1.0 ë²”ìœ„
            scaled_reward = reward * confidence_scale
            
            # 2. ì „ëµ ì ìˆ˜ ê¸°ë°˜ ì¦í­/ê°ì‡ 
            if strategy_score > 0.8:
                scaled_reward *= 1.1  # ê³ ì ìˆ˜ ì‹œ ì¦í­
            elif strategy_score < 0.3:
                scaled_reward *= 0.8  # ì €ì ìˆ˜ ì‹œ ê°ì‡ 
            
            # 3. ì ì‘í˜• í´ë¦¬í•‘ (ì ˆëŒ€ê°’ ê¸°ì¤€)
            max_reward = 0.9 if confidence > 0.7 else 0.7
            min_reward = -max_reward
            
            # 4. ë¶€ë“œëŸ¬ìš´ í´ë¦¬í•‘ (tanh í•¨ìˆ˜ ì‚¬ìš©)
            final_reward = max_reward * np.tanh(scaled_reward / max_reward)
            
            return final_reward
            
        except Exception as e:
            logger.debug(f"ì ì‘í˜• ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ì˜¤ë¥˜: {e}")
            return max(-1.0, min(1.0, reward))
    
    def add_enhanced_training_experience(self, state: np.ndarray, action: int, reward: float, 
                                       next_state: np.ndarray, done: bool, metadata: Dict[str, Any]):
        """í–¥ìƒëœ íŠ¸ë ˆì´ë‹ ê²½í—˜ ì¶”ê°€ (ë©”íƒ€ë°ì´í„° í¬í•¨)"""
        try:
            # ê¸°ë³¸ ê²½í—˜ ì¶”ê°€
            self.add_training_experience(state, action, reward, next_state, done)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥ (í–¥í›„ ë¶„ì„ìš©)
            if not hasattr(self, 'experience_metadata'):
                self.experience_metadata = []
            
            self.experience_metadata.append(metadata)
            
            # ìµœê·¼ 100ê°œë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
            if len(self.experience_metadata) > 100:
                self.experience_metadata.pop(0)
            
        except Exception as e:
            logger.error(f"í–¥ìƒëœ ê²½í—˜ ì¶”ê°€ ì‹¤íŒ¨: {e}")
    
    def _update_performance_tracking(self, final_reward: float, strategy_score: float, 
                                   market_outcome: float, confidence: float):
        """ì„±ê³¼ ì¶”ì  ì—…ë°ì´íŠ¸"""
        try:
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.ppo_predictions += 1
            if final_reward > 0:
                self.ppo_successes += 1
            
            # ì‹ ë¢°ë„ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.model_confidence_history.append(confidence)
            if len(self.model_confidence_history) > 50:
                self.model_confidence_history.pop(0)
            
            # ë³´ìƒ íˆìŠ¤í† ë¦¬ ì¶”ê°€
            if not hasattr(self, 'reward_history'):
                self.reward_history = []
            
            self.reward_history.append({
                'final_reward': final_reward,
                'strategy_score': strategy_score,
                'market_outcome': market_outcome,
                'confidence': confidence,
                'timestamp': datetime.now()
            })
            
            # ìµœê·¼ 50ê°œë§Œ ìœ ì§€
            if len(self.reward_history) > 50:
                self.reward_history.pop(0)
            
        except Exception as e:
            logger.error(f"ì„±ê³¼ ì¶”ì  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _log_reward_shaping_details(self, metadata: Dict[str, Any]):
        """ë³´ìƒ shaping ìƒì„¸ ë¡œê¹…"""
        try:
            # PPO ì ìˆ˜ ë¡œê±° ì‚¬ìš©
            from trading.ppo_score_logger import get_ppo_score_logger
            ppo_logger = get_ppo_score_logger()
            
            # ìƒì„¸ ë¡œê¹…
            ppo_logger.log_score(
                strategy_score=metadata['strategy_score'],
                confidence=metadata['confidence'],
                action=metadata['action_str'],
                market_outcome=metadata['market_outcome'],
                final_reward=metadata['final_reward'],
                selected=True,
                total_predictions=self.ppo_predictions,
                success_rate=self.ppo_successes / self.ppo_predictions if self.ppo_predictions > 0 else 0.0
            )
            
            # ìƒì„¸ ì»´í¬ë„ŒíŠ¸ ë¡œê¹… (ë””ë²„ê·¸)
            components = metadata['reward_components']
            logger.debug(
                f"ğŸ¯ PPO ë³´ìƒ Shaping: "
                f"ìµœì¢…={metadata['final_reward']:.4f} "
                f"[ê¸°ë³¸={components['base_reward']:.3f}, "
                f"ì‹œì¥={components['market_reward']:.3f}, "
                f"ì¼ê´€ì„±={components['consistency_reward']:.3f}, "
                f"ë¦¬ìŠ¤í¬={components['risk_penalty']:.3f}, "
                f"íƒí—˜={components['exploration_bonus']:.3f}] "
                f"ì „ëµì ìˆ˜={metadata['strategy_score']:.3f} "
                f"ì‹ ë¢°ë„={metadata['confidence']:.3f}"
            )
            
        except ImportError:
            logger.warning("PPO ì ìˆ˜ ë¡œê±°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        except Exception as e:
            logger.error(f"ë³´ìƒ shaping ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    def get_reward_shaping_statistics(self) -> Dict[str, Any]:
        """ë³´ìƒ shaping í†µê³„ ë°˜í™˜"""
        try:
            stats = {
                'total_rewards_shaped': len(getattr(self, 'reward_history', [])),
                'average_reward': 0.0,
                'reward_variance': 0.0,
                'positive_reward_ratio': 0.0,
                'recent_confidence_avg': 0.0,
                'component_averages': {}
            }
            
            if hasattr(self, 'reward_history') and self.reward_history:
                rewards = [r['final_reward'] for r in self.reward_history]
                confidences = [r['confidence'] for r in self.reward_history]
                
                stats['average_reward'] = np.mean(rewards)
                stats['reward_variance'] = np.var(rewards)
                stats['positive_reward_ratio'] = sum(1 for r in rewards if r > 0) / len(rewards)
                stats['recent_confidence_avg'] = np.mean(confidences)
            
            if hasattr(self, 'experience_metadata') and self.experience_metadata:
                # ì»´í¬ë„ŒíŠ¸ë³„ í‰ê·  ê³„ì‚°
                components = ['base_reward', 'market_reward', 'consistency_reward', 'risk_penalty', 'exploration_bonus']
                for comp in components:
                    values = [m['reward_components'][comp] for m in self.experience_metadata if comp in m.get('reward_components', {})]
                    if values:
                        stats['component_averages'][comp] = np.mean(values)
            
            return stats
            
        except Exception as e:
            logger.error(f"ë³´ìƒ shaping í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def train_model(self, episodes: int = 1000) -> bool:
        """ëª¨ë¸ íŠ¸ë ˆì´ë‹ ì‹¤í–‰ (Trainer ì‚¬ìš©)"""
        if not self.trainer or not PPO_MODULES_AVAILABLE:
            logger.warning("Trainer not available for training")
            return False
            
        try:
            logger.info(f"Starting PPO training for {episodes} episodes")
            self.trainer.train(episodes)
            
            # ìƒˆë¡œìš´ ëª¨ë¸ë¡œ Agent ì—…ë°ì´íŠ¸
            if self.agent:
                self.agent.load_model(self.config.model_path)
                self.model = self.agent.model
                logger.info("Agent updated with newly trained model")
            
            return True
        except Exception as e:
            logger.error(f"Training failed: {e}")
            return False
    
    def save_model(self, path: str = None) -> bool:
        """ëª¨ë¸ ì €ì¥"""
        if not self.agent or not PPO_MODULES_AVAILABLE:
            logger.warning("Agent not available for saving")
            return False
            
        try:
            save_path = path or self.config.model_path
            success = self.agent.save_model(save_path)
            if success:
                logger.info(f"Model saved to {save_path}")
            return success
        except Exception as e:
            logger.error(f"Failed to save model: {e}")
            return False
    
    def evaluate_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """PPO ì „ëµ ê²°ê³¼ í‰ê°€ (Rule ì „ëµê³¼ í˜¸í™˜ì„±)"""
        try:
            # ê¸°ë³¸ í‰ê°€ ì •ë³´
            evaluation = {
                'strategy_name': 'PPOStrategy',
                'timestamp': datetime.now().isoformat(),
                'ppo_specific': True
            }
            
            # ê±°ë˜ ê²°ê³¼ ì •ë³´ ì¶”ì¶œ
            pnl = result.get('pnl', 0.0)
            entry_price = result.get('entry_price', 0.0)
            exit_price = result.get('exit_price', 0.0)
            side = result.get('side', 'LONG')
            
            # ì„±ê³¼ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.add_market_outcome_ppo(pnl)
            
            # í‰ê°€ ê²°ê³¼
            evaluation.update({
                'pnl': pnl,
                'profitable': pnl > 0,
                'entry_price': entry_price,
                'exit_price': exit_price,
                'side': side,
                'total_trades': self.total_trades,
                'winning_trades': self.winning_trades,
                'losing_trades': self.losing_trades,
                'win_rate': self.winning_trades / self.total_trades if self.total_trades > 0 else 0.0,
                'profit_factor': self.profit_factor,
                'total_pnl': self.total_pnl,
                'ppo_predictions': self.ppo_predictions,
                'ppo_successes': self.ppo_successes,
                'ppo_success_rate': self.ppo_successes / self.ppo_predictions if self.ppo_predictions > 0 else 0.0,
                'model_loaded': self.model_loaded,
                'agent_ready': self.agent.is_ready() if self.agent else False
            })
            
            logger.info(f"PPO ì „ëµ ê²°ê³¼ í‰ê°€ ì™„ë£Œ: PnL={pnl:.3f}, "
                       f"Win Rate={evaluation['win_rate']:.3f}, "
                       f"PPO Success Rate={evaluation['ppo_success_rate']:.3f}")
            
            return evaluation
            
        except Exception as e:
            logger.error(f"PPO evaluate_result ì‹¤íŒ¨: {e}")
            return {
                'strategy_name': 'PPOStrategy',
                'timestamp': datetime.now().isoformat(),
                'error': str(e),
                'ppo_specific': True
            }
    
    def add_market_outcome_ppo(self, outcome: float) -> None:
        """ì‹œì¥ ê²°ê³¼ í”¼ë“œë°± ì¶”ê°€ (BaseRuleStrategy í˜¸í™˜)"""
        try:
            self.total_trades += 1
            self.total_pnl += outcome
            
            if outcome > 0:
                self.winning_trades += 1
                self.total_profit += outcome
            else:
                self.losing_trades += 1
                self.total_loss += abs(outcome)
            
            # Profit factor ì—…ë°ì´íŠ¸
            if self.total_loss > 0:
                self.profit_factor = self.total_profit / self.total_loss
            else:
                self.profit_factor = float('inf') if self.total_profit > 0 else 1.0
            
            win_rate = self.winning_trades / self.total_trades if self.total_trades > 0 else 0.0
            
            logger.debug(f"PPO ì„±ê³¼ ì—…ë°ì´íŠ¸: outcome={outcome:.3f}, "
                        f"win_rate={win_rate:.3f}, profit_factor={self.profit_factor:.3f}")
            
        except Exception as e:
            logger.error(f"PPO ì‹œì¥ ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª PPOStrategy í…ŒìŠ¤íŠ¸")
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    dates = pd.date_range('2024-01-01', periods=100, freq='5min')
    price_data = pd.DataFrame({
        'timestamp': dates,
        'open': np.random.randn(100).cumsum() + 50000,
        'high': np.random.randn(100).cumsum() + 50200,
        'low': np.random.randn(100).cumsum() + 49800,
        'close': np.random.randn(100).cumsum() + 50000,
        'volume': np.random.randint(100, 1000, 100)
    })
    
    # PPO ì „ëµ í…ŒìŠ¤íŠ¸
    config = PPOConfig(
        model_path="/tmp/test_ppo.zip",  # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ë¡œ
        confidence_threshold=0.6
    )
    
    strategy = PPOStrategy(config)
    
    # ì‹ í˜¸ ìƒì„± í…ŒìŠ¤íŠ¸
    signal = strategy.generate_signal(price_data)
    print(f"ìƒì„±ëœ ì‹ í˜¸: {signal}")
    
    # ì ìˆ˜ ê³„ì‚° í…ŒìŠ¤íŠ¸
    score, details = strategy.score(price_data)
    print(f"ì „ëµ ì ìˆ˜: {score:.3f}")
    print(f"ìƒì„¸ ì ìˆ˜: {details}")
    
    # í†µê³„ í™•ì¸
    stats = strategy.get_ppo_statistics()
    print(f"PPO í†µê³„: {stats}")
    
    # Agent í…ŒìŠ¤íŠ¸
    if strategy.agent:
        print(f"Agent ì¤€ë¹„ìƒíƒœ: {strategy.agent.is_ready()}")
        if strategy.agent.is_ready():
            # ë”ë¯¸ íŠ¹ì„±ìœ¼ë¡œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
            dummy_features = np.random.randn(26).astype(np.float32)
            result = strategy.agent.predict(dummy_features)
            if result:
                print(f"Agent ì˜ˆì¸¡ ê²°ê³¼: action={result.action}, confidence={result.confidence}")
    
    # Trainer í…ŒìŠ¤íŠ¸
    if strategy.trainer:
        print(f"Trainer ì‚¬ìš©ê°€ëŠ¥: True")
        # ë”ë¯¸ ê²½í—˜ ì¶”ê°€
        dummy_state = np.random.randn(26).astype(np.float32)
        dummy_next_state = np.random.randn(26).astype(np.float32)
        strategy.add_training_experience(dummy_state, 1, 0.1, dummy_next_state, False)
        print("ë”ë¯¸ íŠ¸ë ˆì´ë‹ ê²½í—˜ ì¶”ê°€ ì™„ë£Œ")
    else:
        print(f"Trainer ì‚¬ìš©ê°€ëŠ¥: False")