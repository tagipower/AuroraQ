#!/usr/bin/env python3
"""
ëª¨ë¸ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
P4: ëª¨ë¸ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ë° Fine-tuning ì‹œìŠ¤í…œ êµ¬ì¶•
"""

import sys
import os
import json
import time
import logging
import asyncio
import sqlite3
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum
import numpy as np
import pandas as pd
from collections import deque, defaultdict
import threading
import warnings

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

warnings.filterwarnings('ignore', category=UserWarning)
logger = logging.getLogger(__name__)

class ModelType(Enum):
    """ëª¨ë¸ íƒ€ì…"""
    FINBERT = "finbert"
    PPO = "ppo"
    CUSTOM = "custom"

class QualityMetric(Enum):
    """í’ˆì§ˆ ë©”íŠ¸ë¦­ íƒ€ì…"""
    ACCURACY = "accuracy"
    PRECISION = "precision" 
    RECALL = "recall"
    F1_SCORE = "f1_score"
    CONFIDENCE = "confidence"
    LATENCY = "latency"
    MEMORY_USAGE = "memory_usage"
    PREDICTION_DRIFT = "prediction_drift"
    BIAS_SCORE = "bias_score"
    STABILITY = "stability"

class QualityStatus(Enum):
    """í’ˆì§ˆ ìƒíƒœ"""
    EXCELLENT = "excellent"  # 95%+
    GOOD = "good"           # 85-95%
    ACCEPTABLE = "acceptable" # 70-85%
    POOR = "poor"           # 50-70%
    CRITICAL = "critical"   # <50%

@dataclass
class QualityThreshold:
    """í’ˆì§ˆ ì„ê³„ê°’ ì„¤ì •"""
    excellent: float = 0.95
    good: float = 0.85
    acceptable: float = 0.70
    poor: float = 0.50
    
    def get_status(self, value: float) -> QualityStatus:
        """ê°’ì— ë”°ë¥¸ ìƒíƒœ ë°˜í™˜"""
        if value >= self.excellent:
            return QualityStatus.EXCELLENT
        elif value >= self.good:
            return QualityStatus.GOOD
        elif value >= self.acceptable:
            return QualityStatus.ACCEPTABLE
        elif value >= self.poor:
            return QualityStatus.POOR
        else:
            return QualityStatus.CRITICAL

@dataclass
class ModelPrediction:
    """ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼"""
    model_type: ModelType
    timestamp: datetime
    input_data: str  # ì…ë ¥ ë°ì´í„° ìš”ì•½
    prediction: Any  # ì˜ˆì¸¡ ê²°ê³¼
    confidence: float = 0.0
    latency_ms: float = 0.0
    memory_mb: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            'model_type': self.model_type.value,
            'timestamp': self.timestamp.isoformat(),
            'input_data': self.input_data,
            'prediction': str(self.prediction),
            'confidence': self.confidence,
            'latency_ms': self.latency_ms,
            'memory_mb': self.memory_mb,
            'metadata': self.metadata
        }

@dataclass
class QualityReport:
    """í’ˆì§ˆ ë³´ê³ ì„œ"""
    model_type: ModelType
    timestamp: datetime
    metrics: Dict[QualityMetric, float]
    status: QualityStatus
    recommendations: List[str] = field(default_factory=list)
    issues: List[str] = field(default_factory=list)
    summary: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            'model_type': self.model_type.value,
            'timestamp': self.timestamp.isoformat(),
            'metrics': {k.value: v for k, v in self.metrics.items()},
            'status': self.status.value,
            'recommendations': self.recommendations,
            'issues': self.issues,
            'summary': self.summary
        }

class ModelQualityDatabase:
    """ëª¨ë¸ í’ˆì§ˆ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì"""
    
    def __init__(self, db_path: str = "model_quality.db"):
        self.db_path = db_path
        self._init_database()
        self._lock = threading.RLock()
    
    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.executescript("""
                    CREATE TABLE IF NOT EXISTS predictions (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        model_type TEXT NOT NULL,
                        timestamp TEXT NOT NULL,
                        input_data TEXT,
                        prediction TEXT,
                        confidence REAL,
                        latency_ms REAL,
                        memory_mb REAL,
                        metadata TEXT
                    );
                    
                    CREATE TABLE IF NOT EXISTS quality_reports (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        model_type TEXT NOT NULL,
                        timestamp TEXT NOT NULL,
                        metrics TEXT NOT NULL,
                        status TEXT NOT NULL,
                        recommendations TEXT,
                        issues TEXT,
                        summary TEXT
                    );
                    
                    CREATE INDEX IF NOT EXISTS idx_predictions_model_time 
                    ON predictions(model_type, timestamp);
                    
                    CREATE INDEX IF NOT EXISTS idx_reports_model_time 
                    ON quality_reports(model_type, timestamp);
                """)
                logger.info(f"Model quality database initialized: {self.db_path}")
        except Exception as e:
            logger.error(f"Failed to initialize database: {e}")
            raise
    
    def save_prediction(self, prediction: ModelPrediction) -> bool:
        """ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥"""
        try:
            with self._lock:
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute("""
                        INSERT INTO predictions 
                        (model_type, timestamp, input_data, prediction, confidence, 
                         latency_ms, memory_mb, metadata)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        prediction.model_type.value,
                        prediction.timestamp.isoformat(),
                        prediction.input_data,
                        str(prediction.prediction),
                        prediction.confidence,
                        prediction.latency_ms,
                        prediction.memory_mb,
                        json.dumps(prediction.metadata)
                    ))
                return True
        except Exception as e:
            logger.error(f"Failed to save prediction: {e}")
            return False
    
    def save_quality_report(self, report: QualityReport) -> bool:
        """í’ˆì§ˆ ë³´ê³ ì„œ ì €ì¥"""
        try:
            with self._lock:
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute("""
                        INSERT INTO quality_reports 
                        (model_type, timestamp, metrics, status, recommendations, issues, summary)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        report.model_type.value,
                        report.timestamp.isoformat(),
                        json.dumps({k.value: v for k, v in report.metrics.items()}),
                        report.status.value,
                        json.dumps(report.recommendations),
                        json.dumps(report.issues),
                        report.summary
                    ))
                return True
        except Exception as e:
            logger.error(f"Failed to save quality report: {e}")
            return False
    
    def get_recent_predictions(self, model_type: ModelType, 
                             hours: int = 24, limit: int = 1000) -> List[Dict[str, Any]]:
        """ìµœê·¼ ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ"""
        try:
            with self._lock:
                with sqlite3.connect(self.db_path) as conn:
                    conn.row_factory = sqlite3.Row
                    cutoff_time = (datetime.now() - timedelta(hours=hours)).isoformat()
                    
                    cursor = conn.execute("""
                        SELECT * FROM predictions 
                        WHERE model_type = ? AND timestamp >= ?
                        ORDER BY timestamp DESC LIMIT ?
                    """, (model_type.value, cutoff_time, limit))
                    
                    results = []
                    for row in cursor:
                        result = dict(row)
                        result['metadata'] = json.loads(result['metadata'] or '{}')
                        results.append(result)
                    
                    return results
        except Exception as e:
            logger.error(f"Failed to get recent predictions: {e}")
            return []
    
    def get_quality_history(self, model_type: ModelType, 
                           days: int = 7) -> List[Dict[str, Any]]:
        """í’ˆì§ˆ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        try:
            with self._lock:
                with sqlite3.connect(self.db_path) as conn:
                    conn.row_factory = sqlite3.Row
                    cutoff_time = (datetime.now() - timedelta(days=days)).isoformat()
                    
                    cursor = conn.execute("""
                        SELECT * FROM quality_reports 
                        WHERE model_type = ? AND timestamp >= ?
                        ORDER BY timestamp DESC
                    """, (model_type.value, cutoff_time))
                    
                    results = []
                    for row in cursor:
                        result = dict(row)
                        result['metrics'] = json.loads(result['metrics'])
                        result['recommendations'] = json.loads(result['recommendations'] or '[]')
                        result['issues'] = json.loads(result['issues'] or '[]')
                        results.append(result)
                    
                    return results
        except Exception as e:
            logger.error(f"Failed to get quality history: {e}")
            return []

class ModelQualityMonitor:
    """ëª¨ë¸ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, db_path: str = "model_quality.db"):
        self.db = ModelQualityDatabase(db_path)
        self.predictions_cache = defaultdict(lambda: deque(maxlen=1000))
        self.thresholds = {
            ModelType.FINBERT: {
                QualityMetric.ACCURACY: QualityThreshold(0.90, 0.80, 0.70, 0.60),
                QualityMetric.CONFIDENCE: QualityThreshold(0.85, 0.75, 0.65, 0.50),
                QualityMetric.LATENCY: QualityThreshold(100, 200, 500, 1000),  # ms (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
                QualityMetric.MEMORY_USAGE: QualityThreshold(500, 1000, 2000, 4000),  # MB (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            },
            ModelType.PPO: {
                QualityMetric.ACCURACY: QualityThreshold(0.85, 0.75, 0.65, 0.55),
                QualityMetric.CONFIDENCE: QualityThreshold(0.80, 0.70, 0.60, 0.50),
                QualityMetric.STABILITY: QualityThreshold(0.90, 0.80, 0.70, 0.60),
                QualityMetric.PREDICTION_DRIFT: QualityThreshold(0.05, 0.10, 0.20, 0.30),  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            }
        }
        
        # í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ
        self._monitoring_active = False
        self._monitor_thread = None
        
        logger.info("Model quality monitor initialized")
    
    def record_prediction(self, model_type: ModelType, input_data: str, 
                         prediction: Any, confidence: float = 0.0,
                         latency_ms: float = 0.0, memory_mb: float = 0.0,
                         metadata: Dict[str, Any] = None) -> bool:
        """ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë¡"""
        try:
            prediction_record = ModelPrediction(
                model_type=model_type,
                timestamp=datetime.now(),
                input_data=input_data[:200] if isinstance(input_data, str) else str(input_data)[:200],
                prediction=prediction,
                confidence=confidence,
                latency_ms=latency_ms,
                memory_mb=memory_mb,
                metadata=metadata or {}
            )
            
            # ìºì‹œì— ì¶”ê°€
            self.predictions_cache[model_type].append(prediction_record)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            return self.db.save_prediction(prediction_record)
            
        except Exception as e:
            logger.error(f"Failed to record prediction: {e}")
            return False
    
    def calculate_quality_metrics(self, model_type: ModelType, 
                                hours: int = 24) -> Dict[QualityMetric, float]:
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {}
        
        try:
            # ìµœê·¼ ì˜ˆì¸¡ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            predictions = self.db.get_recent_predictions(model_type, hours)
            
            if not predictions:
                logger.warning(f"No predictions found for {model_type.value} in last {hours} hours")
                return metrics
            
            # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
            confidences = [p['confidence'] for p in predictions if p['confidence'] > 0]
            latencies = [p['latency_ms'] for p in predictions if p['latency_ms'] > 0]
            memories = [p['memory_mb'] for p in predictions if p['memory_mb'] > 0]
            
            if confidences:
                metrics[QualityMetric.CONFIDENCE] = np.mean(confidences)
            
            if latencies:
                metrics[QualityMetric.LATENCY] = np.mean(latencies)
            
            if memories:
                metrics[QualityMetric.MEMORY_USAGE] = np.mean(memories)
            
            # ì•ˆì •ì„± ê³„ì‚° (confidence ë³€ë™ì„±)
            if len(confidences) > 1:
                confidence_std = np.std(confidences)
                metrics[QualityMetric.STABILITY] = max(0, 1.0 - (confidence_std * 2))
            
            # ì˜ˆì¸¡ ë“œë¦¬í”„íŠ¸ ê³„ì‚° (ì‹œê°„ì— ë”°ë¥¸ ë³€í™”)
            if len(confidences) >= 10:
                # ìµœê·¼ ì ˆë°˜ê³¼ ì´ì „ ì ˆë°˜ ë¹„êµ
                mid_point = len(confidences) // 2
                recent_avg = np.mean(confidences[:mid_point])
                older_avg = np.mean(confidences[mid_point:])
                drift = abs(recent_avg - older_avg) / max(older_avg, 0.01)
                metrics[QualityMetric.PREDICTION_DRIFT] = drift
            
            # ëª¨ë¸ë³„ íŠ¹í™” ë©”íŠ¸ë¦­
            if model_type == ModelType.FINBERT:
                # FinBERT íŠ¹í™” ë©”íŠ¸ë¦­ (ê°ì • ë¶„ì„ ì •í™•ë„ ì¶”ì •)
                positive_ratio = sum(1 for p in predictions 
                                   if 'sentiment' in str(p['prediction']).lower() 
                                   and 'positive' in str(p['prediction']).lower()) / len(predictions)
                # ê· í˜•ìˆëŠ” ì˜ˆì¸¡ì¸ì§€ í™•ì¸ (0.2-0.8 ë²”ìœ„ê°€ ê±´ê°•í•¨)
                balance_score = 1.0 - abs(positive_ratio - 0.5) * 2
                metrics[QualityMetric.ACCURACY] = max(0.5, balance_score * np.mean(confidences) if confidences else 0.5)
                
            elif model_type == ModelType.PPO:
                # PPO íŠ¹í™” ë©”íŠ¸ë¦­ (ì•¡ì…˜ ë¶„í¬ì™€ ì‹ ë¢°ë„)
                action_counts = defaultdict(int)
                for p in predictions:
                    pred_str = str(p['prediction']).upper()
                    if 'BUY' in pred_str:
                        action_counts['BUY'] += 1
                    elif 'SELL' in pred_str:
                        action_counts['SELL'] += 1
                    else:
                        action_counts['HOLD'] += 1
                
                # ì•¡ì…˜ ë‹¤ì–‘ì„± ì ìˆ˜ (ë„ˆë¬´ í¸í–¥ë˜ì§€ ì•Šì€ ê²ƒì´ ì¢‹ìŒ)
                total_actions = sum(action_counts.values())
                if total_actions > 0:
                    action_ratios = [count / total_actions for count in action_counts.values()]
                    diversity_score = 1.0 - max(action_ratios) + 0.3  # ìµœëŒ€ 70% í¸í–¥ê¹Œì§€ í—ˆìš©
                    metrics[QualityMetric.ACCURACY] = min(1.0, diversity_score)
            
            logger.debug(f"Calculated {len(metrics)} metrics for {model_type.value}")
            return metrics
            
        except Exception as e:
            logger.error(f"Failed to calculate quality metrics: {e}")
            return metrics
    
    def generate_quality_report(self, model_type: ModelType, 
                              hours: int = 24) -> QualityReport:
        """í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        try:
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = self.calculate_quality_metrics(model_type, hours)
            
            if not metrics:
                return QualityReport(
                    model_type=model_type,
                    timestamp=datetime.now(),
                    metrics={},
                    status=QualityStatus.CRITICAL,
                    issues=["No data available for quality assessment"],
                    summary="Insufficient data for quality analysis"
                )
            
            # ì „ì²´ ìƒíƒœ ê²°ì •
            thresholds = self.thresholds.get(model_type, {})
            status_scores = []
            issues = []
            recommendations = []
            
            for metric, value in metrics.items():
                threshold = thresholds.get(metric)
                if threshold:
                    # latencyì™€ memoryëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
                    if metric in [QualityMetric.LATENCY, QualityMetric.MEMORY_USAGE, QualityMetric.PREDICTION_DRIFT]:
                        # ì—­ì‚° ì ìˆ˜ ê³„ì‚°
                        if value <= threshold.excellent:
                            status = QualityStatus.EXCELLENT
                            score = 1.0
                        elif value <= threshold.good:
                            status = QualityStatus.GOOD  
                            score = 0.8
                        elif value <= threshold.acceptable:
                            status = QualityStatus.ACCEPTABLE
                            score = 0.6
                        elif value <= threshold.poor:
                            status = QualityStatus.POOR
                            score = 0.4
                        else:
                            status = QualityStatus.CRITICAL
                            score = 0.2
                    else:
                        status = threshold.get_status(value)
                        score = value
                    
                    status_scores.append(score)
                    
                    # ë¬¸ì œì ê³¼ ê¶Œì¥ì‚¬í•­ ìƒì„±
                    if status == QualityStatus.CRITICAL:
                        issues.append(f"{metric.value} is critical: {value:.3f}")
                        recommendations.append(f"Urgent: Address {metric.value} issues")
                    elif status == QualityStatus.POOR:
                        issues.append(f"{metric.value} is poor: {value:.3f}")
                        recommendations.append(f"Improve {metric.value}")
            
            # ì „ì²´ ìƒíƒœ ê²°ì • (í‰ê·  ì ìˆ˜ ê¸°ë°˜)
            if status_scores:
                avg_score = np.mean(status_scores)
                overall_threshold = QualityThreshold()
                overall_status = overall_threshold.get_status(avg_score)
            else:
                overall_status = QualityStatus.CRITICAL
                avg_score = 0.0
            
            # ê¶Œì¥ì‚¬í•­ ì¶”ê°€
            if not recommendations:
                if overall_status == QualityStatus.EXCELLENT:
                    recommendations.append("Model performance is excellent. Continue monitoring.")
                elif overall_status == QualityStatus.GOOD:
                    recommendations.append("Model performance is good. Monitor for consistency.")
                else:
                    recommendations.append("Consider model retraining or fine-tuning.")
            
            # ìš”ì•½ ìƒì„±
            prediction_count = len(self.db.get_recent_predictions(model_type, hours))
            summary = (f"{model_type.value} model quality: {overall_status.value} "
                      f"(score: {avg_score:.2f}) based on {prediction_count} predictions "
                      f"in last {hours} hours")
            
            report = QualityReport(
                model_type=model_type,
                timestamp=datetime.now(),
                metrics=metrics,
                status=overall_status,
                recommendations=recommendations,
                issues=issues,
                summary=summary
            )
            
            # ë³´ê³ ì„œ ì €ì¥
            self.db.save_quality_report(report)
            
            logger.info(f"Quality report generated for {model_type.value}: {overall_status.value}")
            return report
            
        except Exception as e:
            logger.error(f"Failed to generate quality report: {e}")
            return QualityReport(
                model_type=model_type,
                timestamp=datetime.now(),
                metrics={},
                status=QualityStatus.CRITICAL,
                issues=[f"Error generating report: {str(e)}"],
                summary="Failed to generate quality report"
            )
    
    def get_model_status(self, model_type: ModelType) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            # ìµœì‹  í’ˆì§ˆ ë³´ê³ ì„œ ì¡°íšŒ
            recent_reports = self.db.get_quality_history(model_type, days=1)
            latest_report = recent_reports[0] if recent_reports else None
            
            # ì˜ˆì¸¡ í†µê³„
            recent_predictions = self.db.get_recent_predictions(model_type, hours=24)
            prediction_count_24h = len(recent_predictions)
            prediction_count_1h = len([p for p in recent_predictions 
                                     if datetime.fromisoformat(p['timestamp']) > 
                                     datetime.now() - timedelta(hours=1)])
            
            status = {
                'model_type': model_type.value,
                'timestamp': datetime.now().isoformat(),
                'prediction_count_24h': prediction_count_24h,
                'prediction_count_1h': prediction_count_1h,
                'latest_report': latest_report,
                'monitoring_active': self._monitoring_active,
                'cache_size': len(self.predictions_cache[model_type])
            }
            
            if recent_predictions:
                latest_prediction = recent_predictions[0]
                status['latest_prediction_time'] = latest_prediction['timestamp']
                status['latest_confidence'] = latest_prediction['confidence']
            
            return status
            
        except Exception as e:
            logger.error(f"Failed to get model status: {e}")
            return {
                'model_type': model_type.value,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def start_monitoring(self, interval_minutes: int = 60):
        """í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self._monitoring_active:
            logger.warning("Monitoring already active")
            return
        
        self._monitoring_active = True
        
        def monitor_loop():
            while self._monitoring_active:
                try:
                    # ê° ëª¨ë¸ íƒ€ì…ì— ëŒ€í•´ í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±
                    for model_type in [ModelType.FINBERT, ModelType.PPO]:
                        report = self.generate_quality_report(model_type)
                        logger.info(f"Generated quality report for {model_type.value}: {report.status.value}")
                    
                    # ë‹¤ìŒ ì‹¤í–‰ê¹Œì§€ ëŒ€ê¸°
                    for _ in range(interval_minutes * 60):
                        if not self._monitoring_active:
                            break
                        time.sleep(1)
                        
                except Exception as e:
                    logger.error(f"Error in monitoring loop: {e}")
                    time.sleep(60)  # ì—ëŸ¬ ì‹œ 1ë¶„ í›„ ì¬ì‹œë„
        
        self._monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        self._monitor_thread.start()
        
        logger.info(f"Quality monitoring started (interval: {interval_minutes} minutes)")
    
    def stop_monitoring(self):
        """í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        if not self._monitoring_active:
            return
        
        self._monitoring_active = False
        if self._monitor_thread and self._monitor_thread.is_alive():
            self._monitor_thread.join(timeout=5)
        
        logger.info("Quality monitoring stopped")
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.stop_monitoring()
        self.predictions_cache.clear()
        logger.info("Model quality monitor cleanup completed")

# ì „ì—­ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
_global_monitor = None

def get_quality_monitor(db_path: str = None) -> ModelQualityMonitor:
    """ì „ì—­ í’ˆì§ˆ ëª¨ë‹ˆí„° ë°˜í™˜"""
    global _global_monitor
    if _global_monitor is None:
        _global_monitor = ModelQualityMonitor(db_path or "model_quality.db")
    return _global_monitor

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    import asyncio
    import random
    
    async def test_quality_monitor():
        print("ğŸ§ª Model Quality Monitor í…ŒìŠ¤íŠ¸")
        
        monitor = get_quality_monitor("test_quality.db")
        
        print("\n1ï¸âƒ£ ì˜ˆì¸¡ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜")
        
        # FinBERT ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜
        for i in range(50):
            confidence = random.uniform(0.6, 0.95)
            sentiment = "positive" if random.random() > 0.4 else "negative"
            
            monitor.record_prediction(
                model_type=ModelType.FINBERT,
                input_data=f"Test news {i+1}: market analysis",
                prediction=f"sentiment: {sentiment}",
                confidence=confidence,
                latency_ms=random.uniform(50, 200),
                memory_mb=random.uniform(200, 800),
                metadata={"test_run": True}
            )
        
        # PPO ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜
        actions = ["BUY", "SELL", "HOLD"]
        for i in range(30):
            action = random.choice(actions)
            confidence = random.uniform(0.5, 0.9)
            
            monitor.record_prediction(
                model_type=ModelType.PPO,
                input_data=f"Market state {i+1}",
                prediction=action,
                confidence=confidence,
                latency_ms=random.uniform(80, 300),
                memory_mb=random.uniform(300, 1200),
                metadata={"action": action, "test_run": True}
            )
        
        print(f"  âœ… 80ê°œ ì˜ˆì¸¡ ê¸°ë¡ ì™„ë£Œ")
        
        print("\n2ï¸âƒ£ í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±")
        
        # FinBERT í’ˆì§ˆ ë³´ê³ ì„œ
        finbert_report = monitor.generate_quality_report(ModelType.FINBERT)
        print(f"  ğŸ“Š FinBERT ìƒíƒœ: {finbert_report.status.value}")
        print(f"  ğŸ“ˆ ë©”íŠ¸ë¦­ ìˆ˜: {len(finbert_report.metrics)}")
        if finbert_report.recommendations:
            print(f"  ğŸ’¡ ê¶Œì¥ì‚¬í•­: {finbert_report.recommendations[0]}")
        
        # PPO í’ˆì§ˆ ë³´ê³ ì„œ  
        ppo_report = monitor.generate_quality_report(ModelType.PPO)
        print(f"  ğŸ“Š PPO ìƒíƒœ: {ppo_report.status.value}")
        print(f"  ğŸ“ˆ ë©”íŠ¸ë¦­ ìˆ˜: {len(ppo_report.metrics)}")
        if ppo_report.recommendations:
            print(f"  ğŸ’¡ ê¶Œì¥ì‚¬í•­: {ppo_report.recommendations[0]}")
        
        print("\n3ï¸âƒ£ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ")
        
        finbert_status = monitor.get_model_status(ModelType.FINBERT)
        ppo_status = monitor.get_model_status(ModelType.PPO)
        
        print(f"  ğŸ“‹ FinBERT: {finbert_status['prediction_count_24h']}ê°œ ì˜ˆì¸¡ (24h)")
        print(f"  ğŸ“‹ PPO: {ppo_status['prediction_count_24h']}ê°œ ì˜ˆì¸¡ (24h)")
        
        print("\nğŸ‰ Model Quality Monitor í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
        # ì •ë¦¬
        monitor.cleanup()
        
        # í…ŒìŠ¤íŠ¸ DB íŒŒì¼ ì‚­ì œ
        test_db_path = Path("test_quality.db")
        if test_db_path.exists():
            test_db_path.unlink()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_quality_monitor())