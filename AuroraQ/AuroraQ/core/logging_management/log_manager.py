#!/usr/bin/env python3
"""
ê³ ê¸‰ ë¡œê·¸ ê´€ë¦¬ ì‹œìŠ¤í…œ
P7-1: ë¡œê·¸ ê´€ë¦¬ ì‹œìŠ¤í…œ ì„¤ê³„
"""

import sys
import os
import logging
import logging.handlers
import asyncio
import threading
import gzip
import shutil
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union, TextIO
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import json
import re
import warnings
from collections import defaultdict, deque
import traceback

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

warnings.filterwarnings('ignore', category=UserWarning)

class LogLevel(Enum):
    """ë¡œê·¸ ë ˆë²¨"""
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"

class LogRotationPolicy(Enum):
    """ë¡œê·¸ ìˆœí™˜ ì •ì±…"""
    SIZE_BASED = "size_based"
    TIME_BASED = "time_based"
    DAILY = "daily"
    WEEKLY = "weekly"
    MONTHLY = "monthly"

@dataclass
class LogConfig:
    """ë¡œê·¸ ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    log_level: LogLevel = LogLevel.INFO
    log_dir: str = "logs"
    max_file_size_mb: int = 50
    max_backup_count: int = 30
    
    # ìˆœí™˜ ì •ì±…
    rotation_policy: LogRotationPolicy = LogRotationPolicy.SIZE_BASED
    rotation_time: str = "midnight"  # daily ë¡œí…Œì´ì…˜ ì‹œê°„
    
    # í¬ë§· ì„¤ì •
    format_string: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    date_format: str = "%Y-%m-%d %H:%M:%S"
    
    # í•„í„°ë§
    include_modules: List[str] = field(default_factory=list)
    exclude_modules: List[str] = field(default_factory=list)
    sensitive_keywords: List[str] = field(default_factory=lambda: ["password", "token", "key", "secret"])
    
    # ì„±ëŠ¥ ì„¤ì •
    buffer_size: int = 1024
    flush_interval: float = 5.0
    compress_old_logs: bool = True
    
    # ê²½ê³ /ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬
    separate_error_log: bool = True
    error_log_retention_days: int = 90
    
    # ëª¨ë‹ˆí„°ë§
    enable_metrics: bool = True
    metrics_interval: int = 300  # 5ë¶„

@dataclass
class LogEntry:
    """ë¡œê·¸ ì—”íŠ¸ë¦¬"""
    timestamp: datetime
    level: LogLevel
    logger_name: str
    message: str
    module: str = ""
    function: str = ""
    line_number: int = 0
    thread_id: int = 0
    process_id: int = 0
    exception_info: Optional[str] = None
    extra_data: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            'timestamp': self.timestamp.isoformat(),
            'level': self.level.value,
            'logger_name': self.logger_name,
            'message': self.message,
            'module': self.module,
            'function': self.function,
            'line_number': self.line_number,
            'thread_id': self.thread_id,
            'process_id': self.process_id,
            'exception_info': self.exception_info,
            'extra_data': self.extra_data
        }

@dataclass
class LogMetrics:
    """ë¡œê·¸ ë©”íŠ¸ë¦­"""
    total_logs: int = 0
    logs_by_level: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    logs_by_module: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    error_rate: float = 0.0
    warning_rate: float = 0.0
    avg_logs_per_minute: float = 0.0
    peak_logs_per_minute: float = 0.0
    last_error_time: Optional[datetime] = None
    last_warning_time: Optional[datetime] = None
    start_time: datetime = field(default_factory=datetime.now)

class AdvancedFormatter(logging.Formatter):
    """ê³ ê¸‰ ë¡œê·¸ í¬ë§¤í„°"""
    
    def __init__(self, config: LogConfig):
        super().__init__(config.format_string, config.date_format)
        self.config = config
    
    def format(self, record):
        # ë¯¼ê°í•œ ì •ë³´ ë§ˆìŠ¤í‚¹
        original_msg = record.getMessage()
        masked_msg = self._mask_sensitive_info(original_msg)
        record.msg = masked_msg
        record.args = None
        
        # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        record.thread_id = threading.get_ident()
        record.process_id = os.getpid()
        
        # ì˜ˆì™¸ ì •ë³´ í¬í•¨
        if record.exc_info:
            record.exception_info = self.formatException(record.exc_info)
        
        return super().format(record)
    
    def _mask_sensitive_info(self, message: str) -> str:
        """ë¯¼ê°í•œ ì •ë³´ ë§ˆìŠ¤í‚¹"""
        for keyword in self.config.sensitive_keywords:
            # í‚¤ì›Œë“œ = ê°’ íŒ¨í„´ ë§ˆìŠ¤í‚¹
            pattern = rf'({keyword}\s*[=:]\s*)([^\s,\]}}]+)'
            message = re.sub(pattern, r'\1***', message, flags=re.IGNORECASE)
        return message

class LogManager:
    """ê³ ê¸‰ ë¡œê·¸ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Optional[LogConfig] = None, config_file: str = "log_config.json"):
        self.config = config or LogConfig()
        self.config_file = config_file
        
        # ë¡œê±° ê´€ë¦¬
        self.loggers: Dict[str, logging.Logger] = {}
        self.handlers: Dict[str, List[logging.Handler]] = defaultdict(list)
        
        # ë©”íŠ¸ë¦­ ë° ëª¨ë‹ˆí„°ë§
        self.metrics = LogMetrics()
        self.log_buffer: deque = deque(maxlen=self.config.buffer_size)
        self.metrics_history: List[LogMetrics] = []
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        self._metrics_lock = threading.RLock()
        
        # ëª¨ë‹ˆí„°ë§ ì œì–´
        self._monitoring_active = False
        self._monitor_thread = None
        self._buffer_thread = None
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        self.log_dir = Path(self.config.log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„¤ì • ë¡œë“œ ë° ì´ˆê¸°í™”
        self._load_configuration()
        self._setup_root_logger()
        
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        if self.config.enable_metrics:
            self.start_monitoring()
        
        # loggerëŠ” ì´ˆê¸°í™” ì™„ë£Œ í›„ ë³„ë„ë¡œ ì„¤ì •
        try:
            self.logger = self.get_logger("LogManager")
            self.logger.info("Advanced log manager initialized")
        except Exception:
            print("Info: Advanced log manager initialized")
    
    def _load_configuration(self):
        """ì„¤ì • ë¡œë“œ"""
        try:
            if Path(self.config_file).exists():
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                
                # ì„¤ì • ì—…ë°ì´íŠ¸
                for key, value in config_data.items():
                    if hasattr(self.config, key):
                        if key in ['log_level', 'rotation_policy']:
                            # Enum íƒ€ì… ì²˜ë¦¬
                            enum_class = LogLevel if key == 'log_level' else LogRotationPolicy
                            setattr(self.config, key, enum_class(value))
                        else:
                            setattr(self.config, key, value)
                
                print(f"Info: Log configuration loaded from {self.config_file}")
        except Exception as e:
            print(f"Warning: Failed to load log configuration: {e}")
    
    def _save_configuration(self):
        """ì„¤ì • ì €ì¥"""
        try:
            config_data = {
                'log_level': self.config.log_level.value,
                'log_dir': self.config.log_dir,
                'max_file_size_mb': self.config.max_file_size_mb,
                'max_backup_count': self.config.max_backup_count,
                'rotation_policy': self.config.rotation_policy.value,
                'rotation_time': self.config.rotation_time,
                'format_string': self.config.format_string,
                'date_format': self.config.date_format,
                'include_modules': self.config.include_modules,
                'exclude_modules': self.config.exclude_modules,
                'sensitive_keywords': self.config.sensitive_keywords,
                'buffer_size': self.config.buffer_size,
                'flush_interval': self.config.flush_interval,
                'compress_old_logs': self.config.compress_old_logs,
                'separate_error_log': self.config.separate_error_log,
                'error_log_retention_days': self.config.error_log_retention_days,
                'enable_metrics': self.config.enable_metrics,
                'metrics_interval': self.config.metrics_interval
            }
            
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"Warning: Failed to save log configuration: {e}")
    
    def _setup_root_logger(self):
        """ë£¨íŠ¸ ë¡œê±° ì„¤ì •"""
        root_logger = logging.getLogger()
        root_logger.setLevel(getattr(logging, self.config.log_level.value))
        
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
        for handler in root_logger.handlers[:]:
            root_logger.removeHandler(handler)
        
        # ê³ ê¸‰ í¬ë§¤í„°
        formatter = AdvancedFormatter(self.config)
        
        # ë©”ì¸ ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬
        main_handler = self._create_rotating_handler(
            self.log_dir / "auroaq.log",
            formatter
        )
        root_logger.addHandler(main_handler)
        self.handlers["root"].append(main_handler)
        
        # ì—ëŸ¬ ì „ìš© ë¡œê·¸ íŒŒì¼
        if self.config.separate_error_log:
            error_handler = self._create_rotating_handler(
                self.log_dir / "error.log",
                formatter,
                min_level=logging.ERROR
            )
            root_logger.addHandler(error_handler)
            self.handlers["root"].append(error_handler)
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        console_handler.setLevel(getattr(logging, self.config.log_level.value))
        root_logger.addHandler(console_handler)
        self.handlers["root"].append(console_handler)
    
    def _create_rotating_handler(self, log_file: Path, formatter: logging.Formatter, 
                                min_level: int = logging.DEBUG) -> logging.Handler:
        """ìˆœí™˜ í•¸ë“¤ëŸ¬ ìƒì„±"""
        if self.config.rotation_policy == LogRotationPolicy.SIZE_BASED:
            handler = logging.handlers.RotatingFileHandler(
                log_file,
                maxBytes=self.config.max_file_size_mb * 1024 * 1024,
                backupCount=self.config.max_backup_count,
                encoding='utf-8'
            )
        elif self.config.rotation_policy == LogRotationPolicy.TIME_BASED:
            handler = logging.handlers.TimedRotatingFileHandler(
                log_file,
                when='midnight',
                interval=1,
                backupCount=self.config.max_backup_count,
                encoding='utf-8'
            )
        elif self.config.rotation_policy == LogRotationPolicy.DAILY:
            handler = logging.handlers.TimedRotatingFileHandler(
                log_file,
                when='D',
                interval=1,
                backupCount=self.config.max_backup_count,
                encoding='utf-8'
            )
        elif self.config.rotation_policy == LogRotationPolicy.WEEKLY:
            handler = logging.handlers.TimedRotatingFileHandler(
                log_file,
                when='W0',  # Monday
                interval=1,
                backupCount=self.config.max_backup_count,
                encoding='utf-8'
            )
        else:  # MONTHLY
            handler = logging.handlers.TimedRotatingFileHandler(
                log_file,
                when='midnight',
                interval=30,  # ëŒ€ëµ ì›”ë³„
                backupCount=12,
                encoding='utf-8'
            )
        
        handler.setFormatter(formatter)
        handler.setLevel(min_level)
        
        # ì••ì¶• í™œì„±í™”
        if self.config.compress_old_logs:
            handler.rotator = self._compress_rotated_log
        
        return handler
    
    def _compress_rotated_log(self, source: str, dest: str):
        """ë¡œí…Œì´íŠ¸ëœ ë¡œê·¸ ì••ì¶•"""
        try:
            with open(source, 'rb') as f_in:
                with gzip.open(f"{dest}.gz", 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            os.remove(source)
        except Exception as e:
            print(f"Warning: Failed to compress log file {source}: {e}")
    
    def get_logger(self, name: str) -> logging.Logger:
        """ë¡œê±° ë°˜í™˜"""
        with self._lock:
            if name not in self.loggers:
                logger = logging.getLogger(name)
                
                # ëª¨ë“ˆ í•„í„°ë§
                if self.config.include_modules and name not in self.config.include_modules:
                    logger.disabled = True
                elif name in self.config.exclude_modules:
                    logger.disabled = True
                
                # ë¡œê·¸ ì—”íŠ¸ë¦¬ ìˆ˜ì§‘ê¸° ì¶”ê°€
                collector = LogEntryCollector(self)
                logger.addHandler(collector)
                
                self.loggers[name] = logger
            
            return self.loggers[name]
    
    def add_log_entry(self, entry: LogEntry):
        """ë¡œê·¸ ì—”íŠ¸ë¦¬ ì¶”ê°€"""
        try:
            with self._metrics_lock:
                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                self.metrics.total_logs += 1
                self.metrics.logs_by_level[entry.level.value] += 1
                self.metrics.logs_by_module[entry.module] += 1
                
                if entry.level == LogLevel.ERROR:
                    self.metrics.last_error_time = entry.timestamp
                elif entry.level == LogLevel.WARNING:
                    self.metrics.last_warning_time = entry.timestamp
                
                # ë²„í¼ì— ì¶”ê°€
                self.log_buffer.append(entry)
                
        except Exception as e:
            print(f"Warning: Failed to add log entry: {e}")
    
    def search_logs(self, query: str, level: Optional[LogLevel] = None, 
                   start_time: Optional[datetime] = None, end_time: Optional[datetime] = None,
                   limit: int = 1000) -> List[LogEntry]:
        """ë¡œê·¸ ê²€ìƒ‰"""
        try:
            results = []
            
            # ë©”ëª¨ë¦¬ ë²„í¼ì—ì„œ ê²€ìƒ‰
            for entry in self.log_buffer:
                if self._matches_criteria(entry, query, level, start_time, end_time):
                    results.append(entry)
                    if len(results) >= limit:
                        break
            
            return results
            
        except Exception as e:
            print(f"Error: Log search failed: {e}")
            return []
    
    def _matches_criteria(self, entry: LogEntry, query: str, level: Optional[LogLevel],
                         start_time: Optional[datetime], end_time: Optional[datetime]) -> bool:
        """ê²€ìƒ‰ ì¡°ê±´ í™•ì¸"""
        # í…ìŠ¤íŠ¸ ê²€ìƒ‰
        if query and query.lower() not in entry.message.lower():
            return False
        
        # ë ˆë²¨ í•„í„°
        if level and entry.level != level:
            return False
        
        # ì‹œê°„ ë²”ìœ„
        if start_time and entry.timestamp < start_time:
            return False
        if end_time and entry.timestamp > end_time:
            return False
        
        return True
    
    def get_log_statistics(self) -> Dict[str, Any]:
        """ë¡œê·¸ í†µê³„ ë°˜í™˜"""
        try:
            with self._metrics_lock:
                # ì—ëŸ¬ìœ¨ ê³„ì‚°
                total_logs = max(1, self.metrics.total_logs)
                error_count = self.metrics.logs_by_level.get('ERROR', 0)
                warning_count = self.metrics.logs_by_level.get('WARNING', 0)
                
                self.metrics.error_rate = (error_count / total_logs) * 100
                self.metrics.warning_rate = (warning_count / total_logs) * 100
                
                # ë¶„ë‹¹ ë¡œê·¸ ìˆ˜ ê³„ì‚°
                elapsed_minutes = (datetime.now() - self.metrics.start_time).total_seconds() / 60
                if elapsed_minutes > 0:
                    self.metrics.avg_logs_per_minute = self.metrics.total_logs / elapsed_minutes
                
                return {
                    'total_logs': self.metrics.total_logs,
                    'logs_by_level': dict(self.metrics.logs_by_level),
                    'logs_by_module': dict(self.metrics.logs_by_module),
                    'error_rate': f"{self.metrics.error_rate:.2f}%",
                    'warning_rate': f"{self.metrics.warning_rate:.2f}%",
                    'avg_logs_per_minute': f"{self.metrics.avg_logs_per_minute:.1f}",
                    'peak_logs_per_minute': f"{self.metrics.peak_logs_per_minute:.1f}",
                    'last_error_time': self.metrics.last_error_time.isoformat() if self.metrics.last_error_time else None,
                    'last_warning_time': self.metrics.last_warning_time.isoformat() if self.metrics.last_warning_time else None,
                    'uptime_minutes': elapsed_minutes,
                    'buffer_size': len(self.log_buffer),
                    'monitoring_active': self._monitoring_active
                }
                
        except Exception as e:
            return {'error': str(e)}
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self._monitoring_active:
            return
        
        self._monitoring_active = True
        
        def monitor_loop():
            while self._monitoring_active:
                try:
                    # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                    self._collect_metrics()
                    
                    # ë¡œê·¸ íŒŒì¼ ì •ë¦¬
                    self._cleanup_old_logs()
                    
                    # ëŒ€ê¸°
                    for _ in range(self.config.metrics_interval):
                        if not self._monitoring_active:
                            break
                        import time
                        time.sleep(1)
                        
                except Exception as e:
                    print(f"Warning: Monitoring loop error: {e}")
                    time.sleep(10)
        
        self._monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        self._monitor_thread.start()
        
        print(f"Info: Log monitoring started (interval: {self.config.metrics_interval}s)")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        if not self._monitoring_active:
            return
        
        self._monitoring_active = False
        if self._monitor_thread and self._monitor_thread.is_alive():
            self._monitor_thread.join(timeout=10)
        
        print("Info: Log monitoring stopped")
    
    def _collect_metrics(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            with self._metrics_lock:
                # í˜„ì¬ ë©”íŠ¸ë¦­ ìŠ¤ëƒ…ìƒ·
                current_metrics = LogMetrics(
                    total_logs=self.metrics.total_logs,
                    logs_by_level=dict(self.metrics.logs_by_level),
                    logs_by_module=dict(self.metrics.logs_by_module),
                    error_rate=self.metrics.error_rate,
                    warning_rate=self.metrics.warning_rate,
                    avg_logs_per_minute=self.metrics.avg_logs_per_minute,
                    peak_logs_per_minute=self.metrics.peak_logs_per_minute,
                    last_error_time=self.metrics.last_error_time,
                    last_warning_time=self.metrics.last_warning_time,
                    start_time=datetime.now()
                )
                
                self.metrics_history.append(current_metrics)
                
                # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
                if len(self.metrics_history) > 288:  # 24ì‹œê°„ (5ë¶„ ê°„ê²©)
                    self.metrics_history.pop(0)
                
        except Exception as e:
            print(f"Warning: Failed to collect metrics: {e}")
    
    def _cleanup_old_logs(self):
        """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬"""
        try:
            cutoff_date = datetime.now() - timedelta(days=self.config.error_log_retention_days)
            
            for log_file in self.log_dir.glob("*.log*"):
                try:
                    if log_file.stat().st_mtime < cutoff_date.timestamp():
                        log_file.unlink()
                        print(f"Info: Cleaned up old log file: {log_file}")
                except Exception as e:
                    print(f"Warning: Failed to cleanup log file {log_file}: {e}")
                    
        except Exception as e:
            print(f"Error: Log cleanup failed: {e}")
    
    def export_logs(self, output_file: str, format: str = "json", 
                   level: Optional[LogLevel] = None,
                   start_time: Optional[datetime] = None, 
                   end_time: Optional[datetime] = None) -> bool:
        """ë¡œê·¸ ë‚´ë³´ë‚´ê¸°"""
        try:
            entries = self.search_logs("", level, start_time, end_time, limit=10000)
            
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            if format.lower() == "json":
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump([entry.to_dict() for entry in entries], f, indent=2, ensure_ascii=False)
            elif format.lower() == "csv":
                import csv
                with open(output_path, 'w', newline='', encoding='utf-8') as f:
                    if entries:
                        fieldnames = list(entries[0].to_dict().keys())
                        writer = csv.DictWriter(f, fieldnames=fieldnames)
                        writer.writeheader()
                        for entry in entries:
                            writer.writerow(entry.to_dict())
            else:
                # Plain text
                with open(output_path, 'w', encoding='utf-8') as f:
                    for entry in entries:
                        f.write(f"{entry.timestamp.isoformat()} [{entry.level.value}] {entry.logger_name}: {entry.message}\n")
            
            print(f"Info: Exported {len(entries)} log entries to {output_file}")
            return True
            
        except Exception as e:
            print(f"Error: Log export failed: {e}")
            return False
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.stop_monitoring()
            
            # ëª¨ë“  í•¸ë“¤ëŸ¬ ì •ë¦¬
            for handlers in self.handlers.values():
                for handler in handlers:
                    handler.close()
            
            # ì„¤ì • ì €ì¥
            self._save_configuration()
            
            print("Info: Log manager cleanup completed")
            
        except Exception as e:
            print(f"Warning: Log manager cleanup failed: {e}")

class LogEntryCollector(logging.Handler):
    """ë¡œê·¸ ì—”íŠ¸ë¦¬ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, log_manager: LogManager):
        super().__init__()
        self.log_manager = log_manager
    
    def emit(self, record):
        try:
            # LogRecordë¥¼ LogEntryë¡œ ë³€í™˜
            entry = LogEntry(
                timestamp=datetime.fromtimestamp(record.created),
                level=LogLevel(record.levelname),
                logger_name=record.name,
                message=record.getMessage(),
                module=record.module if hasattr(record, 'module') else "",
                function=record.funcName if hasattr(record, 'funcName') else "",
                line_number=record.lineno if hasattr(record, 'lineno') else 0,
                thread_id=getattr(record, 'thread_id', 0),
                process_id=getattr(record, 'process_id', 0),
                exception_info=getattr(record, 'exception_info', None)
            )
            
            self.log_manager.add_log_entry(entry)
            
        except Exception:
            # ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ ì‹œ ë¬´ì‹œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
            pass

# ì „ì—­ ë¡œê·¸ ê´€ë¦¬ì
_global_log_manager = None

def get_log_manager(config: Optional[LogConfig] = None, config_file: str = None) -> LogManager:
    """ì „ì—­ ë¡œê·¸ ê´€ë¦¬ì ë°˜í™˜"""
    global _global_log_manager
    if _global_log_manager is None:
        _global_log_manager = LogManager(
            config=config,
            config_file=config_file or "log_config.json"
        )
    return _global_log_manager

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    import asyncio
    
    async def test_log_manager():
        print("ğŸ§ª Advanced Log Manager í…ŒìŠ¤íŠ¸")
        
        # ì„¤ì •
        config = LogConfig(
            log_level=LogLevel.DEBUG,
            log_dir="test_logs",
            max_file_size_mb=1,  # ì‘ì€ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸
            rotation_policy=LogRotationPolicy.SIZE_BASED,
            enable_metrics=True,
            metrics_interval=5
        )
        
        manager = get_log_manager(config, "test_log_config.json")
        
        print("\n1ï¸âƒ£ ë‹¤ì–‘í•œ ë ˆë²¨ ë¡œê·¸ ìƒì„±")
        logger = manager.get_logger("TestModule")
        
        logger.debug("ë””ë²„ê·¸ ë©”ì‹œì§€")
        logger.info("ì •ë³´ ë©”ì‹œì§€")
        logger.warning("ê²½ê³  ë©”ì‹œì§€")
        logger.error("ì—ëŸ¬ ë©”ì‹œì§€")
        logger.critical("ì‹¬ê°í•œ ì—ëŸ¬ ë©”ì‹œì§€")
        
        # ë¯¼ê°í•œ ì •ë³´ í…ŒìŠ¤íŠ¸
        logger.info("ì‚¬ìš©ì ë¡œê·¸ì¸: password=secret123, token=abc123")
        
        print("\n2ï¸âƒ£ ë¡œê·¸ í†µê³„")
        stats = manager.get_log_statistics()
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        print("\n3ï¸âƒ£ ë¡œê·¸ ê²€ìƒ‰")
        results = manager.search_logs("ì—ëŸ¬", LogLevel.ERROR, limit=5)
        print(f"  ì—ëŸ¬ ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
        for result in results:
            print(f"    {result.timestamp}: {result.message}")
        
        print("\n4ï¸âƒ£ ë¡œê·¸ ë‚´ë³´ë‚´ê¸°")
        export_success = manager.export_logs("test_logs/export.json", "json")
        print(f"  ë‚´ë³´ë‚´ê¸° ì„±ê³µ: {export_success}")
        
        print("\n5ï¸âƒ£ ëŒ€ëŸ‰ ë¡œê·¸ ìƒì„± (ë¡œí…Œì´ì…˜ í…ŒìŠ¤íŠ¸)")
        for i in range(100):
            logger.info(f"ëŒ€ëŸ‰ ë¡œê·¸ ë©”ì‹œì§€ {i+1}")
            if i % 20 == 0:
                await asyncio.sleep(0.1)
        
        await asyncio.sleep(2)
        
        print("\n6ï¸âƒ£ ìµœì¢… í†µê³„")
        final_stats = manager.get_log_statistics()
        print(f"  ì´ ë¡œê·¸ ìˆ˜: {final_stats['total_logs']}")
        print(f"  ì—ëŸ¬ìœ¨: {final_stats['error_rate']}")
        print(f"  ë¶„ë‹¹ í‰ê·  ë¡œê·¸: {final_stats['avg_logs_per_minute']}")
        
        print("\nğŸ‰ Advanced Log Manager í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
        # ì •ë¦¬
        manager.cleanup()
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
        import shutil
        test_dir = Path("test_logs")
        if test_dir.exists():
            shutil.rmtree(test_dir)
        
        test_config = Path("test_log_config.json")
        if test_config.exists():
            test_config.unlink()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_log_manager())