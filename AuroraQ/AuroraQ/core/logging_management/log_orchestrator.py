#!/usr/bin/env python3
"""
ë¡œê·¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í†µí•© ì¸í„°í˜ì´ìŠ¤
P7-5: LogOrchestrator í†µí•© ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
"""

import sys
import os
import asyncio
import threading
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from core.logging_management import (
    LogManager, get_log_manager, LogLevel, LogConfig, LogRotationPolicy,
    BackupManager, get_backup_manager, BackupConfig, BackupType, CompressionType,
    ArchiveManager, get_archive_manager, ArchiveConfig, ArchivePolicy
)

class OperationStatus(Enum):
    """ì‘ì—… ìƒíƒœ"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class OperationType(Enum):
    """ì‘ì—… ìœ í˜•"""
    LOG_ROTATION = "log_rotation"
    BACKUP_CREATION = "backup_creation"
    BACKUP_RESTORATION = "backup_restoration"
    ARCHIVE_CREATION = "archive_creation"
    CLEANUP = "cleanup"
    INTEGRITY_CHECK = "integrity_check"
    WORKFLOW = "workflow"

@dataclass
class Operation:
    """ì‘ì—… ì •ë³´"""
    operation_id: str
    operation_type: OperationType
    status: OperationStatus
    created_at: datetime
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    parameters: Dict[str, Any] = field(default_factory=dict)
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    progress: float = 0.0

@dataclass
class WorkflowStep:
    """ì›Œí¬í”Œë¡œìš° ë‹¨ê³„"""
    step_id: str
    operation_type: OperationType
    parameters: Dict[str, Any]
    depends_on: List[str] = field(default_factory=list)
    timeout_seconds: int = 300
    retry_count: int = 3

@dataclass
class WorkflowDefinition:
    """ì›Œí¬í”Œë¡œìš° ì •ì˜"""
    workflow_id: str
    name: str
    description: str
    steps: List[WorkflowStep]
    created_at: datetime = field(default_factory=datetime.now)

class LogOrchestrator:
    """ë¡œê·¸ ê´€ë¦¬ í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    
    def __init__(self, 
                 log_config: Optional[LogConfig] = None,
                 backup_config: Optional[BackupConfig] = None,
                 archive_config: Optional[ArchiveConfig] = None):
        
        # ê°œë³„ ê´€ë¦¬ì ì´ˆê¸°í™”
        self.log_manager = LogManager(log_config) if log_config else get_log_manager()
        self.backup_manager = BackupManager(backup_config) if backup_config else get_backup_manager()
        self.archive_manager = ArchiveManager(archive_config) if archive_config else get_archive_manager()
        
        # Logger ì´ˆê¸°í™”
        try:
            self.logger = self.log_manager.get_logger("LogOrchestrator")
        except Exception:
            import logging
            logging.basicConfig()
            self.logger = logging.getLogger("LogOrchestrator")
        
        # ì‘ì—… ê´€ë¦¬
        self.operations: Dict[str, Operation] = {}
        self.workflows: Dict[str, WorkflowDefinition] = {}
        self.active_operations: Dict[str, asyncio.Task] = {}
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        # ë‚´ì¥ ì›Œí¬í”Œë¡œìš° ë“±ë¡
        self._register_builtin_workflows()
        
        self.logger.info("Log Orchestrator initialized")
    
    def _register_builtin_workflows(self):
        """ë‚´ì¥ ì›Œí¬í”Œë¡œìš° ë“±ë¡"""
        
        # 1. ì¼ì¼ ë¡œê·¸ ê´€ë¦¬ ì›Œí¬í”Œë¡œìš°
        daily_log_workflow = WorkflowDefinition(
            workflow_id="daily_log_management",
            name="Daily Log Management",
            description="Daily log rotation, backup, and archiving workflow",
            steps=[
                WorkflowStep(
                    step_id="rotate_logs",
                    operation_type=OperationType.LOG_ROTATION,
                    parameters={"max_age_hours": 24}
                ),
                WorkflowStep(
                    step_id="backup_logs",
                    operation_type=OperationType.BACKUP_CREATION,
                    parameters={"backup_type": "incremental", "source": "logs"},
                    depends_on=["rotate_logs"]
                ),
                WorkflowStep(
                    step_id="archive_old_logs",
                    operation_type=OperationType.ARCHIVE_CREATION,
                    parameters={"age_days": 7},
                    depends_on=["backup_logs"]
                ),
                WorkflowStep(
                    step_id="cleanup_temp",
                    operation_type=OperationType.CLEANUP,
                    parameters={"target": "temp_files", "age_days": 1},
                    depends_on=["archive_old_logs"]
                )
            ]
        )
        
        # 2. ë°±ì—… ë¬´ê²°ì„± ê²€ì¦ ì›Œí¬í”Œë¡œìš°
        integrity_check_workflow = WorkflowDefinition(
            workflow_id="backup_integrity_check",
            name="Backup Integrity Check",
            description="Comprehensive backup integrity validation workflow",
            steps=[
                WorkflowStep(
                    step_id="verify_recent_backups",
                    operation_type=OperationType.INTEGRITY_CHECK,
                    parameters={"scope": "recent", "days": 7}
                ),
                WorkflowStep(
                    step_id="test_restore_sample",
                    operation_type=OperationType.BACKUP_RESTORATION,
                    parameters={"test_mode": True, "sample_files": 5},
                    depends_on=["verify_recent_backups"]
                ),
                WorkflowStep(
                    step_id="cleanup_test_restore",
                    operation_type=OperationType.CLEANUP,
                    parameters={"target": "test_restore"},
                    depends_on=["test_restore_sample"]
                )
            ]
        )
        
        # 3. ê¸´ê¸‰ ë³µêµ¬ ì›Œí¬í”Œë¡œìš°
        emergency_recovery_workflow = WorkflowDefinition(
            workflow_id="emergency_recovery",
            name="Emergency Recovery",
            description="Emergency log and data recovery workflow",
            steps=[
                WorkflowStep(
                    step_id="stop_logging",
                    operation_type=OperationType.LOG_ROTATION,
                    parameters={"emergency_stop": True}
                ),
                WorkflowStep(
                    step_id="emergency_backup",
                    operation_type=OperationType.BACKUP_CREATION,
                    parameters={"backup_type": "full", "priority": "emergency"},
                    depends_on=["stop_logging"]
                ),
                WorkflowStep(
                    step_id="restore_from_backup",
                    operation_type=OperationType.BACKUP_RESTORATION,
                    parameters={"restore_latest": True, "verify": True},
                    depends_on=["emergency_backup"]
                )
            ]
        )
        
        self.workflows.update({
            "daily_log_management": daily_log_workflow,
            "backup_integrity_check": integrity_check_workflow,
            "emergency_recovery": emergency_recovery_workflow
        })
    
    async def execute_operation(self, operation_type: OperationType, 
                              parameters: Dict[str, Any]) -> str:
        """ë‹¨ì¼ ì‘ì—… ì‹¤í–‰"""
        operation_id = f"{operation_type.value}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        operation = Operation(
            operation_id=operation_id,
            operation_type=operation_type,
            status=OperationStatus.PENDING,
            created_at=datetime.now(),
            parameters=parameters
        )
        
        with self._lock:
            self.operations[operation_id] = operation
        
        try:
            self.logger.info(f"Starting operation {operation_id}: {operation_type.value}")
            
            operation.status = OperationStatus.RUNNING
            operation.started_at = datetime.now()
            
            # ì‘ì—… ìœ í˜•ë³„ ì‹¤í–‰
            if operation_type == OperationType.LOG_ROTATION:
                result = await self._execute_log_rotation(parameters)
            elif operation_type == OperationType.BACKUP_CREATION:
                result = await self._execute_backup_creation(parameters)
            elif operation_type == OperationType.BACKUP_RESTORATION:
                result = await self._execute_backup_restoration(parameters)
            elif operation_type == OperationType.ARCHIVE_CREATION:
                result = await self._execute_archive_creation(parameters)
            elif operation_type == OperationType.CLEANUP:
                result = await self._execute_cleanup(parameters)
            elif operation_type == OperationType.INTEGRITY_CHECK:
                result = await self._execute_integrity_check(parameters)
            else:
                raise ValueError(f"Unsupported operation type: {operation_type}")
            
            operation.status = OperationStatus.COMPLETED
            operation.completed_at = datetime.now()
            operation.result = result
            operation.progress = 100.0
            
            self.logger.info(f"Operation {operation_id} completed successfully")
            return operation_id
            
        except Exception as e:
            operation.status = OperationStatus.FAILED
            operation.error_message = str(e)
            operation.completed_at = datetime.now()
            
            self.logger.error(f"Operation {operation_id} failed: {e}")
            raise
    
    async def execute_workflow(self, workflow_id: str, 
                             parameters: Optional[Dict[str, Any]] = None) -> str:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        if workflow_id not in self.workflows:
            raise ValueError(f"Workflow not found: {workflow_id}")
        
        workflow = self.workflows[workflow_id]
        workflow_operation_id = f"workflow_{workflow_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        operation = Operation(
            operation_id=workflow_operation_id,
            operation_type=OperationType.WORKFLOW,
            status=OperationStatus.RUNNING,
            created_at=datetime.now(),
            started_at=datetime.now(),
            parameters={"workflow_id": workflow_id, "parameters": parameters or {}}
        )
        
        with self._lock:
            self.operations[workflow_operation_id] = operation
        
        try:
            self.logger.info(f"Starting workflow {workflow_id}")
            
            # ë‹¨ê³„ë³„ ì‹¤í–‰ (ì˜ì¡´ì„± ìˆœì„œëŒ€ë¡œ)
            completed_steps = set()
            step_results = {}
            workflow_context = {}  # ì›Œí¬í”Œë¡œìš° ì»¨í…ìŠ¤íŠ¸ ì €ì¥
            remaining_steps = workflow.steps.copy()
            
            while remaining_steps:
                executed_this_round = False
                
                for i, step in enumerate(remaining_steps):
                    # ì˜ì¡´ì„± í™•ì¸
                    if all(dep in completed_steps for dep in step.depends_on):
                        # ë‹¨ê³„ ì‹¤í–‰
                        self.logger.info(f"Executing workflow step: {step.step_id}")
                        
                        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë§¤ê°œë³€ìˆ˜ ë³´ê°•
                        step_params = step.parameters.copy()
                        
                        # backup_integrity_check ì›Œí¬í”Œë¡œìš°ì˜ íŠ¹ë³„ ì²˜ë¦¬
                        if workflow_id == "backup_integrity_check" and step.step_id == "test_restore_sample":
                            # ì´ì „ ë‹¨ê³„ì—ì„œ ì°¾ì€ ë°±ì—… ì‚¬ìš©
                            if "verify_recent_backups" in step_results:
                                verify_op = self.get_operation_status(step_results["verify_recent_backups"])
                                if verify_op and verify_op.result and verify_op.result.get("checked_backups"):
                                    checked_backups = verify_op.result["checked_backups"]
                                    if checked_backups:
                                        # ê°€ì¥ ìµœê·¼ ë°±ì—… ì‚¬ìš©
                                        latest_backup = checked_backups[0]
                                        step_params["backup_id"] = latest_backup["backup_id"]
                                    else:
                                        step_params["restore_latest"] = True
                                else:
                                    step_params["restore_latest"] = True
                        
                        step_operation_id = await self.execute_operation(
                            step.operation_type, 
                            step_params
                        )
                        
                        # ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì €ì¥
                        step_operation = self.get_operation_status(step_operation_id)
                        if step_operation and step_operation.result:
                            workflow_context[step.step_id] = step_operation.result
                        
                        step_results[step.step_id] = step_operation_id
                        completed_steps.add(step.step_id)
                        
                        # ì™„ë£Œëœ ë‹¨ê³„ ì œê±°
                        remaining_steps.pop(i)
                        executed_this_round = True
                        
                        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                        operation.progress = (len(completed_steps) / len(workflow.steps)) * 100
                        break
                
                # ë¬´í•œ ë£¨í”„ ë°©ì§€
                if not executed_this_round:
                    # ì˜ì¡´ì„± ë¬¸ì œë¡œ ì‹¤í–‰í•  ìˆ˜ ì—†ëŠ” ë‹¨ê³„ë“¤
                    failed_steps = [step.step_id for step in remaining_steps]
                    raise Exception(f"Workflow deadlock: cannot execute steps {failed_steps}")
            
            # ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ
            
            operation.status = OperationStatus.COMPLETED
            operation.completed_at = datetime.now()
            operation.result = {"step_results": step_results}
            
            self.logger.info(f"Workflow {workflow_id} completed successfully")
            return workflow_operation_id
            
        except Exception as e:
            operation.status = OperationStatus.FAILED
            operation.error_message = str(e)
            operation.completed_at = datetime.now()
            
            self.logger.error(f"Workflow {workflow_id} failed: {e}")
            raise
    
    async def _execute_log_rotation(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """ë¡œê·¸ ìˆœí™˜ ì‹¤í–‰"""
        max_age_hours = parameters.get("max_age_hours", 24)
        emergency_stop = parameters.get("emergency_stop", False)
        
        if emergency_stop:
            # ê¸´ê¸‰ ì •ì§€ - í˜„ì¬ ë¡œê¹… ì¤‘ë‹¨
            self.log_manager.stop_monitoring()
            return {"action": "emergency_stop", "stopped": True}
        
        # ì¼ë°˜ ë¡œê·¸ ìˆœí™˜
        cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
        rotated_files = []
        
        log_dir = Path(self.log_manager.config.log_dir)
        if log_dir.exists():
            for log_file in log_dir.glob("*.log"):
                if log_file.stat().st_mtime < cutoff_time.timestamp():
                    # ë¡œê·¸ íŒŒì¼ ìˆœí™˜ (ì••ì¶•)
                    rotated_file = log_file.with_suffix(".log.gz")
                    
                    import gzip
                    with open(log_file, 'rb') as f_in:
                        with gzip.open(rotated_file, 'wb') as f_out:
                            f_out.write(f_in.read())
                    
                    log_file.unlink()
                    rotated_files.append(str(rotated_file))
        
        return {
            "action": "log_rotation",
            "rotated_files": rotated_files,
            "cutoff_time": cutoff_time.isoformat()
        }
    
    async def _execute_backup_creation(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """ë°±ì—… ìƒì„± ì‹¤í–‰"""
        backup_type_str = parameters.get("backup_type", "incremental")
        source = parameters.get("source", "logs")
        priority = parameters.get("priority", "normal")
        
        # BackupType ë³€í™˜
        backup_type = BackupType.INCREMENTAL
        if backup_type_str == "full":
            backup_type = BackupType.FULL
        elif backup_type_str == "differential":
            backup_type = BackupType.DIFFERENTIAL
        elif backup_type_str == "snapshot":
            backup_type = BackupType.SNAPSHOT
        
        # ì†ŒìŠ¤ ê²½ë¡œ ê²°ì •
        if source == "logs":
            source_path = self.log_manager.config.log_dir
        else:
            source_path = source
        
        # ë°±ì—… ì‹¤í–‰
        backup_id = await self.backup_manager.create_backup(
            backup_type=backup_type,
            source_path=source_path
        )
        
        return {
            "action": "backup_creation",
            "backup_id": backup_id,
            "backup_type": backup_type_str,
            "source_path": source_path,
            "priority": priority
        }
    
    async def _execute_backup_restoration(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """ë°±ì—… ë³µì› ì‹¤í–‰ (ê°•í™”ëœ ë¬´ê²°ì„± ê²€ì‚¬ í¬í•¨)"""
        test_mode = parameters.get("test_mode", False)
        restore_latest = parameters.get("restore_latest", False)
        verify = parameters.get("verify", True)
        sample_files = parameters.get("sample_files", 0)
        
        # ë°±ì—… ì„ íƒ
        if restore_latest:
            backups = await self.backup_manager.list_backups()
            if not backups:
                raise ValueError("No backups available for restoration")
            
            # ê°€ì¥ ìµœì‹  ë°±ì—… ì„ íƒ
            latest_backup = max(backups, key=lambda x: x.created_at)
            backup_id = latest_backup.backup_id
        else:
            backup_id = parameters.get("backup_id")
            if not backup_id:
                raise ValueError("backup_id is required when restore_latest=False")
        
        # ë³µì› ëŒ€ìƒ ê²½ë¡œ
        if test_mode:
            restore_path = str(Path(self.backup_manager.config.temp_dir) / "test_restore")
        else:
            restore_path = parameters.get("restore_path")
            if not restore_path:
                raise ValueError("restore_path is required for non-test mode")
        
        # ë°±ì—… ë¬´ê²°ì„± ê²€ì‚¬ ê°•í™”
        backup_entry = await self.backup_manager.get_backup_by_id(backup_id)
        if not backup_entry:
            raise ValueError(f"Backup not found: {backup_id}")
        
        # 1. ì²´í¬ì„¬ ê²€ì¦
        if verify:
            self.logger.info(f"Verifying backup integrity: {backup_id}")
            verification_result = await self.backup_manager.verify_backup(backup_entry)
            if not verification_result:
                raise ValueError(f"Backup integrity check failed: {backup_id}")
        
        # 2. ë°±ì—… íŒŒì¼ ì¡´ì¬ í™•ì¸
        backup_path = Path(backup_entry.backup_path)
        if not backup_path.exists():
            raise ValueError(f"Backup file not found: {backup_path}")
        
        # 3. ë‹¨ê³„ì  ë³µì› (ë¡œê·¸ ë¦¬í”Œë ˆì´ ë°©ì‹)
        self.logger.info(f"Starting step-by-step restoration: {backup_id}")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ë¨¼ì € ì••ì¶• í•´ì œ
        temp_extract_dir = Path(self.backup_manager.config.temp_dir) / f"extract_{backup_id}"
        temp_extract_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # ì••ì¶• í•´ì œ
            if backup_entry.compression == CompressionType.GZIP:
                import gzip
                import tarfile
                
                if backup_path.suffix == '.gz':
                    if backup_path.name.endswith('.tar.gz'):
                        # tar.gz ì••ì¶• í•´ì œ
                        with tarfile.open(backup_path, 'r:gz') as tar:
                            tar.extractall(temp_extract_dir)
                    else:
                        # gzip ì••ì¶• í•´ì œ
                        with gzip.open(backup_path, 'rb') as f_in:
                            extract_file = temp_extract_dir / backup_path.stem
                            with open(extract_file, 'wb') as f_out:
                                f_out.write(f_in.read())
            else:
                # ì••ì¶•ë˜ì§€ ì•Šì€ íŒŒì¼ ë³µì‚¬
                import shutil
                shutil.copy2(backup_path, temp_extract_dir)
            
            # 4. ìƒ˜í”Œ íŒŒì¼ ê²€ì¦ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)
            if test_mode and sample_files > 0:
                extracted_files = list(temp_extract_dir.rglob("*"))
                # íŒŒì¼ë§Œ í•„í„°ë§
                file_list = [f for f in extracted_files if f.is_file()]
                file_count = len(file_list)
                
                if file_count == 0:
                    raise ValueError("No files found in backup")
                
                # ìƒ˜í”Œ íŒŒì¼ ê²€ì¦
                sample_count = min(sample_files, file_count)
                verified_files = 0
                
                for file_path in file_list[:sample_count]:
                    # íŒŒì¼ ì½ê¸° í…ŒìŠ¤íŠ¸
                    try:
                        with open(file_path, 'rb') as f:
                            f.read(1024)  # ì²« 1KB ì½ê¸° í…ŒìŠ¤íŠ¸
                        verified_files += 1
                    except Exception as e:
                        self.logger.warning(f"File verification failed: {file_path} - {e}")
                
                if verified_files == 0:
                    raise ValueError("All sample files failed verification")
            
            # 5. ì‹¤ì œ ë³µì› (í…ŒìŠ¤íŠ¸ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°)
            if not test_mode:
                restore_target = Path(restore_path)
                restore_target.mkdir(parents=True, exist_ok=True)
                
                # íŒŒì¼ ë³µì‚¬
                import shutil
                for item in temp_extract_dir.iterdir():
                    if item.is_file():
                        shutil.copy2(item, restore_target)
                    elif item.is_dir():
                        shutil.copytree(item, restore_target / item.name, dirs_exist_ok=True)
            
            return {
                "action": "backup_restoration",
                "backup_id": backup_id,
                "restore_path": restore_path,
                "test_mode": test_mode,
                "verified": verify,
                "sample_files_verified": verified_files if test_mode else None,
                "success": True
            }
            
        finally:
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            import shutil
            if temp_extract_dir.exists():
                shutil.rmtree(temp_extract_dir, ignore_errors=True)
    
    async def _execute_archive_creation(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """ì•„ì¹´ì´ë¸Œ ìƒì„± ì‹¤í–‰"""
        age_days = parameters.get("age_days", 7)
        source_dir = parameters.get("source_dir", self.log_manager.config.log_dir)
        pattern = parameters.get("pattern", "*.log*")
        
        # ë‚˜ì´ ê¸°ì¤€ íŒŒì¼ í•„í„°ë§
        cutoff_time = datetime.now() - timedelta(days=age_days)
        
        archive_ids = await self.archive_manager.archive_files(
            source_dir=source_dir,
            pattern=pattern
        )
        
        return {
            "action": "archive_creation",
            "archive_ids": archive_ids,
            "source_dir": source_dir,
            "age_days": age_days,
            "cutoff_time": cutoff_time.isoformat()
        }
    
    async def _execute_cleanup(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """ì •ë¦¬ ì‘ì—… ì‹¤í–‰"""
        target = parameters.get("target", "temp_files")
        age_days = parameters.get("age_days", 1)
        
        cleaned_files = []
        cutoff_time = datetime.now() - timedelta(days=age_days)
        
        if target == "temp_files":
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            temp_dirs = [
                Path(self.backup_manager.config.temp_dir),
                Path(self.archive_manager.config.temp_dir)
            ]
            
            for temp_dir in temp_dirs:
                if temp_dir.exists():
                    for file_path in temp_dir.rglob("*"):
                        if (file_path.is_file() and 
                            file_path.stat().st_mtime < cutoff_time.timestamp()):
                            try:
                                file_path.unlink()
                                cleaned_files.append(str(file_path))
                            except Exception as e:
                                self.logger.warning(f"Failed to delete {file_path}: {e}")
        
        elif target == "test_restore":
            # í…ŒìŠ¤íŠ¸ ë³µì› ë””ë ‰í† ë¦¬ ì •ë¦¬
            test_restore_dir = Path(self.backup_manager.config.temp_dir) / "test_restore"
            if test_restore_dir.exists():
                import shutil
                shutil.rmtree(test_restore_dir, ignore_errors=True)
                cleaned_files.append(str(test_restore_dir))
        
        return {
            "action": "cleanup",
            "target": target,
            "cleaned_files": cleaned_files,
            "age_days": age_days
        }
    
    async def _execute_integrity_check(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤í–‰"""
        scope = parameters.get("scope", "recent")
        days = parameters.get("days", 7)
        
        results = {
            "action": "integrity_check",
            "scope": scope,
            "checked_backups": [],
            "failed_backups": [],
            "success_rate": 0.0
        }
        
        # ë°±ì—… ëª©ë¡ ì¡°íšŒ
        all_backups = await self.backup_manager.list_backups()
        
        if scope == "recent":
            cutoff_time = datetime.now() - timedelta(days=days)
            backups_to_check = [
                b for b in all_backups 
                if b.created_at >= cutoff_time
            ]
        else:
            backups_to_check = all_backups
        
        # ê° ë°±ì—… ê²€ì¦
        for backup in backups_to_check:
            try:
                verification_result = await self.backup_manager.verify_backup(backup)
                
                check_result = {
                    "backup_id": backup.backup_id,
                    "created_at": backup.created_at.isoformat(),
                    "verified": verification_result
                }
                
                results["checked_backups"].append(check_result)
                
                if not verification_result:
                    results["failed_backups"].append(backup.backup_id)
                    
            except Exception as e:
                self.logger.error(f"Integrity check failed for {backup.backup_id}: {e}")
                results["failed_backups"].append(backup.backup_id)
        
        # ì„±ê³µë¥  ê³„ì‚°
        total_checked = len(results["checked_backups"])
        if total_checked > 0:
            success_count = total_checked - len(results["failed_backups"])
            results["success_rate"] = (success_count / total_checked) * 100
        
        return results
    
    def get_operation_status(self, operation_id: str) -> Optional[Operation]:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        return self.operations.get(operation_id)
    
    def list_operations(self, status: Optional[OperationStatus] = None) -> List[Operation]:
        """ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
        operations = list(self.operations.values())
        
        if status:
            operations = [op for op in operations if op.status == status]
        
        return sorted(operations, key=lambda x: x.created_at, reverse=True)
    
    def get_workflow_definition(self, workflow_id: str) -> Optional[WorkflowDefinition]:
        """ì›Œí¬í”Œë¡œìš° ì •ì˜ ì¡°íšŒ"""
        return self.workflows.get(workflow_id)
    
    def list_workflows(self) -> List[WorkflowDefinition]:
        """ì›Œí¬í”Œë¡œìš° ëª©ë¡ ì¡°íšŒ"""
        return list(self.workflows.values())
    
    async def get_unified_status(self) -> Dict[str, Any]:
        """í†µí•© ìƒíƒœ ì¡°íšŒ"""
        return {
            "log_manager": self.log_manager.get_log_statistics(),
            "backup_manager": self.backup_manager.get_backup_statistics(),
            "archive_manager": self.archive_manager.get_archive_statistics(),
            "orchestrator": {
                "total_operations": len(self.operations),
                "active_operations": len([op for op in self.operations.values() 
                                        if op.status == OperationStatus.RUNNING]),
                "completed_operations": len([op for op in self.operations.values() 
                                           if op.status == OperationStatus.COMPLETED]),
                "failed_operations": len([op for op in self.operations.values() 
                                        if op.status == OperationStatus.FAILED]),
                "available_workflows": len(self.workflows)
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # í™œì„± ì‘ì—… ì·¨ì†Œ
            for task in self.active_operations.values():
                if not task.done():
                    task.cancel()
            
            # ê°œë³„ ê´€ë¦¬ì ì •ë¦¬
            if hasattr(self.log_manager, 'cleanup'):
                self.log_manager.cleanup()
            
            if hasattr(self.backup_manager, 'cleanup'):
                await self.backup_manager.cleanup()
            
            if hasattr(self.archive_manager, 'cleanup'):
                if asyncio.iscoroutinefunction(self.archive_manager.cleanup):
                    await self.archive_manager.cleanup()
                else:
                    self.archive_manager.cleanup()
            
            self.logger.info("Log Orchestrator cleanup completed")
            
        except Exception as e:
            self.logger.error(f"Orchestrator cleanup failed: {e}")

# ì „ì—­ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
_global_orchestrator = None

def get_log_orchestrator(log_config: Optional[LogConfig] = None,
                        backup_config: Optional[BackupConfig] = None,
                        archive_config: Optional[ArchiveConfig] = None) -> LogOrchestrator:
    """ì „ì—­ ë¡œê·¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë°˜í™˜"""
    global _global_orchestrator
    if _global_orchestrator is None:
        _global_orchestrator = LogOrchestrator(log_config, backup_config, archive_config)
    return _global_orchestrator

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    import asyncio
    
    async def test_orchestrator():
        print("ğŸ§ª Log Orchestrator í…ŒìŠ¤íŠ¸")
        
        # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”
        orchestrator = LogOrchestrator()
        
        print("\n1ï¸âƒ£ í†µí•© ìƒíƒœ í™•ì¸")
        status = await orchestrator.get_unified_status()
        print(f"  ë¡œê·¸ ë§¤ë‹ˆì €: {status['log_manager']['total_logs']} logs")
        print(f"  ë°±ì—… ë§¤ë‹ˆì €: {status['backup_manager']['total_backups']} backups")
        print(f"  ì•„ì¹´ì´ë¸Œ ë§¤ë‹ˆì €: {status['archive_manager']['total_archives']} archives")
        
        print("\n2ï¸âƒ£ ì›Œí¬í”Œë¡œìš° ëª©ë¡")
        workflows = orchestrator.list_workflows()
        for workflow in workflows:
            print(f"  {workflow.workflow_id}: {workflow.name}")
        
        print("\n3ï¸âƒ£ ë°±ì—… ë¬´ê²°ì„± ê²€ì‚¬ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰")
        try:
            workflow_id = await orchestrator.execute_workflow("backup_integrity_check")
            print(f"  ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ID: {workflow_id}")
        except Exception as e:
            print(f"  ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        
        print("\n4ï¸âƒ£ ì‘ì—… ëª©ë¡")
        operations = orchestrator.list_operations()
        for op in operations[:3]:  # ìµœê·¼ 3ê°œë§Œ
            print(f"  {op.operation_id}: {op.status.value} ({op.operation_type.value})")
        
        print("\nğŸ‰ Log Orchestrator í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
        # ì •ë¦¬
        await orchestrator.cleanup()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_orchestrator())