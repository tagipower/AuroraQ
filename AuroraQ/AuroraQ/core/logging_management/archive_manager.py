#!/usr/bin/env python3
"""
ë¡œê·¸ ì•„ì¹´ì´ë¸Œ ë° ìˆœí™˜ ê´€ë¦¬ ì‹œìŠ¤í…œ
P7-3: ë¡œê·¸ ìˆœí™˜ ë° ì•„ì¹´ì´ë¸Œ ì‹œìŠ¤í…œ
"""

import sys
import os
import shutil
import gzip
import zipfile
import tarfile
import sqlite3
import asyncio
import threading
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import json
import warnings
from collections import defaultdict
import tempfile
import fnmatch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

warnings.filterwarnings('ignore', category=UserWarning)

class ArchivePolicy(Enum):
    """ì•„ì¹´ì´ë¸Œ ì •ì±…"""
    TIME_BASED = "time_based"      # ì‹œê°„ ê¸°ë°˜
    SIZE_BASED = "size_based"      # í¬ê¸° ê¸°ë°˜
    COUNT_BASED = "count_based"    # ê°œìˆ˜ ê¸°ë°˜
    HYBRID = "hybrid"              # ë³µí•© ì •ì±…

class CompressionLevel(Enum):
    """ì••ì¶• ë ˆë²¨"""
    NONE = 0
    FAST = 1
    NORMAL = 6
    BEST = 9

class StorageTier(Enum):
    """ìŠ¤í† ë¦¬ì§€ ê³„ì¸µ"""
    HOT = "hot"          # ë¹ ë¥¸ ì•¡ì„¸ìŠ¤ (SSD)
    WARM = "warm"        # ì¤‘ê°„ ì•¡ì„¸ìŠ¤ (HDD)
    COLD = "cold"        # ëŠë¦° ì•¡ì„¸ìŠ¤ (ì•„ì¹´ì´ë¸Œ)
    FROZEN = "frozen"    # ì¥ê¸° ë³´ê´€ (í…Œì´í”„/í´ë¼ìš°ë“œ)

@dataclass
class ArchiveConfig:
    """ì•„ì¹´ì´ë¸Œ ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    archive_dir: str = "archives"
    temp_dir: str = "temp_archive"
    source_patterns: List[str] = field(default_factory=lambda: ["*.log", "*.log.*"])
    
    # ì•„ì¹´ì´ë¸Œ ì •ì±…
    policy: ArchivePolicy = ArchivePolicy.HYBRID
    
    # ì‹œê°„ ê¸°ë°˜ ì„¤ì •
    archive_after_days: int = 7
    delete_after_days: int = 90
    
    # í¬ê¸° ê¸°ë°˜ ì„¤ì •
    max_file_size_mb: int = 100
    max_total_size_gb: float = 10.0
    
    # ê°œìˆ˜ ê¸°ë°˜ ì„¤ì •
    max_files_per_directory: int = 1000
    max_archive_count: int = 500
    
    # ì••ì¶• ì„¤ì •
    compression_enabled: bool = True
    compression_level: CompressionLevel = CompressionLevel.NORMAL
    compression_format: str = "gzip"  # gzip, bzip2, lzma
    
    # ìŠ¤í† ë¦¬ì§€ ê³„ì¸µ
    enable_tiered_storage: bool = True
    hot_storage_days: int = 7
    warm_storage_days: int = 30
    cold_storage_days: int = 90
    
    # ê²€ì¦ ë° ë¬´ê²°ì„±
    verify_archives: bool = True
    checksum_algorithm: str = "sha256"
    duplicate_detection: bool = True
    
    # ì¸ë±ì‹±
    enable_indexing: bool = True
    index_content: bool = True
    full_text_search: bool = False
    
    # ì„±ëŠ¥
    parallel_processing: bool = True
    max_workers: int = 4
    chunk_size_mb: int = 64
    
    # ì •ë¦¬
    auto_cleanup: bool = True
    cleanup_interval_hours: int = 24

@dataclass
class ArchiveEntry:
    """ì•„ì¹´ì´ë¸Œ ì—”íŠ¸ë¦¬"""
    archive_id: str
    original_path: str
    archive_path: str
    storage_tier: StorageTier
    created_at: datetime
    original_size_bytes: int
    compressed_size_bytes: int
    compression_ratio: float
    checksum: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    indexed: bool = False
    access_count: int = 0
    last_accessed: Optional[datetime] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            'archive_id': self.archive_id,
            'original_path': self.original_path,
            'archive_path': self.archive_path,
            'storage_tier': self.storage_tier.value,
            'created_at': self.created_at.isoformat(),
            'original_size_bytes': self.original_size_bytes,
            'compressed_size_bytes': self.compressed_size_bytes,
            'compression_ratio': self.compression_ratio,
            'checksum': self.checksum,
            'metadata': self.metadata,
            'indexed': self.indexed,
            'access_count': self.access_count,
            'last_accessed': self.last_accessed.isoformat() if self.last_accessed else None
        }

@dataclass
class ArchiveIndex:
    """ì•„ì¹´ì´ë¸Œ ì¸ë±ìŠ¤"""
    archive_id: str
    file_path: str
    line_number: int
    content_hash: str
    keywords: List[str] = field(default_factory=list)
    timestamp: Optional[datetime] = None
    level: Optional[str] = None
    module: Optional[str] = None

@dataclass
class ArchiveMetrics:
    """ì•„ì¹´ì´ë¸Œ ë©”íŠ¸ë¦­"""
    total_archives: int = 0
    total_original_size_bytes: int = 0
    total_compressed_size_bytes: int = 0
    avg_compression_ratio: float = 0.0
    storage_efficiency: float = 0.0
    hot_storage_count: int = 0
    warm_storage_count: int = 0
    cold_storage_count: int = 0
    frozen_storage_count: int = 0
    last_archive_time: Optional[datetime] = None
    last_cleanup_time: Optional[datetime] = None

class ArchiveManager:
    """ë¡œê·¸ ì•„ì¹´ì´ë¸Œ ë° ìˆœí™˜ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Optional[ArchiveConfig] = None, config_file: str = "archive_config.json"):
        self.config = config or ArchiveConfig()
        self.config_file = config_file
        
        # Logger ë¨¼ì € ì´ˆê¸°í™”
        try:
            import logging
            self.logger = logging.getLogger("ArchiveManager")
        except Exception:
            import logging
            logging.basicConfig()
            self.logger = logging.getLogger("ArchiveManager")
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.archive_dir = Path(self.config.archive_dir)
        self.temp_dir = Path(self.config.temp_dir)
        self.archive_dir.mkdir(parents=True, exist_ok=True)
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        
        # ìŠ¤í† ë¦¬ì§€ ê³„ì¸µ ë””ë ‰í† ë¦¬
        if self.config.enable_tiered_storage:
            for tier in StorageTier:
                tier_dir = self.archive_dir / tier.value
                tier_dir.mkdir(parents=True, exist_ok=True)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
        self.db_path = self.archive_dir / "archive_registry.db"
        self._init_database()
        
        # ì•„ì¹´ì´ë¸Œ ìƒíƒœ ì¶”ì 
        self.active_archives: Dict[str, ArchiveEntry] = {}
        self.metrics = ArchiveMetrics()
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
        self._cleanup_active = False
        self._cleanup_thread = None
        
        # ì„¤ì • ë¡œë“œ
        self._load_configuration()
        
        # ìë™ ì •ë¦¬ ì‹œì‘
        if self.config.auto_cleanup:
            self.start_auto_cleanup()
        
        self.logger.info("Archive manager initialized")
    
    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                # ì•„ì¹´ì´ë¸Œ í…Œì´ë¸”
                conn.execute("""
                CREATE TABLE IF NOT EXISTS archives (
                    archive_id TEXT PRIMARY KEY,
                    original_path TEXT NOT NULL,
                    archive_path TEXT NOT NULL,
                    storage_tier TEXT NOT NULL,
                    created_at TEXT NOT NULL,
                    original_size_bytes INTEGER DEFAULT 0,
                    compressed_size_bytes INTEGER DEFAULT 0,
                    compression_ratio REAL DEFAULT 0.0,
                    checksum TEXT,
                    metadata TEXT,
                    indexed BOOLEAN DEFAULT 0,
                    access_count INTEGER DEFAULT 0,
                    last_accessed TEXT
                )
                """)
                
                # ì¸ë±ìŠ¤ í…Œì´ë¸”
                conn.execute("""
                CREATE TABLE IF NOT EXISTS archive_index (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    archive_id TEXT NOT NULL,
                    file_path TEXT NOT NULL,
                    line_number INTEGER,
                    content_hash TEXT,
                    keywords TEXT,
                    timestamp TEXT,
                    level TEXT,
                    module TEXT,
                    FOREIGN KEY (archive_id) REFERENCES archives (archive_id)
                )
                """)
                
                # ì¤‘ë³µ ê°ì§€ í…Œì´ë¸”
                conn.execute("""
                CREATE TABLE IF NOT EXISTS duplicates (
                    content_hash TEXT PRIMARY KEY,
                    archive_id TEXT NOT NULL,
                    original_path TEXT NOT NULL,
                    created_at TEXT NOT NULL,
                    FOREIGN KEY (archive_id) REFERENCES archives (archive_id)
                )
                """)
                
                # ì¸ë±ìŠ¤ ìƒì„±
                conn.execute("CREATE INDEX IF NOT EXISTS idx_archive_created_at ON archives(created_at)")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_archive_tier ON archives(storage_tier)")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_archive_checksum ON archives(checksum)")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_index_archive_id ON archive_index(archive_id)")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_index_keywords ON archive_index(keywords)")
                
                conn.commit()
                
        except Exception as e:
            self.logger.error(f"Database initialization failed: {e}")
    
    def _load_configuration(self):
        """ì„¤ì • ë¡œë“œ"""
        try:
            if Path(self.config_file).exists():
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                
                # ì„¤ì • ì—…ë°ì´íŠ¸
                for key, value in config_data.items():
                    if hasattr(self.config, key):
                        if key in ['policy', 'compression_level']:
                            # Enum íƒ€ì… ì²˜ë¦¬
                            enum_class = ArchivePolicy if key == 'policy' else CompressionLevel
                            setattr(self.config, key, enum_class(value))
                        else:
                            setattr(self.config, key, value)
                
                self.logger.info(f"Archive configuration loaded from {self.config_file}")
        except Exception as e:
            self.logger.warning(f"Failed to load archive configuration: {e}")
    
    def _save_configuration(self):
        """ì„¤ì • ì €ì¥"""
        try:
            config_data = {
                'archive_dir': self.config.archive_dir,
                'temp_dir': self.config.temp_dir,
                'source_patterns': self.config.source_patterns,
                'policy': self.config.policy.value,
                'archive_after_days': self.config.archive_after_days,
                'delete_after_days': self.config.delete_after_days,
                'max_file_size_mb': self.config.max_file_size_mb,
                'max_total_size_gb': self.config.max_total_size_gb,
                'max_files_per_directory': self.config.max_files_per_directory,
                'max_archive_count': self.config.max_archive_count,
                'compression_enabled': self.config.compression_enabled,
                'compression_level': self.config.compression_level.value,
                'compression_format': self.config.compression_format,
                'enable_tiered_storage': self.config.enable_tiered_storage,
                'hot_storage_days': self.config.hot_storage_days,
                'warm_storage_days': self.config.warm_storage_days,
                'cold_storage_days': self.config.cold_storage_days,
                'verify_archives': self.config.verify_archives,
                'checksum_algorithm': self.config.checksum_algorithm,
                'duplicate_detection': self.config.duplicate_detection,
                'enable_indexing': self.config.enable_indexing,
                'index_content': self.config.index_content,
                'full_text_search': self.config.full_text_search,
                'parallel_processing': self.config.parallel_processing,
                'max_workers': self.config.max_workers,
                'chunk_size_mb': self.config.chunk_size_mb,
                'auto_cleanup': self.config.auto_cleanup,
                'cleanup_interval_hours': self.config.cleanup_interval_hours
            }
            
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            self.logger.error(f"Failed to save archive configuration: {e}")
    
    async def archive_files(self, source_dir: Union[str, Path], 
                          pattern: Optional[str] = None) -> List[str]:
        """íŒŒì¼ë“¤ì„ ì•„ì¹´ì´ë¸Œ"""
        try:
            source_path = Path(source_dir)
            if not source_path.exists():
                raise Exception(f"Source directory not found: {source_path}")
            
            # íŒ¨í„´ ì„¤ì •
            patterns = [pattern] if pattern else self.config.source_patterns
            
            # ì•„ì¹´ì´ë¸Œí•  íŒŒì¼ë“¤ ì°¾ê¸°
            files_to_archive = []
            for pattern in patterns:
                files_to_archive.extend(source_path.glob(pattern))
            
            # ì•„ì¹´ì´ë¸Œ ì •ì±…ì— ë”°ë¥¸ í•„í„°ë§
            filtered_files = await self._filter_files_for_archive(files_to_archive)
            
            # ë³‘ë ¬ ì²˜ë¦¬
            if self.config.parallel_processing and len(filtered_files) > 1:
                archive_ids = await self._archive_files_parallel(filtered_files)
            else:
                archive_ids = []
                for file_path in filtered_files:
                    archive_id = await self._archive_single_file(file_path)
                    if archive_id:
                        archive_ids.append(archive_id)
            
            self.logger.info(f"Archived {len(archive_ids)} files from {source_dir}")
            return archive_ids
            
        except Exception as e:
            self.logger.error(f"File archiving failed: {e}")
            return []
    
    async def _filter_files_for_archive(self, files: List[Path]) -> List[Path]:
        """ì•„ì¹´ì´ë¸Œí•  íŒŒì¼ í•„í„°ë§"""
        filtered_files = []
        current_time = datetime.now()
        
        try:
            for file_path in files:
                if not file_path.is_file():
                    continue
                
                # ì •ì±…ì— ë”°ë¥¸ í•„í„°ë§
                should_archive = False
                file_stat = file_path.stat()
                file_age = current_time - datetime.fromtimestamp(file_stat.st_mtime)
                file_size_mb = file_stat.st_size / (1024 * 1024)
                
                if self.config.policy == ArchivePolicy.TIME_BASED:
                    should_archive = file_age.days >= self.config.archive_after_days
                    
                elif self.config.policy == ArchivePolicy.SIZE_BASED:
                    should_archive = file_size_mb >= self.config.max_file_size_mb
                    
                elif self.config.policy == ArchivePolicy.COUNT_BASED:
                    # ë””ë ‰í† ë¦¬ë‹¹ íŒŒì¼ ìˆ˜ í™•ì¸
                    parent_files = len(list(file_path.parent.glob("*")))
                    should_archive = parent_files > self.config.max_files_per_directory
                    
                else:  # HYBRID
                    should_archive = (
                        file_age.days >= self.config.archive_after_days or
                        file_size_mb >= self.config.max_file_size_mb
                    )
                
                # ì¤‘ë³µ ê²€ì‚¬
                if should_archive and self.config.duplicate_detection:
                    checksum = await self._calculate_file_checksum(file_path)
                    if await self._is_duplicate(checksum):
                        self.logger.debug(f"Skipping duplicate file: {file_path}")
                        continue
                
                if should_archive:
                    filtered_files.append(file_path)
            
            return filtered_files
            
        except Exception as e:
            self.logger.error(f"File filtering failed: {e}")
            return []
    
    async def _archive_files_parallel(self, files: List[Path]) -> List[str]:
        """ë³‘ë ¬ íŒŒì¼ ì•„ì¹´ì´ë¸Œ"""
        import concurrent.futures
        
        archive_ids = []
        
        try:
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                # ì‘ì—… ì œì¶œ
                future_to_file = {
                    executor.submit(self._archive_single_file_sync, file_path): file_path 
                    for file_path in files
                }
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for future in concurrent.futures.as_completed(future_to_file):
                    file_path = future_to_file[future]
                    try:
                        archive_id = future.result()
                        if archive_id:
                            archive_ids.append(archive_id)
                    except Exception as e:
                        self.logger.error(f"Failed to archive {file_path}: {e}")
            
            return archive_ids
            
        except Exception as e:
            self.logger.error(f"Parallel archiving failed: {e}")
            return []
    
    def _archive_single_file_sync(self, file_path: Path) -> Optional[str]:
        """ë‹¨ì¼ íŒŒì¼ ì•„ì¹´ì´ë¸Œ (ë™ê¸° ë²„ì „)"""
        import asyncio
        return asyncio.run(self._archive_single_file(file_path))
    
    async def _archive_single_file(self, file_path: Path) -> Optional[str]:
        """ë‹¨ì¼ íŒŒì¼ ì•„ì¹´ì´ë¸Œ"""
        try:
            # ì•„ì¹´ì´ë¸Œ ID ìƒì„±
            archive_id = f"arch_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file_path.stem}"
            
            # ìŠ¤í† ë¦¬ì§€ ê³„ì¸µ ê²°ì •
            storage_tier = self._determine_storage_tier(file_path)
            
            # ì•„ì¹´ì´ë¸Œ ê²½ë¡œ ê²°ì •
            tier_dir = self.archive_dir / storage_tier.value if self.config.enable_tiered_storage else self.archive_dir
            archive_filename = f"{archive_id}.{self._get_archive_extension()}"
            archive_path = tier_dir / archive_filename
            
            # ì›ë³¸ íŒŒì¼ ì •ë³´
            file_stat = file_path.stat()
            original_size = file_stat.st_size
            
            # ì²´í¬ì„¬ ê³„ì‚°
            checksum = await self._calculate_file_checksum(file_path)
            
            # ì••ì¶• ë° ì•„ì¹´ì´ë¸Œ
            if self.config.compression_enabled:
                compressed_size = await self._compress_file(file_path, archive_path)
            else:
                shutil.copy2(file_path, archive_path)
                compressed_size = original_size
            
            # ì••ì¶• ë¹„ìœ¨ ê³„ì‚°
            compression_ratio = (original_size - compressed_size) / max(1, original_size) if original_size > 0 else 0.0
            
            # ì•„ì¹´ì´ë¸Œ ì—”íŠ¸ë¦¬ ìƒì„±
            archive_entry = ArchiveEntry(
                archive_id=archive_id,
                original_path=str(file_path),
                archive_path=str(archive_path),
                storage_tier=storage_tier,
                created_at=datetime.now(),
                original_size_bytes=original_size,
                compressed_size_bytes=compressed_size,
                compression_ratio=compression_ratio,
                checksum=checksum
            )
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            self._save_archive_entry(archive_entry)
            
            # ì¤‘ë³µ ê²€ì‚¬ ì •ë³´ ì €ì¥
            if self.config.duplicate_detection:
                await self._save_duplicate_info(checksum, archive_id, str(file_path))
            
            # ì¸ë±ì‹±
            if self.config.enable_indexing:
                await self._index_archive(archive_entry)
            
            # ê²€ì¦
            if self.config.verify_archives:
                await self._verify_archive(archive_entry)
            
            # ì›ë³¸ íŒŒì¼ ì‚­ì œ (ì•„ì¹´ì´ë¸Œ ì™„ë£Œ í›„)
            file_path.unlink()
            
            self.logger.debug(f"Archived file: {file_path} -> {archive_path}")
            return archive_id
            
        except Exception as e:
            self.logger.error(f"Failed to archive file {file_path}: {e}")
            return None
    
    def _determine_storage_tier(self, file_path: Path) -> StorageTier:
        """ìŠ¤í† ë¦¬ì§€ ê³„ì¸µ ê²°ì •"""
        try:
            if not self.config.enable_tiered_storage:
                return StorageTier.HOT
            
            file_age = datetime.now() - datetime.fromtimestamp(file_path.stat().st_mtime)
            
            if file_age.days <= self.config.hot_storage_days:
                return StorageTier.HOT
            elif file_age.days <= self.config.warm_storage_days:
                return StorageTier.WARM
            elif file_age.days <= self.config.cold_storage_days:
                return StorageTier.COLD
            else:
                return StorageTier.FROZEN
                
        except Exception:
            return StorageTier.HOT
    
    def _get_archive_extension(self) -> str:
        """ì•„ì¹´ì´ë¸Œ í™•ì¥ì ë°˜í™˜"""
        if not self.config.compression_enabled:
            return "archive"
        
        if self.config.compression_format == "gzip":
            return "gz"
        elif self.config.compression_format == "bzip2":
            return "bz2"
        elif self.config.compression_format == "lzma":
            return "xz"
        else:
            return "archive"
    
    async def _compress_file(self, source_path: Path, archive_path: Path) -> int:
        """íŒŒì¼ ì••ì¶•"""
        try:
            if self.config.compression_format == "gzip":
                with open(source_path, 'rb') as f_in:
                    with gzip.open(archive_path, 'wb', compresslevel=self.config.compression_level.value) as f_out:
                        await self._copy_file_async(f_in, f_out)
                        
            elif self.config.compression_format == "bzip2":
                import bz2
                with open(source_path, 'rb') as f_in:
                    with bz2.open(archive_path, 'wb', compresslevel=self.config.compression_level.value) as f_out:
                        await self._copy_file_async(f_in, f_out)
                        
            elif self.config.compression_format == "lzma":
                import lzma
                with open(source_path, 'rb') as f_in:
                    with lzma.open(archive_path, 'wb', preset=self.config.compression_level.value) as f_out:
                        await self._copy_file_async(f_in, f_out)
            
            return archive_path.stat().st_size
            
        except Exception as e:
            self.logger.error(f"File compression failed: {e}")
            raise
    
    async def _copy_file_async(self, f_in, f_out):
        """ë¹„ë™ê¸° íŒŒì¼ ë³µì‚¬"""
        chunk_size = self.config.chunk_size_mb * 1024 * 1024
        
        while True:
            chunk = f_in.read(chunk_size)
            if not chunk:
                break
            f_out.write(chunk)
            await asyncio.sleep(0.001)  # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ì–‘ë³´
    
    async def _calculate_file_checksum(self, file_path: Path) -> str:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚°"""
        try:
            hash_obj = hashlib.new(self.config.checksum_algorithm)
            
            with open(file_path, 'rb') as f:
                while chunk := f.read(self.config.chunk_size_mb * 1024 * 1024):
                    hash_obj.update(chunk)
                    await asyncio.sleep(0.001)
            
            return hash_obj.hexdigest()
            
        except Exception as e:
            self.logger.error(f"Checksum calculation failed: {e}")
            return ""
    
    async def _is_duplicate(self, checksum: str) -> bool:
        """ì¤‘ë³µ íŒŒì¼ í™•ì¸"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                cursor = conn.execute("SELECT COUNT(*) FROM duplicates WHERE content_hash = ?", (checksum,))
                count = cursor.fetchone()[0]
                return count > 0
                
        except Exception:
            return False
    
    async def _save_duplicate_info(self, checksum: str, archive_id: str, original_path: str):
        """ì¤‘ë³µ ì •ë³´ ì €ì¥"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                conn.execute("""
                INSERT OR REPLACE INTO duplicates 
                (content_hash, archive_id, original_path, created_at)
                VALUES (?, ?, ?, ?)
                """, (checksum, archive_id, original_path, datetime.now().isoformat()))
                conn.commit()
                
        except Exception as e:
            self.logger.error(f"Failed to save duplicate info: {e}")
    
    async def _index_archive(self, archive_entry: ArchiveEntry):
        """ì•„ì¹´ì´ë¸Œ ì¸ë±ì‹±"""
        try:
            if not self.config.index_content:
                return
            
            # ì••ì¶•ëœ íŒŒì¼ ì½ê¸°
            content_lines = await self._read_archive_content(archive_entry)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì¸ë±ì‹±
            for line_num, line in enumerate(content_lines, 1):
                # ë¡œê·¸ ë ˆë²¨ ì¶”ì¶œ
                level = self._extract_log_level(line)
                
                # ëª¨ë“ˆëª… ì¶”ì¶œ
                module = self._extract_module_name(line)
                
                # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
                timestamp = self._extract_timestamp(line)
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self._extract_keywords(line)
                
                # ì½˜í…ì¸  í•´ì‹œ
                content_hash = hashlib.sha256(line.encode()).hexdigest()[:16]
                
                # ì¸ë±ìŠ¤ ì €ì¥
                await self._save_index_entry(
                    archive_entry.archive_id,
                    archive_entry.original_path,
                    line_num,
                    content_hash,
                    keywords,
                    timestamp,
                    level,
                    module
                )
            
            # ì¸ë±ì‹± ì™„ë£Œ í‘œì‹œ
            archive_entry.indexed = True
            self._save_archive_entry(archive_entry)
            
        except Exception as e:
            self.logger.error(f"Archive indexing failed: {e}")
    
    async def _read_archive_content(self, archive_entry: ArchiveEntry) -> List[str]:
        """ì•„ì¹´ì´ë¸Œ ë‚´ìš© ì½ê¸°"""
        try:
            archive_path = Path(archive_entry.archive_path)
            
            if self.config.compression_format == "gzip":
                with gzip.open(archive_path, 'rt', encoding='utf-8') as f:
                    return f.readlines()
                    
            elif self.config.compression_format == "bzip2":
                import bz2
                with bz2.open(archive_path, 'rt', encoding='utf-8') as f:
                    return f.readlines()
                    
            elif self.config.compression_format == "lzma":
                import lzma
                with lzma.open(archive_path, 'rt', encoding='utf-8') as f:
                    return f.readlines()
            else:
                with open(archive_path, 'r', encoding='utf-8') as f:
                    return f.readlines()
                    
        except Exception as e:
            self.logger.error(f"Failed to read archive content: {e}")
            return []
    
    def _extract_log_level(self, line: str) -> Optional[str]:
        """ë¡œê·¸ ë ˆë²¨ ì¶”ì¶œ"""
        import re
        
        levels = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']
        for level in levels:
            if re.search(rf'\b{level}\b', line, re.IGNORECASE):
                return level.upper()
        return None
    
    def _extract_module_name(self, line: str) -> Optional[str]:
        """ëª¨ë“ˆëª… ì¶”ì¶œ"""
        import re
        
        # ì¼ë°˜ì ì¸ ë¡œê·¸ íŒ¨í„´ì—ì„œ ëª¨ë“ˆëª… ì¶”ì¶œ
        patterns = [
            r'- ([a-zA-Z_][a-zA-Z0-9_.]*) -',  # logger name pattern
            r'\.([a-zA-Z_][a-zA-Z0-9_]*):',    # module.function: pattern
        ]
        
        for pattern in patterns:
            match = re.search(pattern, line)
            if match:
                return match.group(1)
        
        return None
    
    def _extract_timestamp(self, line: str) -> Optional[datetime]:
        """íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ"""
        import re
        
        # ì¼ë°˜ì ì¸ íƒ€ì„ìŠ¤íƒ¬í”„ íŒ¨í„´ë“¤
        patterns = [
            r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})',
            r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})',
            r'(\d{2}/\d{2}/\d{4} \d{2}:\d{2}:\d{2})',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, line)
            if match:
                timestamp_str = match.group(1)
                try:
                    # ë‹¤ì–‘í•œ í¬ë§· ì‹œë„
                    for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%dT%H:%M:%S', '%m/%d/%Y %H:%M:%S']:
                        try:
                            return datetime.strptime(timestamp_str, fmt)
                        except ValueError:
                            continue
                except ValueError:
                    pass
        
        return None
    
    def _extract_keywords(self, line: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        import re
        
        # ë‹¨ì–´ ì¶”ì¶œ (3ê¸€ì ì´ìƒ)
        words = re.findall(r'\b[a-zA-Z]{3,}\b', line.lower())
        
        # ë¡œê·¸ ë ˆë²¨, ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸
        exclude_words = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'who', 'boy', 'did', 'way', 'when', 'what', 'with'}
        
        keywords = [word for word in set(words) if word not in exclude_words and len(word) >= 3]
        return keywords[:10]  # ìµœëŒ€ 10ê°œ í‚¤ì›Œë“œ
    
    async def _save_index_entry(self, archive_id: str, file_path: str, line_number: int,
                              content_hash: str, keywords: List[str], timestamp: Optional[datetime],
                              level: Optional[str], module: Optional[str]):
        """ì¸ë±ìŠ¤ ì—”íŠ¸ë¦¬ ì €ì¥"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                conn.execute("""
                INSERT INTO archive_index 
                (archive_id, file_path, line_number, content_hash, keywords, timestamp, level, module)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    archive_id,
                    file_path,
                    line_number,
                    content_hash,
                    json.dumps(keywords),
                    timestamp.isoformat() if timestamp else None,
                    level,
                    module
                ))
                conn.commit()
                
        except Exception as e:
            self.logger.error(f"Failed to save index entry: {e}")
    
    async def _verify_archive(self, archive_entry: ArchiveEntry):
        """ì•„ì¹´ì´ë¸Œ ê²€ì¦"""
        try:
            archive_path = Path(archive_entry.archive_path)
            if not archive_path.exists():
                raise Exception("Archive file not found")
            
            # ì²´í¬ì„¬ ê²€ì¦
            calculated_checksum = await self._calculate_file_checksum(archive_path)
            
            # ì••ì¶• íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬
            content_lines = await self._read_archive_content(archive_entry)
            if not content_lines:
                raise Exception("Archive content is empty or corrupted")
            
            self.logger.debug(f"Archive verification successful: {archive_entry.archive_id}")
            
        except Exception as e:
            self.logger.error(f"Archive verification failed: {e}")
            raise
    
    def _save_archive_entry(self, entry: ArchiveEntry):
        """ì•„ì¹´ì´ë¸Œ ì—”íŠ¸ë¦¬ ì €ì¥"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                conn.execute("""
                INSERT OR REPLACE INTO archives 
                (archive_id, original_path, archive_path, storage_tier, created_at,
                 original_size_bytes, compressed_size_bytes, compression_ratio,
                 checksum, metadata, indexed, access_count, last_accessed)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    entry.archive_id,
                    entry.original_path,
                    entry.archive_path,
                    entry.storage_tier.value,
                    entry.created_at.isoformat(),
                    entry.original_size_bytes,
                    entry.compressed_size_bytes,
                    entry.compression_ratio,
                    entry.checksum,
                    json.dumps(entry.metadata),
                    entry.indexed,
                    entry.access_count,
                    entry.last_accessed.isoformat() if entry.last_accessed else None
                ))
                conn.commit()
                
        except Exception as e:
            self.logger.error(f"Failed to save archive entry: {e}")
    
    async def search_archives(self, query: str, level: Optional[str] = None,
                            module: Optional[str] = None, start_time: Optional[datetime] = None,
                            end_time: Optional[datetime] = None, limit: int = 100) -> List[Dict[str, Any]]:
        """ì•„ì¹´ì´ë¸Œ ê²€ìƒ‰"""
        try:
            if not self.config.enable_indexing:
                return []
            
            with sqlite3.connect(str(self.db_path)) as conn:
                # ê²€ìƒ‰ ì¡°ê±´ êµ¬ì„±
                where_conditions = []
                params = []
                
                if query:
                    where_conditions.append("keywords LIKE ?")
                    params.append(f"%{query.lower()}%")
                
                if level:
                    where_conditions.append("level = ?")
                    params.append(level.upper())
                
                if module:
                    where_conditions.append("module = ?")
                    params.append(module)
                
                if start_time:
                    where_conditions.append("timestamp >= ?")
                    params.append(start_time.isoformat())
                
                if end_time:
                    where_conditions.append("timestamp <= ?")
                    params.append(end_time.isoformat())
                
                where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"
                
                # ê²€ìƒ‰ ì‹¤í–‰
                cursor = conn.execute(f"""
                SELECT ai.*, a.original_path, a.archive_path, a.storage_tier
                FROM archive_index ai
                JOIN archives a ON ai.archive_id = a.archive_id
                WHERE {where_clause}
                ORDER BY ai.timestamp DESC
                LIMIT ?
                """, params + [limit])
                
                results = []
                for row in cursor.fetchall():
                    results.append({
                        'archive_id': row[1],
                        'file_path': row[2],
                        'line_number': row[3],
                        'content_hash': row[4],
                        'keywords': json.loads(row[5]) if row[5] else [],
                        'timestamp': row[6],
                        'level': row[7],
                        'module': row[8],
                        'original_path': row[9],
                        'archive_path': row[10],
                        'storage_tier': row[11]
                    })
                
                return results
                
        except Exception as e:
            self.logger.error(f"Archive search failed: {e}")
            return []
    
    async def migrate_storage_tier(self, archive_id: str, target_tier: StorageTier) -> bool:
        """ìŠ¤í† ë¦¬ì§€ ê³„ì¸µ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        try:
            # ì•„ì¹´ì´ë¸Œ ì •ë³´ ì¡°íšŒ
            archive_entry = self._get_archive_entry(archive_id)
            if not archive_entry:
                raise Exception(f"Archive not found: {archive_id}")
            
            if archive_entry.storage_tier == target_tier:
                return True  # ì´ë¯¸ ëª©í‘œ ê³„ì¸µì— ìˆìŒ
            
            # ìƒˆ ê²½ë¡œ ê²°ì •
            new_tier_dir = self.archive_dir / target_tier.value
            new_tier_dir.mkdir(parents=True, exist_ok=True)
            
            old_path = Path(archive_entry.archive_path)
            new_path = new_tier_dir / old_path.name
            
            # íŒŒì¼ ì´ë™
            shutil.move(old_path, new_path)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
            archive_entry.archive_path = str(new_path)
            archive_entry.storage_tier = target_tier
            self._save_archive_entry(archive_entry)
            
            self.logger.info(f"Migrated archive {archive_id} to {target_tier.value}")
            return True
            
        except Exception as e:
            self.logger.error(f"Storage tier migration failed: {e}")
            return False
    
    def _get_archive_entry(self, archive_id: str) -> Optional[ArchiveEntry]:
        """ì•„ì¹´ì´ë¸Œ ì—”íŠ¸ë¦¬ ì¡°íšŒ"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                cursor = conn.execute("SELECT * FROM archives WHERE archive_id = ?", (archive_id,))
                row = cursor.fetchone()
                
                if row:
                    return ArchiveEntry(
                        archive_id=row[0],
                        original_path=row[1],
                        archive_path=row[2],
                        storage_tier=StorageTier(row[3]),
                        created_at=datetime.fromisoformat(row[4]),
                        original_size_bytes=row[5],
                        compressed_size_bytes=row[6],
                        compression_ratio=row[7],
                        checksum=row[8] or "",
                        metadata=json.loads(row[9]) if row[9] else {},
                        indexed=bool(row[10]),
                        access_count=row[11],
                        last_accessed=datetime.fromisoformat(row[12]) if row[12] else None
                    )
                
                return None
                
        except Exception as e:
            self.logger.error(f"Failed to get archive entry: {e}")
            return None
    
    def start_auto_cleanup(self):
        """ìë™ ì •ë¦¬ ì‹œì‘"""
        if self._cleanup_active:
            return
        
        self._cleanup_active = True
        
        def cleanup_loop():
            while self._cleanup_active:
                try:
                    # ì •ë¦¬ ì‘ì—… ì‹¤í–‰
                    self._perform_cleanup()
                    
                    # ìŠ¤í† ë¦¬ì§€ ê³„ì¸µ ë§ˆì´ê·¸ë ˆì´ì…˜
                    self._perform_tier_migration()
                    
                    # ëŒ€ê¸°
                    for _ in range(self.config.cleanup_interval_hours * 3600):
                        if not self._cleanup_active:
                            break
                        import time
                    time.sleep(1)
                        
                except Exception as e:
                    self.logger.error(f"Cleanup loop error: {e}")
                    time.sleep(3600)  # 1ì‹œê°„ í›„ ì¬ì‹œë„
        
        self._cleanup_thread = threading.Thread(target=cleanup_loop, daemon=True)
        self._cleanup_thread.start()
        
        self.logger.info("Auto cleanup started")
    
    def stop_auto_cleanup(self):
        """ìë™ ì •ë¦¬ ì¤‘ì§€"""
        if not self._cleanup_active:
            return
        
        self._cleanup_active = False
        if self._cleanup_thread and self._cleanup_thread.is_alive():
            self._cleanup_thread.join(timeout=10)
        
        self.logger.info("Auto cleanup stopped")
    
    def _perform_cleanup(self):
        """ì •ë¦¬ ì‘ì—… ìˆ˜í–‰"""
        try:
            cutoff_date = datetime.now() - timedelta(days=self.config.delete_after_days)
            
            with sqlite3.connect(str(self.db_path)) as conn:
                # ì˜¤ë˜ëœ ì•„ì¹´ì´ë¸Œ ì¡°íšŒ
                cursor = conn.execute("""
                SELECT archive_id, archive_path FROM archives 
                WHERE created_at < ?
                """, (cutoff_date.isoformat(),))
                
                old_archives = cursor.fetchall()
                
                # ì•„ì¹´ì´ë¸Œ íŒŒì¼ ë° ë°ì´í„° ì‚­ì œ
                for archive_id, archive_path in old_archives:
                    try:
                        # íŒŒì¼ ì‚­ì œ
                        path = Path(archive_path)
                        if path.exists():
                            path.unlink()
                        
                        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚­ì œ
                        conn.execute("DELETE FROM archives WHERE archive_id = ?", (archive_id,))
                        conn.execute("DELETE FROM archive_index WHERE archive_id = ?", (archive_id,))
                        conn.execute("DELETE FROM duplicates WHERE archive_id = ?", (archive_id,))
                        
                        self.logger.info(f"Cleaned up old archive: {archive_id}")
                        
                    except Exception as e:
                        self.logger.warning(f"Failed to cleanup archive {archive_id}: {e}")
                
                conn.commit()
                
        except Exception as e:
            self.logger.error(f"Cleanup operation failed: {e}")
    
    def _perform_tier_migration(self):
        """ìŠ¤í† ë¦¬ì§€ ê³„ì¸µ ë§ˆì´ê·¸ë ˆì´ì…˜ ìˆ˜í–‰"""
        try:
            if not self.config.enable_tiered_storage:
                return
            
            current_time = datetime.now()
            
            with sqlite3.connect(str(self.db_path)) as conn:
                # ë§ˆì´ê·¸ë ˆì´ì…˜ í›„ë³´ ì¡°íšŒ
                cursor = conn.execute("""
                SELECT archive_id, created_at, storage_tier FROM archives 
                WHERE storage_tier != 'frozen'
                """)
                
                for archive_id, created_at_str, current_tier in cursor.fetchall():
                    created_at = datetime.fromisoformat(created_at_str)
                    age_days = (current_time - created_at).days
                    
                    target_tier = None
                    
                    if current_tier == 'hot' and age_days > self.config.hot_storage_days:
                        target_tier = StorageTier.WARM
                    elif current_tier == 'warm' and age_days > self.config.warm_storage_days:
                        target_tier = StorageTier.COLD
                    elif current_tier == 'cold' and age_days > self.config.cold_storage_days:
                        target_tier = StorageTier.FROZEN
                    
                    if target_tier:
                        asyncio.create_task(self.migrate_storage_tier(archive_id, target_tier))
                        
        except Exception as e:
            self.logger.error(f"Tier migration failed: {e}")
    
    def get_archive_statistics(self) -> Dict[str, Any]:
        """ì•„ì¹´ì´ë¸Œ í†µê³„ ë°˜í™˜"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                # ê¸°ë³¸ í†µê³„
                cursor = conn.execute("""
                SELECT 
                    COUNT(*) as total,
                    SUM(original_size_bytes) as total_original_size,
                    SUM(compressed_size_bytes) as total_compressed_size,
                    AVG(compression_ratio) as avg_compression,
                    storage_tier,
                    COUNT(*) as tier_count
                FROM archives 
                GROUP BY storage_tier
                """)
                
                tier_stats = {}
                total_archives = 0
                total_original_size = 0
                total_compressed_size = 0
                
                for row in cursor.fetchall():
                    tier_stats[row[4]] = row[5]
                    if row[4] == list(tier_stats.keys())[0]:  # ì²« ë²ˆì§¸ í–‰ì—ì„œë§Œ
                        total_archives = row[0]
                        total_original_size = row[1] or 0
                        total_compressed_size = row[2] or 0
                
                # ì „ì²´ í†µê³„ ë‹¤ì‹œ ê³„ì‚°
                cursor = conn.execute("""
                SELECT 
                    COUNT(*) as total,
                    SUM(original_size_bytes) as total_original_size,
                    SUM(compressed_size_bytes) as total_compressed_size,
                    AVG(compression_ratio) as avg_compression
                FROM archives
                """)
                
                total_stats = cursor.fetchone()
                
                return {
                    'total_archives': total_stats[0],
                    'total_original_size_bytes': total_stats[1] or 0,
                    'total_compressed_size_bytes': total_stats[2] or 0,
                    'total_original_size_gb': (total_stats[1] or 0) / (1024**3),
                    'total_compressed_size_gb': (total_stats[2] or 0) / (1024**3),
                    'avg_compression_ratio': f"{(total_stats[3] or 0) * 100:.1f}%",
                    'storage_efficiency': f"{((total_stats[1] or 1) - (total_stats[2] or 0)) / (total_stats[1] or 1) * 100:.1f}%",
                    'tier_distribution': tier_stats,
                    'cleanup_active': self._cleanup_active,
                    'indexing_enabled': self.config.enable_indexing,
                    'tiered_storage_enabled': self.config.enable_tiered_storage
                }
                
        except Exception as e:
            self.logger.error(f"Failed to get archive statistics: {e}")
            return {'error': str(e)}
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.stop_auto_cleanup()
            self._save_configuration()
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            if self.temp_dir.exists():
                shutil.rmtree(self.temp_dir, ignore_errors=True)
            
            self.logger.info("Archive manager cleanup completed")
            
        except Exception as e:
            self.logger.error(f"Archive manager cleanup failed: {e}")

# ì „ì—­ ì•„ì¹´ì´ë¸Œ ê´€ë¦¬ì
_global_archive_manager = None

def get_archive_manager(config: Optional[ArchiveConfig] = None, config_file: str = None) -> ArchiveManager:
    """ì „ì—­ ì•„ì¹´ì´ë¸Œ ê´€ë¦¬ì ë°˜í™˜"""
    global _global_archive_manager
    if _global_archive_manager is None:
        _global_archive_manager = ArchiveManager(
            config=config,
            config_file=config_file or "archive_config.json"
        )
    return _global_archive_manager

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    import asyncio
    
    async def test_archive_manager():
        print("ğŸ§ª Archive Manager í…ŒìŠ¤íŠ¸")
        
        # ì„¤ì •
        config = ArchiveConfig(
            archive_dir="test_archives",
            temp_dir="test_temp_archive",
            source_patterns=["*.log"],
            policy=ArchivePolicy.TIME_BASED,
            archive_after_days=0,  # ì¦‰ì‹œ ì•„ì¹´ì´ë¸Œ
            compression_enabled=True,
            compression_format="gzip",
            enable_indexing=True,
            enable_tiered_storage=True,
            auto_cleanup=False
        )
        
        manager = get_archive_manager(config, "test_archive_config.json")
        
        # í…ŒìŠ¤íŠ¸ ë¡œê·¸ íŒŒì¼ ìƒì„±
        test_logs_dir = Path("test_logs")
        test_logs_dir.mkdir(exist_ok=True)
        
        log_file = test_logs_dir / "test.log"
        with open(log_file, 'w') as f:
            f.write("2024-01-01 10:00:00 - INFO - TestModule - Test log message\n")
            f.write("2024-01-01 10:01:00 - ERROR - TestModule - Error occurred\n")
            f.write("2024-01-01 10:02:00 - WARNING - TestModule - Warning message\n")
        
        print("\n1ï¸âƒ£ íŒŒì¼ ì•„ì¹´ì´ë¸Œ")
        archive_ids = await manager.archive_files(test_logs_dir, "*.log")
        print(f"  ì•„ì¹´ì´ë¸Œëœ íŒŒì¼ ìˆ˜: {len(archive_ids)}")
        
        print("\n2ï¸âƒ£ ì•„ì¹´ì´ë¸Œ ê²€ìƒ‰")
        search_results = await manager.search_archives("error", limit=10)
        print(f"  ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
        for result in search_results:
            print(f"    {result['archive_id']}: {result['level']} - {result['keywords']}")
        
        print("\n3ï¸âƒ£ ì•„ì¹´ì´ë¸Œ í†µê³„")
        stats = manager.get_archive_statistics()
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        print("\n4ï¸âƒ£ ìŠ¤í† ë¦¬ì§€ ê³„ì¸µ ë§ˆì´ê·¸ë ˆì´ì…˜")
        if archive_ids:
            migration_success = await manager.migrate_storage_tier(archive_ids[0], StorageTier.COLD)
            print(f"  ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³µ: {migration_success}")
        
        print("\nğŸ‰ Archive Manager í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
        # ì •ë¦¬
        manager.cleanup()
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
        import shutil
        for dir_name in ["test_archives", "test_temp_archive", "test_logs"]:
            test_dir = Path(dir_name)
            if test_dir.exists():
                shutil.rmtree(test_dir, ignore_errors=True)
        
        test_config = Path("test_archive_config.json")
        if test_config.exists():
            test_config.unlink()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_archive_manager())