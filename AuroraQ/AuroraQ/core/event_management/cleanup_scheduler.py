#!/usr/bin/env python3
"""
TTL ê¸°ë°˜ ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ
P8-3: TTL-based Automatic Cleanup System
"""

import sys
import os
import asyncio
import sqlite3
import json
import logging
import shutil
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import uuid
import time
from collections import defaultdict
import zipfile
import tarfile

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from .ttl_event_manager import EventEntry, EventStatus, TTLEventManager, get_ttl_event_manager
from .expiry_processor import ExpiryProcessor, get_expiry_processor, ProcessingResult

class CleanupScope(Enum):
    """ì •ë¦¬ ë²”ìœ„"""
    EXPIRED_ONLY = "expired_only"           # ë§Œë£Œëœ ì´ë²¤íŠ¸ë§Œ
    PROCESSED_ONLY = "processed_only"       # ì²˜ë¦¬ëœ ì´ë²¤íŠ¸ë§Œ
    ARCHIVED_ONLY = "archived_only"         # ì•„ì¹´ì´ë¸Œëœ ì´ë²¤íŠ¸ë§Œ
    CANCELLED_ONLY = "cancelled_only"       # ì·¨ì†Œëœ ì´ë²¤íŠ¸ë§Œ
    ALL_INACTIVE = "all_inactive"           # ëª¨ë“  ë¹„í™œì„± ì´ë²¤íŠ¸
    DATABASE_OPTIMIZATION = "db_optimization"  # ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

class CleanupAction(Enum):
    """ì •ë¦¬ ì•¡ì…˜"""
    DELETE = "delete"                       # ì™„ì „ ì‚­ì œ
    ARCHIVE_TO_FILE = "archive_to_file"    # íŒŒì¼ë¡œ ì•„ì¹´ì´ë¸Œ
    COMPRESS = "compress"                   # ì••ì¶• ì €ì¥
    BACKUP_AND_DELETE = "backup_and_delete"  # ë°±ì—… í›„ ì‚­ì œ
    MOVE_TO_COLD_STORAGE = "move_to_cold"  # ì½œë“œ ìŠ¤í† ë¦¬ì§€ ì´ë™
    SUMMARIZE = "summarize"                 # ìš”ì•½ ì •ë³´ë§Œ ë³´ê´€

class CleanupFrequency(Enum):
    """ì •ë¦¬ ì£¼ê¸°"""
    HOURLY = "hourly"
    DAILY = "daily"
    WEEKLY = "weekly"
    MONTHLY = "monthly"
    CUSTOM = "custom"

@dataclass
class CleanupPolicy:
    """ì •ë¦¬ ì •ì±…"""
    policy_id: str
    scope: CleanupScope
    action: CleanupAction
    frequency: CleanupFrequency
    
    # ì¡°ê±´ ì„¤ì •
    age_threshold_hours: int = 24           # ìµœì†Œ ë‚˜ì´ (ì‹œê°„)
    max_records_to_keep: Optional[int] = None  # ìœ ì§€í•  ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜
    size_threshold_mb: Optional[float] = None  # í¬ê¸° ì„ê³„ê°’ (MB)
    
    # í•„í„° ì¡°ê±´
    event_type_patterns: List[str] = field(default_factory=list)  # ì´ë²¤íŠ¸ íƒ€ì… íŒ¨í„´
    tag_filters: List[str] = field(default_factory=list)         # íƒœê·¸ í•„í„°
    priority_filters: List[str] = field(default_factory=list)    # ìš°ì„ ìˆœìœ„ í•„í„°
    
    # ì‹¤í–‰ ì¡°ê±´
    enabled: bool = True
    dry_run: bool = False                   # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
    require_confirmation: bool = False      # í™•ì¸ í•„ìš”
    
    # ë³´ê´€ ì„¤ì •
    archive_path: Optional[str] = None      # ì•„ì¹´ì´ë¸Œ ê²½ë¡œ
    compression_level: int = 6              # ì••ì¶• ë ˆë²¨ (1-9)
    encryption_enabled: bool = False        # ì•”í˜¸í™” ì‚¬ìš©
    
    # ë©”íƒ€ë°ì´í„°
    created_at: datetime = field(default_factory=datetime.now)
    last_executed: Optional[datetime] = None
    description: str = ""

@dataclass
class CleanupResult:
    """ì •ë¦¬ ê²°ê³¼"""
    policy_id: str
    execution_id: str
    started_at: datetime
    completed_at: Optional[datetime] = None
    
    # ì²˜ë¦¬ í†µê³„
    records_scanned: int = 0
    records_cleaned: int = 0
    records_archived: int = 0
    records_failed: int = 0
    
    # í¬ê¸° ì •ë³´
    bytes_cleaned: int = 0
    bytes_archived: int = 0
    
    # ì„±ëŠ¥ ì •ë³´
    execution_time_seconds: float = 0.0
    throughput_records_per_second: float = 0.0
    
    # ì˜¤ë¥˜ ì •ë³´
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    
    # ìƒíƒœ
    success: bool = False
    dry_run: bool = False

@dataclass
class CleanupMetrics:
    """ì •ë¦¬ ë©”íŠ¸ë¦­"""
    total_executions: int = 0
    successful_executions: int = 0
    failed_executions: int = 0
    
    total_records_cleaned: int = 0
    total_bytes_cleaned: int = 0
    
    average_execution_time: float = 0.0
    last_execution_time: Optional[datetime] = None
    
    # ì •ì±…ë³„ í†µê³„
    policy_stats: Dict[str, Dict[str, Any]] = field(default_factory=dict)

class CleanupScheduler:
    """TTL ê¸°ë°˜ ìë™ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, ttl_manager=None, expiry_processor=None):
        # ë¡œê±° ì´ˆê¸°í™”
        self.logger = logging.getLogger(__name__)
        
        # ë§¤ë‹ˆì € ì°¸ì¡°
        self.ttl_manager = ttl_manager or get_ttl_event_manager()
        self.expiry_processor = expiry_processor or get_expiry_processor()
        
        # ì •ì±… ê´€ë¦¬
        self.cleanup_policies: Dict[str, CleanupPolicy] = {}
        
        # ì‹¤í–‰ ì œì–´
        self.is_running = False
        self.scheduler_task: Optional[asyncio.Task] = None
        self.policy_tasks: Dict[str, asyncio.Task] = {}
        
        # ë©”íŠ¸ë¦­
        self.metrics = CleanupMetrics()
        self.execution_history: List[CleanupResult] = []
        
        # ì„¤ì •
        self.base_archive_path = Path("archives")
        self.base_archive_path.mkdir(parents=True, exist_ok=True)
        
        self.temp_path = Path("temp_cleanup")
        self.temp_path.mkdir(parents=True, exist_ok=True)
        
        # ê¸°ë³¸ ì •ì±… ë¡œë“œ
        self._load_default_policies()
        
        self.logger.info("Cleanup Scheduler initialized")
    
    def _load_default_policies(self):
        """ê¸°ë³¸ ì •ë¦¬ ì •ì±… ë¡œë“œ"""
        
        default_policies = [
            # ì¼ì¼ ë§Œë£Œ ì´ë²¤íŠ¸ ì •ë¦¬
            CleanupPolicy(
                policy_id="daily_expired_cleanup",
                scope=CleanupScope.EXPIRED_ONLY,
                action=CleanupAction.BACKUP_AND_DELETE,
                frequency=CleanupFrequency.DAILY,
                age_threshold_hours=24,
                archive_path=str(self.base_archive_path / "expired"),
                description="ë§¤ì¼ 24ì‹œê°„ ì´ìƒ ëœ ë§Œë£Œ ì´ë²¤íŠ¸ ë°±ì—… í›„ ì‚­ì œ"
            ),
            
            # ì£¼ê°„ ì²˜ë¦¬ëœ ì´ë²¤íŠ¸ ì•„ì¹´ì´ë¸Œ
            CleanupPolicy(
                policy_id="weekly_processed_archive",
                scope=CleanupScope.PROCESSED_ONLY,
                action=CleanupAction.ARCHIVE_TO_FILE,
                frequency=CleanupFrequency.WEEKLY,
                age_threshold_hours=168,  # 1ì£¼ì¼
                max_records_to_keep=1000,
                archive_path=str(self.base_archive_path / "processed"),
                compression_level=7,
                description="ì£¼ê°„ ì²˜ë¦¬ëœ ì´ë²¤íŠ¸ ì••ì¶• ì•„ì¹´ì´ë¸Œ"
            ),
            
            # ì›”ê°„ ì•„ì¹´ì´ë¸Œëœ ì´ë²¤íŠ¸ ì••ì¶•
            CleanupPolicy(
                policy_id="monthly_archive_compress",
                scope=CleanupScope.ARCHIVED_ONLY,
                action=CleanupAction.COMPRESS,
                frequency=CleanupFrequency.MONTHLY,
                age_threshold_hours=720,  # 30ì¼
                archive_path=str(self.base_archive_path / "compressed"),
                compression_level=9,
                description="ì›”ê°„ ì˜¤ë˜ëœ ì•„ì¹´ì´ë¸Œ ì´ë²¤íŠ¸ ê³ ì••ì¶•"
            ),
            
            # ì‹œê°„ë³„ ì·¨ì†Œëœ ì´ë²¤íŠ¸ ì •ë¦¬
            CleanupPolicy(
                policy_id="hourly_cancelled_cleanup",
                scope=CleanupScope.CANCELLED_ONLY,
                action=CleanupAction.DELETE,
                frequency=CleanupFrequency.HOURLY,
                age_threshold_hours=1,
                description="ì‹œê°„ë³„ ì·¨ì†Œëœ ì´ë²¤íŠ¸ ì¦‰ì‹œ ì‚­ì œ"
            ),
            
            # ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
            CleanupPolicy(
                policy_id="weekly_db_optimization",
                scope=CleanupScope.DATABASE_OPTIMIZATION,
                action=CleanupAction.SUMMARIZE,
                frequency=CleanupFrequency.WEEKLY,
                description="ì£¼ê°„ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë° í†µê³„ ê°±ì‹ "
            )
        ]
        
        for policy in default_policies:
            self.cleanup_policies[policy.policy_id] = policy
        
        self.logger.info(f"Loaded {len(default_policies)} default cleanup policies")
    
    def add_cleanup_policy(self, policy: CleanupPolicy):
        """ì •ë¦¬ ì •ì±… ì¶”ê°€"""
        self.cleanup_policies[policy.policy_id] = policy
        self.logger.info(f"Added cleanup policy: {policy.policy_id}")
    
    def remove_cleanup_policy(self, policy_id: str) -> bool:
        """ì •ë¦¬ ì •ì±… ì œê±°"""
        if policy_id in self.cleanup_policies:
            # ì‹¤í–‰ ì¤‘ì¸ íƒœìŠ¤í¬ ì·¨ì†Œ
            if policy_id in self.policy_tasks:
                self.policy_tasks[policy_id].cancel()
                del self.policy_tasks[policy_id]
            
            del self.cleanup_policies[policy_id]
            self.logger.info(f"Removed cleanup policy: {policy_id}")
            return True
        return False
    
    async def start_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        if self.is_running:
            self.logger.warning("Cleanup scheduler already running")
            return
        
        self.is_running = True
        self.scheduler_task = asyncio.create_task(self._scheduler_loop())
        self.logger.info("Cleanup scheduler started")
    
    async def stop_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
        self.is_running = False
        
        # ë©”ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ íƒœìŠ¤í¬ ì·¨ì†Œ
        if self.scheduler_task and not self.scheduler_task.done():
            self.scheduler_task.cancel()
            try:
                await self.scheduler_task
            except asyncio.CancelledError:
                pass
        
        # ëª¨ë“  ì •ì±… íƒœìŠ¤í¬ ì·¨ì†Œ
        for task in self.policy_tasks.values():
            if not task.done():
                task.cancel()
        
        # íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
        if self.policy_tasks:
            await asyncio.gather(*self.policy_tasks.values(), return_exceptions=True)
        
        self.policy_tasks.clear()
        self.logger.info("Cleanup scheduler stopped")
    
    async def _scheduler_loop(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ë£¨í”„"""
        while self.is_running:
            try:
                # ì‹¤í–‰í•  ì •ì±…ë“¤ í™•ì¸
                policies_to_execute = self._get_policies_to_execute()
                
                # ì •ì±… ì‹¤í–‰
                for policy in policies_to_execute:
                    if policy.policy_id not in self.policy_tasks or self.policy_tasks[policy.policy_id].done():
                        self.policy_tasks[policy.policy_id] = asyncio.create_task(
                            self._execute_cleanup_policy(policy)
                        )
                
                # ì™„ë£Œëœ íƒœìŠ¤í¬ ì •ë¦¬
                completed_tasks = [
                    policy_id for policy_id, task in self.policy_tasks.items()
                    if task.done()
                ]
                for policy_id in completed_tasks:
                    del self.policy_tasks[policy_id]
                
                # 1ë¶„ ëŒ€ê¸°
                await asyncio.sleep(60)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Scheduler loop error: {e}")
                await asyncio.sleep(300)  # 5ë¶„ ëŒ€ê¸°
    
    def _get_policies_to_execute(self) -> List[CleanupPolicy]:
        """ì‹¤í–‰í•  ì •ì±…ë“¤ ë°˜í™˜"""
        now = datetime.now()
        policies_to_execute = []
        
        for policy in self.cleanup_policies.values():
            if not policy.enabled:
                continue
            
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš° ìŠ¤í‚µ
            if policy.policy_id in self.policy_tasks and not self.policy_tasks[policy.policy_id].done():
                continue
            
            should_execute = False
            
            # ì£¼ê¸°ë³„ ì‹¤í–‰ ì¡°ê±´ í™•ì¸
            if policy.frequency == CleanupFrequency.HOURLY:
                if not policy.last_executed or (now - policy.last_executed).total_seconds() >= 3600:
                    should_execute = True
            
            elif policy.frequency == CleanupFrequency.DAILY:
                if not policy.last_executed or (now - policy.last_executed).days >= 1:
                    should_execute = True
            
            elif policy.frequency == CleanupFrequency.WEEKLY:
                if not policy.last_executed or (now - policy.last_executed).days >= 7:
                    should_execute = True
            
            elif policy.frequency == CleanupFrequency.MONTHLY:
                if not policy.last_executed or (now - policy.last_executed).days >= 30:
                    should_execute = True
            
            if should_execute:
                policies_to_execute.append(policy)
        
        return policies_to_execute
    
    async def _execute_cleanup_policy(self, policy: CleanupPolicy) -> CleanupResult:
        """ì •ë¦¬ ì •ì±… ì‹¤í–‰"""
        execution_id = str(uuid.uuid4())
        result = CleanupResult(
            policy_id=policy.policy_id,
            execution_id=execution_id,
            started_at=datetime.now(),
            dry_run=policy.dry_run
        )
        
        try:
            self.logger.info(f"Executing cleanup policy: {policy.policy_id} ({execution_id})")
            
            # ë²”ìœ„ë³„ ì²˜ë¦¬
            if policy.scope == CleanupScope.DATABASE_OPTIMIZATION:
                await self._execute_database_optimization(policy, result)
            else:
                await self._execute_event_cleanup(policy, result)
            
            result.completed_at = datetime.now()
            result.execution_time_seconds = (
                result.completed_at - result.started_at
            ).total_seconds()
            
            if result.execution_time_seconds > 0:
                result.throughput_records_per_second = result.records_scanned / result.execution_time_seconds
            
            result.success = len(result.errors) == 0
            
            # ì •ì±… ì—…ë°ì´íŠ¸
            policy.last_executed = result.completed_at
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_metrics(result)
            
            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.execution_history.append(result)
            if len(self.execution_history) > 100:  # ìµœëŒ€ 100ê°œ ìœ ì§€
                self.execution_history.pop(0)
            
            self.logger.info(
                f"Cleanup policy completed: {policy.policy_id} - "
                f"Cleaned: {result.records_cleaned}, "
                f"Archived: {result.records_archived}, "
                f"Time: {result.execution_time_seconds:.2f}s"
            )
            
        except Exception as e:
            result.completed_at = datetime.now()
            result.execution_time_seconds = (
                result.completed_at - result.started_at
            ).total_seconds()
            result.success = False
            result.errors.append(f"Policy execution failed: {e}")
            self.logger.error(f"Cleanup policy failed: {policy.policy_id} - {e}")
        
        return result
    
    async def _execute_event_cleanup(self, policy: CleanupPolicy, result: CleanupResult):
        """ì´ë²¤íŠ¸ ì •ë¦¬ ì‹¤í–‰"""
        # ëŒ€ìƒ ì´ë²¤íŠ¸ ì¡°íšŒ
        target_events = await self._get_target_events(policy)
        result.records_scanned = len(target_events)
        
        if not target_events:
            self.logger.debug(f"No events to clean for policy: {policy.policy_id}")
            return
        
        # ì•¡ì…˜ë³„ ì²˜ë¦¬
        for event in target_events:
            try:
                if policy.action == CleanupAction.DELETE:
                    await self._delete_event(event, policy, result)
                
                elif policy.action == CleanupAction.ARCHIVE_TO_FILE:
                    await self._archive_event_to_file(event, policy, result)
                
                elif policy.action == CleanupAction.COMPRESS:
                    await self._compress_event(event, policy, result)
                
                elif policy.action == CleanupAction.BACKUP_AND_DELETE:
                    await self._backup_and_delete_event(event, policy, result)
                
                elif policy.action == CleanupAction.MOVE_TO_COLD_STORAGE:
                    await self._move_to_cold_storage(event, policy, result)
                
                elif policy.action == CleanupAction.SUMMARIZE:
                    await self._summarize_event(event, policy, result)
                
                result.records_cleaned += 1
                
            except Exception as e:
                result.records_failed += 1
                result.errors.append(f"Failed to process event {event.event_id}: {e}")
    
    async def _get_target_events(self, policy: CleanupPolicy) -> List[EventEntry]:
        """ëŒ€ìƒ ì´ë²¤íŠ¸ ì¡°íšŒ"""
        cutoff_time = datetime.now() - timedelta(hours=policy.age_threshold_hours)
        
        # ìƒíƒœë³„ í•„í„°ë§
        target_statuses = []
        if policy.scope == CleanupScope.EXPIRED_ONLY:
            target_statuses = [EventStatus.EXPIRED]
        elif policy.scope == CleanupScope.PROCESSED_ONLY:
            target_statuses = [EventStatus.PROCESSED]
        elif policy.scope == CleanupScope.ARCHIVED_ONLY:
            target_statuses = [EventStatus.ARCHIVED]
        elif policy.scope == CleanupScope.CANCELLED_ONLY:
            target_statuses = [EventStatus.CANCELLED]
        elif policy.scope == CleanupScope.ALL_INACTIVE:
            target_statuses = [EventStatus.EXPIRED, EventStatus.PROCESSED, 
                             EventStatus.ARCHIVED, EventStatus.CANCELLED]
        
        all_events = []
        for status in target_statuses:
            events = await self.ttl_manager.list_events(status=status)
            all_events.extend(events)
        
        # ë‚˜ì´ í•„í„°ë§
        filtered_events = [
            event for event in all_events
            if (event.processed_at or event.created_at) < cutoff_time
        ]
        
        # ì¶”ê°€ í•„í„°ë§
        if policy.event_type_patterns:
            import re
            filtered_events = [
                event for event in filtered_events
                if any(re.match(pattern, event.event_type) for pattern in policy.event_type_patterns)
            ]
        
        if policy.tag_filters:
            filtered_events = [
                event for event in filtered_events
                if any(tag in event.tags for tag in policy.tag_filters)
            ]
        
        if policy.priority_filters:
            filtered_events = [
                event for event in filtered_events
                if event.priority.value in policy.priority_filters
            ]
        
        # ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜ ì œí•œ
        if policy.max_records_to_keep:
            # ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ì œí•œ
            filtered_events.sort(key=lambda x: x.created_at, reverse=True)
            if len(filtered_events) > policy.max_records_to_keep:
                filtered_events = filtered_events[policy.max_records_to_keep:]
        
        return filtered_events
    
    async def _delete_event(self, event: EventEntry, policy: CleanupPolicy, result: CleanupResult):
        """ì´ë²¤íŠ¸ ì‚­ì œ"""
        if not policy.dry_run:
            await self.ttl_manager.delete_event(event.event_id)
        
        # í¬ê¸° ê³„ì‚° (JSON ì§ë ¬í™” í¬ê¸° ì¶”ì •)
        event_size = len(json.dumps(event.to_dict()).encode('utf-8'))
        result.bytes_cleaned += event_size
    
    async def _archive_event_to_file(self, event: EventEntry, policy: CleanupPolicy, result: CleanupResult):
        """ì´ë²¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì•„ì¹´ì´ë¸Œ"""
        if not policy.archive_path:
            raise ValueError("Archive path not specified")
        
        archive_dir = Path(policy.archive_path)
        archive_dir.mkdir(parents=True, exist_ok=True)
        
        # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ êµ¬ì„±
        date_dir = archive_dir / event.created_at.strftime("%Y-%m-%d")
        date_dir.mkdir(parents=True, exist_ok=True)
        
        # ì•„ì¹´ì´ë¸Œ íŒŒì¼ ê²½ë¡œ
        archive_file = date_dir / f"{event.event_id}.json"
        
        if not policy.dry_run:
            # JSONìœ¼ë¡œ ì €ì¥
            event_data = {
                "archived_at": datetime.now().isoformat(),
                "policy_id": policy.policy_id,
                "event": event.to_dict()
            }
            
            with open(archive_file, 'w', encoding='utf-8') as f:
                json.dump(event_data, f, indent=2, ensure_ascii=False)
            
            # ì••ì¶• ì˜µì…˜
            if policy.compression_level > 0:
                await self._compress_file(archive_file, policy.compression_level)
            
            # ì›ë³¸ ì‚­ì œ
            await self.ttl_manager.delete_event(event.event_id)
        
        result.records_archived += 1
        result.bytes_archived += archive_file.stat().st_size if archive_file.exists() else 0
    
    async def _compress_event(self, event: EventEntry, policy: CleanupPolicy, result: CleanupResult):
        """ì´ë²¤íŠ¸ ì••ì¶•"""
        await self._archive_event_to_file(event, policy, result)
    
    async def _backup_and_delete_event(self, event: EventEntry, policy: CleanupPolicy, result: CleanupResult):
        """ì´ë²¤íŠ¸ ë°±ì—… í›„ ì‚­ì œ"""
        # ë°±ì—… (ì•„ì¹´ì´ë¸Œì™€ ë™ì¼)
        await self._archive_event_to_file(event, policy, result)
    
    async def _move_to_cold_storage(self, event: EventEntry, policy: CleanupPolicy, result: CleanupResult):
        """ì½œë“œ ìŠ¤í† ë¦¬ì§€ë¡œ ì´ë™"""
        # ê°„ë‹¨ êµ¬í˜„: íŠ¹ë³„í•œ ì•„ì¹´ì´ë¸Œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        cold_policy = CleanupPolicy(
            policy_id=f"{policy.policy_id}_cold",
            scope=policy.scope,
            action=CleanupAction.ARCHIVE_TO_FILE,
            frequency=policy.frequency,
            archive_path=str(Path(policy.archive_path or "cold_storage") / "cold"),
            compression_level=9,  # ìµœëŒ€ ì••ì¶•
            dry_run=policy.dry_run
        )
        
        await self._archive_event_to_file(event, cold_policy, result)
    
    async def _summarize_event(self, event: EventEntry, policy: CleanupPolicy, result: CleanupResult):
        """ì´ë²¤íŠ¸ ìš”ì•½"""
        # ìš”ì•½ ì •ë³´ë§Œ ë‚¨ê¸°ê³  ì›ë³¸ ì‚­ì œ
        summary = {
            "event_id": event.event_id,
            "event_type": event.event_type,
            "status": event.status.value,
            "priority": event.priority.value,
            "created_at": event.created_at.isoformat(),
            "expires_at": event.expires_at.isoformat(),
            "processed_at": event.processed_at.isoformat() if event.processed_at else None,
            "ttl_seconds": event.ttl_seconds,
            "access_count": event.access_count,
            "tags": event.tags,
            "summarized_at": datetime.now().isoformat(),
            "summary_policy": policy.policy_id
        }
        
        if policy.archive_path:
            summary_dir = Path(policy.archive_path) / "summaries"
            summary_dir.mkdir(parents=True, exist_ok=True)
            
            summary_file = summary_dir / f"summary_{event.event_id}.json"
            
            if not policy.dry_run:
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump(summary, f, indent=2, ensure_ascii=False)
                
                # ì›ë³¸ ì‚­ì œ
                await self.ttl_manager.delete_event(event.event_id)
    
    async def _compress_file(self, file_path: Path, compression_level: int):
        """íŒŒì¼ ì••ì¶•"""
        if compression_level <= 0:
            return
        
        try:
            compressed_path = file_path.with_suffix(file_path.suffix + '.gz')
            
            import gzip
            with open(file_path, 'rb') as f_in:
                with gzip.open(compressed_path, 'wb', compresslevel=compression_level) as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            # ì›ë³¸ íŒŒì¼ ì‚­ì œ
            file_path.unlink()
            
        except Exception as e:
            self.logger.warning(f"Failed to compress file {file_path}: {e}")
    
    async def _execute_database_optimization(self, policy: CleanupPolicy, result: CleanupResult):
        """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”"""
        try:
            if not policy.dry_run:
                # SQLite VACUUM ì‹¤í–‰
                with sqlite3.connect(str(self.ttl_manager.db_path)) as conn:
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    conn.execute("ANALYZE")
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ ì••ì¶•
                    conn.execute("VACUUM")
                    
                    # ì¸ë±ìŠ¤ ì¬êµ¬ì„±
                    conn.execute("REINDEX")
                    
                    conn.commit()
            
            # ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ì •ë³´
            if self.ttl_manager.db_path.exists():
                db_size = self.ttl_manager.db_path.stat().st_size
                result.bytes_cleaned = db_size  # ìµœì í™”ëœ í¬ê¸°
            
            result.records_cleaned = 1  # ìµœì í™” ì‘ì—… 1íšŒ
            
        except Exception as e:
            result.errors.append(f"Database optimization failed: {e}")
    
    def _update_metrics(self, result: CleanupResult):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.metrics.total_executions += 1
        
        if result.success:
            self.metrics.successful_executions += 1
        else:
            self.metrics.failed_executions += 1
        
        self.metrics.total_records_cleaned += result.records_cleaned
        self.metrics.total_bytes_cleaned += result.bytes_cleaned
        
        # í‰ê·  ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸
        if self.metrics.total_executions > 0:
            total_time = (self.metrics.average_execution_time * (self.metrics.total_executions - 1) + 
                         result.execution_time_seconds)
            self.metrics.average_execution_time = total_time / self.metrics.total_executions
        
        self.metrics.last_execution_time = result.completed_at
        
        # ì •ì±…ë³„ í†µê³„
        if result.policy_id not in self.metrics.policy_stats:
            self.metrics.policy_stats[result.policy_id] = {
                "executions": 0,
                "records_cleaned": 0,
                "bytes_cleaned": 0,
                "last_execution": None
            }
        
        policy_stats = self.metrics.policy_stats[result.policy_id]
        policy_stats["executions"] += 1
        policy_stats["records_cleaned"] += result.records_cleaned
        policy_stats["bytes_cleaned"] += result.bytes_cleaned
        policy_stats["last_execution"] = result.completed_at.isoformat() if result.completed_at else None
    
    async def execute_policy_now(self, policy_id: str) -> CleanupResult:
        """ì •ì±… ì¦‰ì‹œ ì‹¤í–‰"""
        policy = self.cleanup_policies.get(policy_id)
        if not policy:
            raise ValueError(f"Policy not found: {policy_id}")
        
        return await self._execute_cleanup_policy(policy)
    
    def get_cleanup_policies(self) -> List[CleanupPolicy]:
        """ì •ë¦¬ ì •ì±… ëª©ë¡ ì¡°íšŒ"""
        return list(self.cleanup_policies.values())
    
    def get_execution_history(self, limit: int = 50) -> List[CleanupResult]:
        """ì‹¤í–‰ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        return self.execution_history[-limit:]
    
    def get_scheduler_metrics(self) -> CleanupMetrics:
        """ìŠ¤ì¼€ì¤„ëŸ¬ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        return self.metrics
    
    def get_scheduler_status(self) -> Dict[str, Any]:
        """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì¡°íšŒ"""
        return {
            "is_running": self.is_running,
            "active_policies": len([p for p in self.cleanup_policies.values() if p.enabled]),
            "total_policies": len(self.cleanup_policies),
            "running_tasks": len(self.policy_tasks),
            "metrics": {
                "total_executions": self.metrics.total_executions,
                "success_rate": (
                    self.metrics.successful_executions / max(1, self.metrics.total_executions) * 100
                ),
                "total_records_cleaned": self.metrics.total_records_cleaned,
                "total_bytes_cleaned": self.metrics.total_bytes_cleaned,
                "average_execution_time": self.metrics.average_execution_time,
                "last_execution": self.metrics.last_execution_time.isoformat() if self.metrics.last_execution_time else None
            },
            "next_executions": self._get_next_execution_times()
        }
    
    def _get_next_execution_times(self) -> Dict[str, str]:
        """ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡"""
        next_executions = {}
        now = datetime.now()
        
        for policy in self.cleanup_policies.values():
            if not policy.enabled:
                continue
            
            next_time = None
            
            if policy.frequency == CleanupFrequency.HOURLY:
                if policy.last_executed:
                    next_time = policy.last_executed + timedelta(hours=1)
                else:
                    next_time = now + timedelta(hours=1)
            
            elif policy.frequency == CleanupFrequency.DAILY:
                if policy.last_executed:
                    next_time = policy.last_executed + timedelta(days=1)
                else:
                    next_time = now + timedelta(days=1)
            
            elif policy.frequency == CleanupFrequency.WEEKLY:
                if policy.last_executed:
                    next_time = policy.last_executed + timedelta(weeks=1)
                else:
                    next_time = now + timedelta(weeks=1)
            
            elif policy.frequency == CleanupFrequency.MONTHLY:
                if policy.last_executed:
                    next_time = policy.last_executed + timedelta(days=30)
                else:
                    next_time = now + timedelta(days=30)
            
            if next_time:
                next_executions[policy.policy_id] = next_time.isoformat()
        
        return next_executions
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            await self.stop_scheduler()
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if self.temp_path.exists():
                shutil.rmtree(self.temp_path, ignore_errors=True)
            
            self.logger.info("Cleanup Scheduler cleanup completed")
        
        except Exception as e:
            self.logger.error(f"Cleanup Scheduler cleanup failed: {e}")

# ì „ì—­ ìŠ¤ì¼€ì¤„ëŸ¬
_global_cleanup_scheduler = None

def get_cleanup_scheduler(ttl_manager=None, expiry_processor=None) -> CleanupScheduler:
    """ì „ì—­ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ ë°˜í™˜"""
    global _global_cleanup_scheduler
    if _global_cleanup_scheduler is None:
        _global_cleanup_scheduler = CleanupScheduler(ttl_manager, expiry_processor)
    return _global_cleanup_scheduler

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    import asyncio
    
    async def test_cleanup_scheduler():
        print("ğŸ§ª Cleanup Scheduler í…ŒìŠ¤íŠ¸")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
        scheduler = CleanupScheduler()
        
        print("\n1ï¸âƒ£ ê¸°ë³¸ ì •ì±… í™•ì¸")
        policies = scheduler.get_cleanup_policies()
        print(f"  ê¸°ë³¸ ì •ì±… ìˆ˜: {len(policies)}")
        for policy in policies[:3]:
            print(f"  - {policy.policy_id}: {policy.description}")
        
        print("\n2ï¸âƒ£ ì»¤ìŠ¤í…€ ì •ì±… ì¶”ê°€")
        custom_policy = CleanupPolicy(
            policy_id="test_custom_policy",
            scope=CleanupScope.EXPIRED_ONLY,
            action=CleanupAction.DELETE,
            frequency=CleanupFrequency.HOURLY,
            age_threshold_hours=1,
            dry_run=True,  # í…ŒìŠ¤íŠ¸ìš© ì‹œë®¬ë ˆì´ì…˜
            description="í…ŒìŠ¤íŠ¸ìš© ì»¤ìŠ¤í…€ ì •ì±…"
        )
        scheduler.add_cleanup_policy(custom_policy)
        print(f"  ì¶”ê°€ëœ ì •ì±…: {custom_policy.policy_id}")
        
        print("\n3ï¸âƒ£ ì •ì±… ì¦‰ì‹œ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)")
        try:
            result = await scheduler.execute_policy_now("test_custom_policy")
            print(f"  ì‹¤í–‰ ê²°ê³¼:")
            print(f"  - ìŠ¤ìº”ëœ ë ˆì½”ë“œ: {result.records_scanned}")
            print(f"  - ì •ë¦¬ëœ ë ˆì½”ë“œ: {result.records_cleaned}")
            print(f"  - ì‹¤í–‰ ì‹œê°„: {result.execution_time_seconds:.2f}ì´ˆ")
            print(f"  - ì„±ê³µ ì—¬ë¶€: {result.success}")
        except Exception as e:
            print(f"  ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        
        print("\n4ï¸âƒ£ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸")
        status = scheduler.get_scheduler_status()
        print(f"  í™œì„± ì •ì±…: {status['active_policies']}")
        print(f"  ì´ ì •ì±…: {status['total_policies']}")
        print(f"  ì„±ê³µë¥ : {status['metrics']['success_rate']:.1f}%")
        
        print("\n5ï¸âƒ£ ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„")
        next_executions = status['next_executions']
        for policy_id, next_time in list(next_executions.items())[:3]:
            print(f"  {policy_id}: {next_time}")
        
        print("\n6ï¸âƒ£ ë©”íŠ¸ë¦­ ì¡°íšŒ")
        metrics = scheduler.get_scheduler_metrics()
        print(f"  ì´ ì‹¤í–‰ íšŸìˆ˜: {metrics.total_executions}")
        print(f"  ì´ ì •ë¦¬ëœ ë ˆì½”ë“œ: {metrics.total_records_cleaned}")
        print(f"  í‰ê·  ì‹¤í–‰ ì‹œê°„: {metrics.average_execution_time:.2f}ì´ˆ")
        
        print("\nğŸ‰ Cleanup Scheduler í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
        # ì •ë¦¬
        await scheduler.cleanup()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_cleanup_scheduler())