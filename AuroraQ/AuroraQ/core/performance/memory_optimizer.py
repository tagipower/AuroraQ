#!/usr/bin/env python3
"""
VPS ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ
FinBERT ì§€ì—° ë¡œë”© ë° ë™ì  ë©”ëª¨ë¦¬ ê´€ë¦¬
"""

import os
import gc
import psutil
import time
import asyncio
import threading
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from pathlib import Path
import weakref

logger = logging.getLogger(__name__)

@dataclass
class MemoryStats:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„"""
    total_mb: float
    used_mb: float
    available_mb: float
    percent: float
    process_mb: float
    timestamp: datetime

@dataclass
class ModelLoadState:
    """ëª¨ë¸ ë¡œë”© ìƒíƒœ"""
    is_loaded: bool = False
    load_time: Optional[datetime] = None
    memory_usage_mb: float = 0.0
    last_used: Optional[datetime] = None
    use_count: int = 0

class LazyModelLoader:
    """ì§€ì—° ë¡œë”© ëª¨ë¸ ê´€ë¦¬ì"""
    
    def __init__(self, memory_limit_mb: float = 3072):  # 3GB
        self.memory_limit_mb = memory_limit_mb
        self.models: Dict[str, Any] = {}
        self.model_states: Dict[str, ModelLoadState] = {}
        self._lock = threading.Lock()
        self._cleanup_thread = None
        self._stop_cleanup = threading.Event()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.memory_history: List[MemoryStats] = []
        self.max_history_size = 100
        
        # ëª¨ë¸ë³„ ì„¤ì •
        self.model_configs = {
            'finbert': {
                'priority': 1,  # ë†’ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„
                'max_idle_minutes': 30,
                'estimated_memory_mb': 1500,
                'loader_func': self._load_finbert_model
            },
            'ppo': {
                'priority': 2,
                'max_idle_minutes': 60,
                'estimated_memory_mb': 500,
                'loader_func': self._load_ppo_model
            }
        }
        
        # ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
        self._start_cleanup_thread()
    
    def get_memory_stats(self) -> MemoryStats:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        memory = psutil.virtual_memory()
        process = psutil.Process()
        
        return MemoryStats(
            total_mb=memory.total / 1024 / 1024,
            used_mb=memory.used / 1024 / 1024,
            available_mb=memory.available / 1024 / 1024,
            percent=memory.percent,
            process_mb=process.memory_info().rss / 1024 / 1024,
            timestamp=datetime.now()
        )
    
    def check_memory_pressure(self) -> bool:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ í™•ì¸"""
        stats = self.get_memory_stats()
        
        # VPS ë©”ëª¨ë¦¬ ì œí•œ ê¸°ì¤€ìœ¼ë¡œ ì••ë°• ìƒíƒœ íŒë‹¨
        process_limit_mb = self.memory_limit_mb * 0.8  # 80% ì œí•œ
        system_pressure = stats.percent > 85  # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ 85% ì´ˆê³¼
        process_pressure = stats.process_mb > process_limit_mb
        
        return system_pressure or process_pressure
    
    async def get_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ì§€ì—° ë¡œë”© ë° ë°˜í™˜"""
        with self._lock:
            # ì´ë¯¸ ë¡œë”©ëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë°˜í™˜
            if model_name in self.models and self.model_states[model_name].is_loaded:
                self._update_model_usage(model_name)
                logger.debug(f"âœ… ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                return self.models[model_name]
            
            # ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ í™•ì¸
            if self.check_memory_pressure():
                logger.warning("âš ï¸ ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ - ê¸°ì¡´ ëª¨ë¸ ì •ë¦¬ ì‹œë„")
                await self._cleanup_idle_models(force=True)
            
            # ëª¨ë¸ ë¡œë”© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            config = self.model_configs.get(model_name)
            if not config:
                logger.error(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}")
                return None
            
            estimated_memory = config['estimated_memory_mb']
            current_stats = self.get_memory_stats()
            
            if current_stats.process_mb + estimated_memory > self.memory_limit_mb:
                logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ {model_name} ë¡œë”© ë¶ˆê°€")
                # ê°•ì œ ì •ë¦¬ ì‹œë„
                await self._cleanup_idle_models(force=True)
                
                # ì¬í™•ì¸
                current_stats = self.get_memory_stats()
                if current_stats.process_mb + estimated_memory > self.memory_limit_mb:
                    logger.error(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ {model_name} ë¡œë”© ì‹¤íŒ¨")
                    return None
            
            # ëª¨ë¸ ë¡œë”©
            logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
            start_time = time.time()
            
            try:
                model = await config['loader_func'](**kwargs)
                if model is None:
                    return None
                
                load_time = time.time() - start_time
                memory_after = self.get_memory_stats()
                actual_memory = memory_after.process_mb - current_stats.process_mb
                
                # ëª¨ë¸ ì €ì¥ ë° ìƒíƒœ ì—…ë°ì´íŠ¸
                self.models[model_name] = model
                self.model_states[model_name] = ModelLoadState(
                    is_loaded=True,
                    load_time=datetime.now(),
                    memory_usage_mb=actual_memory,
                    last_used=datetime.now(),
                    use_count=1
                )
                
                logger.info(
                    f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name} "
                    f"(ì†Œìš”ì‹œê°„: {load_time:.1f}s, ë©”ëª¨ë¦¬: {actual_memory:.1f}MB)"
                )
                
                return model
                
            except Exception as e:
                logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model_name} - {e}")
                return None
    
    def _update_model_usage(self, model_name: str):
        """ëª¨ë¸ ì‚¬ìš© ì •ë³´ ì—…ë°ì´íŠ¸"""
        if model_name in self.model_states:
            state = self.model_states[model_name]
            state.last_used = datetime.now()
            state.use_count += 1
    
    async def _load_finbert_model(self, **kwargs):
        """FinBERT ëª¨ë¸ ë¡œë”©"""
        try:
            # ONNX FinBERT ë¶„ì„ê¸° ë¡œë”©
            from sentiment_service.models.onnx_finbert_analyzer import get_onnx_analyzer, initialize_onnx_analyzer
            
            logger.info("ğŸ“¥ FinBERT ONNX ëª¨ë¸ ë¡œë”©...")
            analyzer = await initialize_onnx_analyzer()
            
            # ì›œì—… ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
            if hasattr(analyzer, '_warmup'):
                analyzer._warmup = lambda: None
            
            return analyzer
            
        except ImportError:
            # ë°±ì—…: ê¸°ë³¸ ê°ì • ë¶„ì„ê¸°
            logger.warning("âš ï¸ ONNX FinBERT ì‚¬ìš© ë¶ˆê°€ - ê¸°ë³¸ ë¶„ì„ê¸° ì‚¬ìš©")
            return self._create_fallback_sentiment_analyzer()
        except Exception as e:
            logger.error(f"âŒ FinBERT ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_ppo_model(self, **kwargs):
        """PPO ëª¨ë¸ ë¡œë”©"""
        try:
            from trading.ppo_agent import PPOAgent, PPOAgentConfig
            
            model_path = kwargs.get('model_path') or os.getenv('PPO_MODEL_PATH', '/app/models/ppo_model.zip')
            
            if not os.path.exists(model_path):
                logger.warning(f"âš ï¸ PPO ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
                return None
            
            logger.info(f"ğŸ“¥ PPO ëª¨ë¸ ë¡œë”©: {model_path}")
            
            config = PPOAgentConfig()
            agent = PPOAgent(config)
            await agent.load_model(model_path)
            
            return agent
            
        except Exception as e:
            logger.error(f"âŒ PPO ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _create_fallback_sentiment_analyzer(self):
        """ë°±ì—… ê°ì • ë¶„ì„ê¸° ìƒì„±"""
        class FallbackSentimentAnalyzer:
            async def analyze_single(self, text: str):
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
                positive_words = ['good', 'great', 'positive', 'bullish', 'up', 'gain', 'profit']
                negative_words = ['bad', 'terrible', 'negative', 'bearish', 'down', 'loss', 'crash']
                
                text_lower = text.lower()
                pos_count = sum(1 for word in positive_words if word in text_lower)
                neg_count = sum(1 for word in negative_words if word in text_lower)
                
                if pos_count > neg_count:
                    score = 0.6 + (pos_count - neg_count) * 0.1
                elif neg_count > pos_count:
                    score = 0.4 - (neg_count - pos_count) * 0.1
                else:
                    score = 0.5
                
                score = max(0.0, min(1.0, score))
                
                from dataclasses import dataclass
                @dataclass
                class SentimentResult:
                    score: float
                    label: str
                    confidence: float
                    processing_time: float = 0.001
                    model_version: str = "fallback-v1.0"
                
                return SentimentResult(
                    score=score,
                    label="positive" if score > 0.6 else "negative" if score < 0.4 else "neutral",
                    confidence=0.5
                )
            
            async def analyze_batch(self, texts: List[str]):
                results = []
                for text in texts:
                    result = await self.analyze_single(text)
                    results.append(result)
                return results
        
        return FallbackSentimentAnalyzer()
    
    async def _cleanup_idle_models(self, force: bool = False):
        """ìœ íœ´ ëª¨ë¸ ì •ë¦¬"""
        if not self.models:
            return
        
        current_time = datetime.now()
        models_to_remove = []
        
        for model_name, state in self.model_states.items():
            if not state.is_loaded or not state.last_used:
                continue
            
            config = self.model_configs.get(model_name, {})
            max_idle_minutes = config.get('max_idle_minutes', 30)
            idle_time = current_time - state.last_used
            
            # ê°•ì œ ì •ë¦¬ ë˜ëŠ” ìœ íœ´ ì‹œê°„ ì´ˆê³¼
            should_remove = force or idle_time > timedelta(minutes=max_idle_minutes)
            
            if should_remove:
                models_to_remove.append(model_name)
        
        # ìš°ì„ ìˆœìœ„ ë‚®ì€ ëª¨ë¸ë¶€í„° ì œê±°
        models_to_remove.sort(key=lambda name: self.model_configs.get(name, {}).get('priority', 0))
        
        for model_name in models_to_remove:
            await self._unload_model(model_name)
    
    async def _unload_model(self, model_name: str):
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        if model_name not in self.models:
            return
        
        logger.info(f"ğŸ—‘ï¸ ëª¨ë¸ ì–¸ë¡œë“œ: {model_name}")
        
        try:
            model = self.models[model_name]
            
            # ëª¨ë¸ë³„ ì •ë¦¬ ì‘ì—…
            if hasattr(model, 'cleanup'):
                await model.cleanup()
            elif hasattr(model, 'close'):
                await model.close()
            
            # ì°¸ì¡° ì œê±°
            del self.models[model_name]
            self.model_states[model_name].is_loaded = False
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            gc.collect()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            stats = self.get_memory_stats()
            logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ í›„: {stats.process_mb:.1f}MB")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
    
    def _start_cleanup_thread(self):
        """ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘"""
        def cleanup_worker():
            while not self._stop_cleanup.is_set():
                try:
                    # 5ë¶„ë§ˆë‹¤ ì •ë¦¬ í™•ì¸
                    if self._stop_cleanup.wait(300):  # 5ë¶„
                        break
                    
                    # ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ í™•ì¸
                    if self.check_memory_pressure():
                        logger.info("ğŸ§¹ ë©”ëª¨ë¦¬ ì••ë°•ìœ¼ë¡œ ì¸í•œ ìë™ ì •ë¦¬ ì‹œì‘")
                        asyncio.run(self._cleanup_idle_models(force=True))
                    else:
                        # ì¼ë°˜ ì •ë¦¬
                        asyncio.run(self._cleanup_idle_models(force=False))
                    
                    # ë©”ëª¨ë¦¬ í†µê³„ ê¸°ë¡
                    stats = self.get_memory_stats()
                    self.memory_history.append(stats)
                    
                    # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
                    if len(self.memory_history) > self.max_history_size:
                        self.memory_history = self.memory_history[-self.max_history_size:]
                        
                except Exception as e:
                    logger.error(f"âŒ ì •ë¦¬ ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}")
        
        self._cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        self._cleanup_thread.start()
        logger.info("ğŸ”„ ë©”ëª¨ë¦¬ ê´€ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘")
    
    def get_model_status(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        status = {
            'memory_stats': self.get_memory_stats().__dict__,
            'memory_pressure': self.check_memory_pressure(),
            'loaded_models': {},
            'total_memory_used_mb': 0.0
        }
        
        for model_name, state in self.model_states.items():
            if state.is_loaded:
                status['loaded_models'][model_name] = {
                    'load_time': state.load_time.isoformat() if state.load_time else None,
                    'memory_usage_mb': state.memory_usage_mb,
                    'last_used': state.last_used.isoformat() if state.last_used else None,
                    'use_count': state.use_count
                }
                status['total_memory_used_mb'] += state.memory_usage_mb
        
        return status
    
    def force_cleanup(self):
        """ê°•ì œ ì •ë¦¬"""
        logger.info("ğŸ§¹ ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘")
        asyncio.run(self._cleanup_idle_models(force=True))
        gc.collect()
    
    def shutdown(self):
        """ì¢…ë£Œ"""
        logger.info("ğŸ›‘ ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ ì¢…ë£Œ")
        self._stop_cleanup.set()
        
        if self._cleanup_thread and self._cleanup_thread.is_alive():
            self._cleanup_thread.join(timeout=5)
        
        # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
        for model_name in list(self.models.keys()):
            asyncio.run(self._unload_model(model_name))

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_memory_optimizer = None

def get_memory_optimizer() -> LazyModelLoader:
    """ì „ì—­ ë©”ëª¨ë¦¬ ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _memory_optimizer
    if _memory_optimizer is None:
        memory_limit_mb = float(os.getenv('VPS_MEMORY_LIMIT_GB', '3')) * 1024
        _memory_optimizer = LazyModelLoader(memory_limit_mb)
    return _memory_optimizer

async def get_finbert_model(**kwargs):
    """FinBERT ëª¨ë¸ ì§€ì—° ë¡œë”©"""
    optimizer = get_memory_optimizer()
    return await optimizer.get_model('finbert', **kwargs)

async def get_ppo_model(**kwargs):
    """PPO ëª¨ë¸ ì§€ì—° ë¡œë”©"""
    optimizer = get_memory_optimizer()
    return await optimizer.get_model('ppo', **kwargs)

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸")
    parser.add_argument("--test-finbert", action="store_true", help="FinBERT ë¡œë”© í…ŒìŠ¤íŠ¸")
    parser.add_argument("--test-ppo", action="store_true", help="PPO ë¡œë”© í…ŒìŠ¤íŠ¸")
    parser.add_argument("--status", action="store_true", help="ìƒíƒœ ì¡°íšŒ")
    
    args = parser.parse_args()
    
    async def run_tests():
        optimizer = get_memory_optimizer()
        
        if args.status:
            status = optimizer.get_model_status()
            print("ğŸ“Š ë©”ëª¨ë¦¬ ìµœì í™” ìƒíƒœ:")
            print(f"  ğŸ’¾ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬: {status['memory_stats']['process_mb']:.1f}MB")
            print(f"  âš ï¸ ë©”ëª¨ë¦¬ ì••ë°•: {'ì˜ˆ' if status['memory_pressure'] else 'ì•„ë‹ˆì˜¤'}")
            print(f"  ğŸ¤– ë¡œë”©ëœ ëª¨ë¸: {len(status['loaded_models'])}ê°œ")
        
        if args.test_finbert:
            print("ğŸ”„ FinBERT ë¡œë”© í…ŒìŠ¤íŠ¸...")
            model = await get_finbert_model()
            if model:
                print("âœ… FinBERT ë¡œë”© ì„±ê³µ")
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
                result = await model.analyze_single("Bitcoin price is going up")
                print(f"  ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {result.score:.3f} ({result.label})")
            else:
                print("âŒ FinBERT ë¡œë”© ì‹¤íŒ¨")
        
        if args.test_ppo:
            print("ğŸ”„ PPO ë¡œë”© í…ŒìŠ¤íŠ¸...")
            model = await get_ppo_model()
            if model:
                print("âœ… PPO ë¡œë”© ì„±ê³µ")
            else:
                print("âŒ PPO ë¡œë”© ì‹¤íŒ¨")
        
        # ìµœì¢… ìƒíƒœ
        final_status = optimizer.get_model_status()
        print(f"\nğŸ“Š ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ: {final_status['memory_stats']['process_mb']:.1f}MB")
    
    asyncio.run(run_tests())

if __name__ == "__main__":
    main()