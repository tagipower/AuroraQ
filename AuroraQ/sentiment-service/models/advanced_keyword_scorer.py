#!/usr/bin/env python3
"""
Advanced Multi-Modal Keyword Scorer
ë©€í‹°ëª¨ë‹¬ ê°ì • ë¶„ì„ ë° ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
"""

import re
import time
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import logging
from functools import lru_cache
import statistics

from .keyword_scorer import KeywordScorer, KeywordScore, SentimentDirection

logger = logging.getLogger(__name__)

class MarketRegime(Enum):
    """ì‹œì¥ êµ­ë©´"""
    BULL_MARKET = "bull_market"
    BEAR_MARKET = "bear_market"
    SIDEWAYS = "sideways"
    HIGH_VOLATILITY = "high_volatility"
    LOW_VOLATILITY = "low_volatility"

class EmotionalState(Enum):
    """ê°ì • ìƒíƒœ"""
    FEAR = "fear"
    GREED = "greed"
    EUPHORIA = "euphoria"
    PANIC = "panic"
    OPTIMISM = "optimism"
    PESSIMISM = "pessimism"
    UNCERTAINTY = "uncertainty"
    CONFIDENCE = "confidence"

@dataclass
class MultiModalSentiment:
    """ë©€í‹°ëª¨ë‹¬ ê°ì • ë¶„ì„ ê²°ê³¼"""
    # ê¸°ë³¸ ê°ì • ìŠ¤ì½”ì–´
    text_sentiment: float
    price_action_sentiment: float
    volume_sentiment: float
    social_engagement: float
    
    # ê³ ê¸‰ í”¼ì²˜
    emotional_state: EmotionalState
    market_regime: MarketRegime
    viral_score: float
    network_effect: float
    
    # ì‹œê³„ì—´ í”¼ì²˜
    momentum_1h: float
    momentum_4h: float
    momentum_24h: float
    volatility_regime: str
    
    # ìœ„í—˜ ì§€í‘œ
    black_swan_probability: float
    tail_risk: float
    herding_behavior: float
    panic_indicator: float
    
    # ë©”íƒ€ í”¼ì²˜
    confidence_calibration: float
    prediction_stability: float
    ensemble_agreement: float
    
    # ê¸°ë³¸ ì •ë³´
    processing_time: float
    timestamp: datetime
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class AdvancedFeatures:
    """ê³ ê¸‰ í”¼ì²˜ ì„¸íŠ¸"""
    # ë©€í‹°ëª¨ë‹¬ ê°ì •
    multimodal_sentiment: Dict[str, float]
    
    # ì‹œê³„ì—´ í”¼ì²˜
    temporal_features: Dict[str, float]
    
    # ë„¤íŠ¸ì›Œí¬ íš¨ê³¼
    network_features: Dict[str, float]
    
    # ìœ„í—˜ ì§€í‘œ
    risk_features: Dict[str, float]
    
    # ê°ì • ì§€ëŠ¥
    emotional_ai: Dict[str, float]
    
    # ì˜ˆì¸¡ ë©”íƒ€ í”¼ì²˜
    prediction_meta: Dict[str, float]

class AdvancedKeywordScorer(KeywordScorer):
    """ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ í‚¤ì›Œë“œ ìŠ¤ì½”ì–´ëŸ¬"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        super().__init__()
        self._initialize_advanced_dictionaries()
        self._initialize_models()
        self.analysis_history = []
        self.regime_detector = MarketRegimeDetector()
        self.emotional_analyzer = EmotionalIntelligence()
        logger.info("AdvancedKeywordScorer initialized successfully")
    
    def _initialize_advanced_dictionaries(self):
        """ê³ ê¸‰ í‚¤ì›Œë“œ ì‚¬ì „ ì´ˆê¸°í™”"""
        
        # ê°ì • ìƒíƒœ í‚¤ì›Œë“œ
        self.emotional_keywords = {
            # ê³µí¬ ê´€ë ¨
            "fear": -0.8, "scared": -0.7, "terrified": -0.9, "worried": -0.5,
            "anxiety": -0.6, "panic": -0.9, "nervous": -0.5, "concerned": -0.4,
            
            # íƒìš• ê´€ë ¨
            "greed": 0.7, "greedy": 0.6, "fomo": 0.8, "euphoric": 0.9,
            "excited": 0.6, "pumped": 0.8, "moon": 0.9, "lambo": 0.8,
            
            # ë¶ˆí™•ì‹¤ì„±
            "uncertain": -0.4, "confused": -0.3, "mixed": -0.2, "unclear": -0.3,
            "volatile": -0.4, "unstable": -0.5, "chaotic": -0.7,
            
            # ì‹ ë¢°
            "confident": 0.7, "certain": 0.6, "sure": 0.5, "convinced": 0.8,
            "bullish": 0.7, "bearish": -0.7, "optimistic": 0.6, "pessimistic": -0.6
        }
        
        # ë„¤íŠ¸ì›Œí¬ íš¨ê³¼ í‚¤ì›Œë“œ
        self.network_keywords = {
            # ë°”ì´ëŸ´ í™•ì‚°
            "viral": 0.8, "trending": 0.7, "popular": 0.6, "buzz": 0.7,
            "hype": 0.6, "attention": 0.5, "spotlight": 0.6, "mainstream": 0.7,
            
            # ì˜í–¥ë ¥
            "influencer": 0.6, "celebrity": 0.7, "endorsement": 0.8, "backing": 0.7,
            "support": 0.5, "adoption": 0.8, "partnership": 0.7, "collaboration": 0.6,
            
            # ì»¤ë®¤ë‹ˆí‹°
            "community": 0.5, "followers": 0.4, "subscribers": 0.4, "members": 0.3,
            "engagement": 0.6, "discussion": 0.4, "debate": 0.3, "conversation": 0.4
        }
        
        # ì‹œì¥ ë¯¸ì‹œêµ¬ì¡° í‚¤ì›Œë“œ
        self.microstructure_keywords = {
            # ìœ ë™ì„±
            "liquidity": 0.4, "liquid": 0.3, "illiquid": -0.6, "thin": -0.5,
            "depth": 0.4, "spread": -0.3, "slippage": -0.5, "impact": -0.3,
            
            # ì£¼ë¬¸ í”Œë¡œìš°
            "buying": 0.5, "selling": -0.5, "accumulation": 0.6, "distribution": -0.6,
            "volume": 0.3, "participation": 0.4, "activity": 0.3, "flow": 0.2,
            
            # ì‹œì¥ êµ¬ì¡°
            "maker": 0.2, "taker": -0.1, "arbitrage": 0.3, "efficiency": 0.4,
            "friction": -0.3, "costs": -0.2, "fees": -0.2, "latency": -0.3
        }
    
    def _initialize_models(self):
        """ê³ ê¸‰ ëª¨ë¸ ì´ˆê¸°í™”"""
        # ì‹œê³„ì—´ ë¶„ì„ ì„¤ì •
        self.lookback_periods = {
            "short": 60,    # 1ì‹œê°„
            "medium": 240,  # 4ì‹œê°„
            "long": 1440    # 24ì‹œê°„
        }
        
        # ìœ„í—˜ ì„ê³„ê°’
        self.risk_thresholds = {
            "black_swan": 0.01,
            "tail_risk": 0.05,
            "volatility": 0.03,
            "correlation": 0.8
        }
        
        # ê°ì • ìº˜ë¦¬ë¸Œë ˆì´ì…˜ íŒŒë¼ë¯¸í„°
        self.emotion_params = {
            "fear_threshold": -0.6,
            "greed_threshold": 0.7,
            "uncertainty_threshold": 0.4,
            "confidence_threshold": 0.8
        }
    
    def analyze_advanced(self, 
                        text: str, 
                        price_data: Optional[Dict] = None,
                        volume_data: Optional[Dict] = None,
                        social_data: Optional[Dict] = None) -> MultiModalSentiment:
        """ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ ê°ì • ë¶„ì„"""
        start_time = time.time()
        
        try:
            # 1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„
            basic_result = self.analyze(text)
            text_sentiment = basic_result.score
            
            # 2. ê°€ê²© í–‰ë™ ê°ì •
            price_sentiment = self._analyze_price_action(price_data) if price_data else 0.0
            
            # 3. ê±°ë˜ëŸ‰ ê°ì •
            volume_sentiment = self._analyze_volume_sentiment(volume_data) if volume_data else 0.0
            
            # 4. ì†Œì…œ ì°¸ì—¬ë„
            social_engagement = self._analyze_social_engagement(social_data) if social_data else 0.0
            
            # 5. ê°ì • ìƒíƒœ ë¶„ë¥˜
            emotional_state = self._classify_emotional_state(text, text_sentiment)
            
            # 6. ì‹œì¥ êµ­ë©´ ê°ì§€
            market_regime = self._detect_market_regime(price_data, volume_data)
            
            # 7. ë°”ì´ëŸ´ ì ìˆ˜
            viral_score = self._calculate_viral_score(text, social_data)
            
            # 8. ë„¤íŠ¸ì›Œí¬ íš¨ê³¼
            network_effect = self._calculate_network_effect(text, social_data)
            
            # 9. ì‹œê³„ì—´ ëª¨ë©˜í…€
            momentum_1h = self._calculate_momentum(price_data, "1h") if price_data else 0.0
            momentum_4h = self._calculate_momentum(price_data, "4h") if price_data else 0.0
            momentum_24h = self._calculate_momentum(price_data, "24h") if price_data else 0.0
            
            # 10. ë³€ë™ì„± êµ­ë©´
            volatility_regime = self._detect_volatility_regime(price_data) if price_data else "normal"
            
            # 11. ìœ„í—˜ ì§€í‘œë“¤
            black_swan_prob = self._calculate_black_swan_probability(price_data, text_sentiment)
            tail_risk = self._calculate_tail_risk(price_data)
            herding_behavior = self._detect_herding_behavior(text, social_data)
            panic_indicator = self._calculate_panic_indicator(text_sentiment, volume_sentiment)
            
            # 12. ë©”íƒ€ í”¼ì²˜ë“¤
            confidence_calibration = self._calibrate_confidence(basic_result.confidence, text_sentiment)
            prediction_stability = self._calculate_prediction_stability()
            ensemble_agreement = self._calculate_ensemble_agreement(text_sentiment, price_sentiment, volume_sentiment)
            
            # ê²°ê³¼ ìƒì„±
            result = MultiModalSentiment(
                text_sentiment=text_sentiment,
                price_action_sentiment=price_sentiment,
                volume_sentiment=volume_sentiment,
                social_engagement=social_engagement,
                emotional_state=emotional_state,
                market_regime=market_regime,
                viral_score=viral_score,
                network_effect=network_effect,
                momentum_1h=momentum_1h,
                momentum_4h=momentum_4h,
                momentum_24h=momentum_24h,
                volatility_regime=volatility_regime,
                black_swan_probability=black_swan_prob,
                tail_risk=tail_risk,
                herding_behavior=herding_behavior,
                panic_indicator=panic_indicator,
                confidence_calibration=confidence_calibration,
                prediction_stability=prediction_stability,
                ensemble_agreement=ensemble_agreement,
                processing_time=time.time() - start_time,
                timestamp=datetime.now(),
                metadata={
                    "basic_result": basic_result.__dict__,
                    "input_sources": {
                        "has_price": price_data is not None,
                        "has_volume": volume_data is not None,
                        "has_social": social_data is not None
                    }
                }
            )
            
            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.analysis_history.append(result)
            if len(self.analysis_history) > 1000:  # ìµœëŒ€ 1000ê°œ ìœ ì§€
                self.analysis_history.pop(0)
            
            return result
            
        except Exception as e:
            logger.error(f"Advanced analysis failed: {e}", exc_info=True)
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return self._create_default_result(start_time)
    
    def _analyze_price_action(self, price_data: Dict) -> float:
        """ê°€ê²© í–‰ë™ ê°ì • ë¶„ì„"""
        try:
            if not price_data or 'prices' not in price_data:
                return 0.0
            
            prices = price_data['prices'][-50:]  # ìµœê·¼ 50ê°œ ë°ì´í„°í¬ì¸íŠ¸
            if len(prices) < 2:
                return 0.0
            
            # ê°€ê²© ë³€í™”ìœ¨
            returns = np.diff(prices) / prices[:-1]
            
            # íŠ¸ë Œë“œ ê°•ë„
            trend = np.mean(returns)
            
            # ë³€ë™ì„±
            volatility = np.std(returns)
            
            # ëª¨ë©˜í…€ (ìµœê·¼ vs ì´ì „)
            recent_momentum = np.mean(returns[-10:]) if len(returns) >= 10 else trend
            
            # ê°ì • ì ìˆ˜ ê³„ì‚° (-1 ~ 1)
            sentiment = np.tanh(trend * 100) * (1 - min(volatility * 10, 0.5))
            sentiment += np.tanh(recent_momentum * 50) * 0.3
            
            return max(-1.0, min(1.0, sentiment))
            
        except Exception as e:
            logger.warning(f"Price action analysis failed: {e}")
            return 0.0
    
    def _analyze_volume_sentiment(self, volume_data: Dict) -> float:
        """ê±°ë˜ëŸ‰ ê°ì • ë¶„ì„"""
        try:
            if not volume_data or 'volumes' not in volume_data:
                return 0.0
            
            volumes = volume_data['volumes'][-50:]
            if len(volumes) < 2:
                return 0.0
            
            # ê±°ë˜ëŸ‰ ë³€í™”ìœ¨
            volume_changes = np.diff(volumes) / (volumes[:-1] + 1e-8)
            
            # í‰ê·  ê±°ë˜ëŸ‰ ëŒ€ë¹„ í˜„ì¬ ê±°ë˜ëŸ‰
            avg_volume = np.mean(volumes[:-1])
            current_volume = volumes[-1]
            volume_ratio = current_volume / (avg_volume + 1e-8)
            
            # ê±°ë˜ëŸ‰ ëª¨ë©˜í…€
            volume_momentum = np.mean(volume_changes[-10:]) if len(volume_changes) >= 10 else 0
            
            # ê°ì • ì ìˆ˜ ê³„ì‚°
            sentiment = 0.0
            
            # ë†’ì€ ê±°ë˜ëŸ‰ = ê´€ì‹¬ ì¦ê°€ = ê¸ì •ì 
            if volume_ratio > 1.5:
                sentiment += 0.3
            elif volume_ratio > 1.2:
                sentiment += 0.1
            elif volume_ratio < 0.7:
                sentiment -= 0.2
            
            # ê±°ë˜ëŸ‰ ì¦ê°€ íŠ¸ë Œë“œ = ê¸ì •ì 
            if volume_momentum > 0.1:
                sentiment += 0.4
            elif volume_momentum < -0.1:
                sentiment -= 0.3
            
            return max(-1.0, min(1.0, sentiment))
            
        except Exception as e:
            logger.warning(f"Volume sentiment analysis failed: {e}")
            return 0.0
    
    def _analyze_social_engagement(self, social_data: Dict) -> float:
        """ì†Œì…œ ì°¸ì—¬ë„ ë¶„ì„"""
        try:
            if not social_data:
                return 0.0
            
            engagement = 0.0
            total_weight = 0.0
            
            # íŠ¸ìœ„í„° ë°ì´í„°
            if 'twitter' in social_data:
                twitter = social_data['twitter']
                likes = twitter.get('likes', 0)
                retweets = twitter.get('retweets', 0)
                replies = twitter.get('replies', 0)
                
                # ì •ê·œí™”ëœ ì°¸ì—¬ë„ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
                twitter_engagement = np.log1p(likes + retweets * 2 + replies * 1.5) / 10
                engagement += twitter_engagement * 0.4
                total_weight += 0.4
            
            # ë ˆë”§ ë°ì´í„°
            if 'reddit' in social_data:
                reddit = social_data['reddit']
                upvotes = reddit.get('upvotes', 0)
                comments = reddit.get('comments', 0)
                
                reddit_engagement = np.log1p(upvotes + comments * 2) / 8
                engagement += reddit_engagement * 0.3
                total_weight += 0.3
            
            # ê¸°íƒ€ í”Œë«í¼
            if 'other_platforms' in social_data:
                other = social_data['other_platforms']
                total_mentions = other.get('mentions', 0)
                
                other_engagement = np.log1p(total_mentions) / 5
                engagement += other_engagement * 0.3
                total_weight += 0.3
            
            if total_weight > 0:
                engagement = engagement / total_weight
            
            return max(0.0, min(1.0, engagement))
            
        except Exception as e:
            logger.warning(f"Social engagement analysis failed: {e}")
            return 0.0
    
    def _classify_emotional_state(self, text: str, sentiment: float) -> EmotionalState:
        """ê°ì • ìƒíƒœ ë¶„ë¥˜"""
        try:
            processed_text = self._preprocess_text(text).lower()
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ê°ì§€
            fear_words = ["fear", "scared", "panic", "worried", "anxiety", "crash", "dump"]
            greed_words = ["moon", "pump", "lambo", "fomo", "greed", "euphoric"]
            uncertainty_words = ["uncertain", "confused", "mixed", "volatile", "unclear"]
            confidence_words = ["confident", "sure", "bullish", "optimistic", "strong"]
            
            fear_count = sum(1 for word in fear_words if word in processed_text)
            greed_count = sum(1 for word in greed_words if word in processed_text)
            uncertainty_count = sum(1 for word in uncertainty_words if word in processed_text)
            confidence_count = sum(1 for word in confidence_words if word in processed_text)
            
            # ê°ì • ì ìˆ˜ì™€ í‚¤ì›Œë“œë¥¼ ê²°í•©í•˜ì—¬ ë¶„ë¥˜
            if sentiment < -0.6 or fear_count >= 2:
                return EmotionalState.FEAR if sentiment < -0.8 else EmotionalState.PESSIMISM
            elif sentiment > 0.7 or greed_count >= 2:
                return EmotionalState.GREED if sentiment > 0.8 else EmotionalState.OPTIMISM
            elif uncertainty_count >= 2 or abs(sentiment) < 0.2:
                return EmotionalState.UNCERTAINTY
            elif confidence_count >= 2 or sentiment > 0.5:
                return EmotionalState.CONFIDENCE
            else:
                return EmotionalState.UNCERTAINTY
                
        except Exception as e:
            logger.warning(f"Emotional state classification failed: {e}")
            return EmotionalState.UNCERTAINTY
    
    def _detect_market_regime(self, price_data: Optional[Dict], volume_data: Optional[Dict]) -> MarketRegime:
        """ì‹œì¥ êµ­ë©´ ê°ì§€"""
        try:
            if not price_data or 'prices' not in price_data:
                return MarketRegime.SIDEWAYS
            
            prices = price_data['prices'][-100:]  # ìµœê·¼ 100ê°œ ë°ì´í„°í¬ì¸íŠ¸
            if len(prices) < 20:
                return MarketRegime.SIDEWAYS
            
            # ê°€ê²© ë³€í™”ìœ¨
            returns = np.diff(prices) / prices[:-1]
            
            # ë³€ë™ì„± ê³„ì‚°
            volatility = np.std(returns) * np.sqrt(len(returns))
            
            # íŠ¸ë Œë“œ ê°•ë„
            trend = np.mean(returns)
            
            # êµ­ë©´ ë¶„ë¥˜
            if volatility > 0.05:  # 5% ì´ìƒ ë³€ë™ì„±
                return MarketRegime.HIGH_VOLATILITY
            elif volatility < 0.01:  # 1% ë¯¸ë§Œ ë³€ë™ì„±
                return MarketRegime.LOW_VOLATILITY
            elif trend > 0.02:  # 2% ì´ìƒ ìƒìŠ¹ íŠ¸ë Œë“œ
                return MarketRegime.BULL_MARKET
            elif trend < -0.02:  # 2% ì´ìƒ í•˜ë½ íŠ¸ë Œë“œ
                return MarketRegime.BEAR_MARKET
            else:
                return MarketRegime.SIDEWAYS
                
        except Exception as e:
            logger.warning(f"Market regime detection failed: {e}")
            return MarketRegime.SIDEWAYS
    
    def _calculate_viral_score(self, text: str, social_data: Optional[Dict]) -> float:
        """ë°”ì´ëŸ´ ì ìˆ˜ ê³„ì‚°"""
        try:
            viral_score = 0.0
            
            # í…ìŠ¤íŠ¸ ê¸°ë°˜ ë°”ì´ëŸ´ í‚¤ì›Œë“œ
            processed_text = self._preprocess_text(text).lower()
            viral_keywords = ["viral", "trending", "buzz", "hype", "attention", "mainstream"]
            viral_count = sum(1 for word in viral_keywords if word in processed_text)
            viral_score += min(viral_count * 0.2, 0.5)
            
            # ì†Œì…œ ë°ì´í„° ê¸°ë°˜
            if social_data:
                # í™•ì‚° ì†ë„ (retweet velocity)
                if 'retweet_velocity' in social_data:
                    velocity = social_data['retweet_velocity']
                    viral_score += min(np.log1p(velocity) / 10, 0.3)
                
                # í”Œë«í¼ ê°„ í™•ì‚°
                if 'cross_platform_spread' in social_data:
                    spread = social_data['cross_platform_spread']
                    viral_score += spread * 0.2
            
            return max(0.0, min(1.0, viral_score))
            
        except Exception as e:
            logger.warning(f"Viral score calculation failed: {e}")
            return 0.0
    
    def _calculate_network_effect(self, text: str, social_data: Optional[Dict]) -> float:
        """ë„¤íŠ¸ì›Œí¬ íš¨ê³¼ ê³„ì‚°"""
        try:
            network_effect = 0.0
            
            # ë„¤íŠ¸ì›Œí¬ í‚¤ì›Œë“œ ë§¤ì¹­
            processed_text = self._preprocess_text(text).lower()
            words = processed_text.split()
            
            for word in words:
                if word in self.network_keywords:
                    network_effect += self.network_keywords[word] * 0.1
            
            # ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ì§€í‘œ
            if social_data:
                # ì¸í”Œë£¨ì–¸ì„œ ì¦í­ë„
                if 'influencer_amplification' in social_data:
                    amplification = social_data['influencer_amplification']
                    network_effect += min(amplification * 0.3, 0.4)
                
                # ì—ì½”ì±”ë²„ íš¨ê³¼
                if 'echo_chamber_score' in social_data:
                    echo = social_data['echo_chamber_score']
                    network_effect += echo * 0.2
            
            return max(0.0, min(1.0, network_effect))
            
        except Exception as e:
            logger.warning(f"Network effect calculation failed: {e}")
            return 0.0
    
    def _calculate_momentum(self, price_data: Dict, period: str) -> float:
        """ëª¨ë©˜í…€ ê³„ì‚°"""
        try:
            prices = price_data['prices']
            
            # ê¸°ê°„ë³„ ë°ì´í„°í¬ì¸íŠ¸ ìˆ˜
            periods = {"1h": -12, "4h": -48, "24h": -144}  # 5ë¶„ ê°„ê²© ê¸°ì¤€
            lookback = periods.get(period, -24)
            
            if len(prices) < abs(lookback):
                return 0.0
            
            recent_prices = prices[lookback:]
            momentum = (recent_prices[-1] - recent_prices[0]) / recent_prices[0]
            
            return max(-1.0, min(1.0, momentum * 10))  # ìŠ¤ì¼€ì¼ë§
            
        except Exception as e:
            logger.warning(f"Momentum calculation failed: {e}")
            return 0.0
    
    def _detect_volatility_regime(self, price_data: Dict) -> str:
        """ë³€ë™ì„± êµ­ë©´ ê°ì§€"""
        try:
            prices = price_data['prices'][-50:]
            if len(prices) < 10:
                return "normal"
            
            returns = np.diff(prices) / prices[:-1]
            volatility = np.std(returns)
            
            if volatility > 0.05:
                return "high"
            elif volatility < 0.01:
                return "low"
            else:
                return "normal"
                
        except Exception as e:
            logger.warning(f"Volatility regime detection failed: {e}")
            return "normal"
    
    def _calculate_black_swan_probability(self, price_data: Optional[Dict], sentiment: float) -> float:
        """ë¸”ë™ ìŠ¤ì™„ í™•ë¥  ê³„ì‚°"""
        try:
            base_probability = 0.01  # 1% ê¸°ë³¸ í™•ë¥ 
            
            # ê·¹ë‹¨ì  ê°ì •ì€ ë¸”ë™ ìŠ¤ì™„ í™•ë¥  ì¦ê°€
            if abs(sentiment) > 0.8:
                base_probability *= 2
            
            # ê°€ê²© ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ ë¶„ì„
            if price_data and 'prices' in price_data:
                prices = price_data['prices'][-50:]
                if len(prices) > 10:
                    returns = np.diff(prices) / prices[:-1]
                    # ê·¹ë‹¨ì  ìˆ˜ìµë¥  ë¹ˆë„
                    extreme_returns = np.sum(np.abs(returns) > 3 * np.std(returns))
                    if extreme_returns > 0:
                        base_probability *= (1 + extreme_returns * 0.5)
            
            return min(base_probability, 0.1)  # ìµœëŒ€ 10%
            
        except Exception as e:
            logger.warning(f"Black swan probability calculation failed: {e}")
            return 0.01
    
    def _calculate_tail_risk(self, price_data: Optional[Dict]) -> float:
        """í…Œì¼ ë¦¬ìŠ¤í¬ ê³„ì‚°"""
        try:
            if not price_data or 'prices' not in price_data:
                return 0.05  # ê¸°ë³¸ê°’
            
            prices = price_data['prices'][-100:]
            if len(prices) < 20:
                return 0.05
            
            returns = np.diff(prices) / prices[:-1]
            
            # VaR (Value at Risk) 95% ì‹ ë¢°êµ¬ê°„
            var_95 = np.percentile(returns, 5)
            
            # í…Œì¼ ë¦¬ìŠ¤í¬ëŠ” VaRì˜ ì ˆëŒ“ê°’ì„ ì •ê·œí™”
            tail_risk = abs(var_95) * 10  # ìŠ¤ì¼€ì¼ë§
            
            return max(0.0, min(1.0, tail_risk))
            
        except Exception as e:
            logger.warning(f"Tail risk calculation failed: {e}")
            return 0.05
    
    def _detect_herding_behavior(self, text: str, social_data: Optional[Dict]) -> float:
        """êµ°ì§‘í–‰ë™ ê°ì§€"""
        try:
            herding_score = 0.0
            
            # í…ìŠ¤íŠ¸ ê¸°ë°˜ êµ°ì§‘í–‰ë™ í‚¤ì›Œë“œ
            processed_text = self._preprocess_text(text).lower()
            herding_words = ["everyone", "all", "consensus", "majority", "crowd", "follow"]
            herding_count = sum(1 for word in herding_words if word in processed_text)
            herding_score += min(herding_count * 0.2, 0.4)
            
            # ì†Œì…œ ë°ì´í„° ê¸°ë°˜
            if social_data:
                # ëª¨ë“  í”Œë«í¼ì—ì„œ ê°™ì€ ë°©í–¥ ê°ì •
                if 'sentiment_agreement' in social_data:
                    agreement = social_data['sentiment_agreement']
                    herding_score += agreement * 0.6
            
            return max(0.0, min(1.0, herding_score))
            
        except Exception as e:
            logger.warning(f"Herding behavior detection failed: {e}")
            return 0.0
    
    def _calculate_panic_indicator(self, text_sentiment: float, volume_sentiment: float) -> float:
        """íŒ¨ë‹‰ ì§€í‘œ ê³„ì‚°"""
        try:
            # ë¶€ì •ì  ê°ì • + ë†’ì€ ê±°ë˜ëŸ‰ = íŒ¨ë‹‰
            if text_sentiment < -0.6 and volume_sentiment > 0.3:
                panic = abs(text_sentiment) * volume_sentiment
                return max(0.0, min(1.0, panic))
            return 0.0
            
        except Exception as e:
            logger.warning(f"Panic indicator calculation failed: {e}")
            return 0.0
    
    def _calibrate_confidence(self, base_confidence: float, sentiment: float) -> float:
        """ì‹ ë¢°ë„ ìº˜ë¦¬ë¸Œë ˆì´ì…˜"""
        try:
            # ê·¹ë‹¨ì  ê°ì •ì¼ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ì„
            extreme_factor = abs(sentiment) * 0.3
            calibrated = base_confidence + extreme_factor
            return max(0.0, min(1.0, calibrated))
            
        except Exception as e:
            logger.warning(f"Confidence calibration failed: {e}")
            return base_confidence
    
    def _calculate_prediction_stability(self) -> float:
        """ì˜ˆì¸¡ ì•ˆì •ì„± ê³„ì‚°"""
        try:
            if len(self.analysis_history) < 5:
                return 0.5  # ê¸°ë³¸ê°’
            
            # ìµœê·¼ 5ê°œ ë¶„ì„ì˜ ê°ì • ì ìˆ˜ ë¶„ì‚°
            recent_sentiments = [h.text_sentiment for h in self.analysis_history[-5:]]
            stability = 1.0 - min(np.std(recent_sentiments), 1.0)
            
            return max(0.0, min(1.0, stability))
            
        except Exception as e:
            logger.warning(f"Prediction stability calculation failed: {e}")
            return 0.5
    
    def _calculate_ensemble_agreement(self, text_sentiment: float, price_sentiment: float, volume_sentiment: float) -> float:
        """ì•™ìƒë¸” í•©ì˜ë„ ê³„ì‚°"""
        try:
            sentiments = [text_sentiment, price_sentiment, volume_sentiment]
            sentiments = [s for s in sentiments if s != 0.0]  # 0ì´ ì•„ë‹Œ ê°’ë§Œ
            
            if len(sentiments) < 2:
                return 0.5
            
            # ëª¨ë“  ê°ì • ì ìˆ˜ê°€ ê°™ì€ ë°©í–¥ì¸ì§€ í™•ì¸
            positive_count = sum(1 for s in sentiments if s > 0.1)
            negative_count = sum(1 for s in sentiments if s < -0.1)
            neutral_count = len(sentiments) - positive_count - negative_count
            
            max_agreement = max(positive_count, negative_count, neutral_count)
            agreement = max_agreement / len(sentiments)
            
            return max(0.0, min(1.0, agreement))
            
        except Exception as e:
            logger.warning(f"Ensemble agreement calculation failed: {e}")
            return 0.5
    
    def _create_default_result(self, start_time: float) -> MultiModalSentiment:
        """ê¸°ë³¸ ê²°ê³¼ ìƒì„±"""
        return MultiModalSentiment(
            text_sentiment=0.0,
            price_action_sentiment=0.0,
            volume_sentiment=0.0,
            social_engagement=0.0,
            emotional_state=EmotionalState.UNCERTAINTY,
            market_regime=MarketRegime.SIDEWAYS,
            viral_score=0.0,
            network_effect=0.0,
            momentum_1h=0.0,
            momentum_4h=0.0,
            momentum_24h=0.0,
            volatility_regime="normal",
            black_swan_probability=0.01,
            tail_risk=0.05,
            herding_behavior=0.0,
            panic_indicator=0.0,
            confidence_calibration=0.5,
            prediction_stability=0.5,
            ensemble_agreement=0.5,
            processing_time=time.time() - start_time,
            timestamp=datetime.now(),
            metadata={"error": "Analysis failed, using defaults"}
        )
    
    def extract_advanced_features(self, result: MultiModalSentiment) -> AdvancedFeatures:
        """ê³ ê¸‰ í”¼ì²˜ ì¶”ì¶œ"""
        return AdvancedFeatures(
            multimodal_sentiment={
                "text_sentiment": result.text_sentiment,
                "price_action_sentiment": result.price_action_sentiment,
                "volume_sentiment": result.volume_sentiment,
                "social_engagement": result.social_engagement
            },
            temporal_features={
                "momentum_1h": result.momentum_1h,
                "momentum_4h": result.momentum_4h,
                "momentum_24h": result.momentum_24h,
                "volatility_regime_score": {"high": 1.0, "normal": 0.5, "low": 0.0}[result.volatility_regime]
            },
            network_features={
                "viral_score": result.viral_score,
                "network_effect": result.network_effect,
                "herding_behavior": result.herding_behavior
            },
            risk_features={
                "black_swan_probability": result.black_swan_probability,
                "tail_risk": result.tail_risk,
                "panic_indicator": result.panic_indicator
            },
            emotional_ai={
                "emotional_state_score": {
                    EmotionalState.FEAR: -0.8,
                    EmotionalState.PANIC: -0.9,
                    EmotionalState.PESSIMISM: -0.6,
                    EmotionalState.UNCERTAINTY: 0.0,
                    EmotionalState.CONFIDENCE: 0.6,
                    EmotionalState.OPTIMISM: 0.7,
                    EmotionalState.GREED: 0.8,
                    EmotionalState.EUPHORIA: 0.9
                }[result.emotional_state]
            },
            prediction_meta={
                "confidence_calibration": result.confidence_calibration,
                "prediction_stability": result.prediction_stability,
                "ensemble_agreement": result.ensemble_agreement
            }
        )

class MarketRegimeDetector:
    """ì‹œì¥ êµ­ë©´ ê°ì§€ê¸°"""
    
    def __init__(self):
        self.regime_history = []
    
    def detect_regime(self, data: Dict) -> MarketRegime:
        # êµ¬í˜„ ì˜ˆì •
        return MarketRegime.SIDEWAYS

class EmotionalIntelligence:
    """ê°ì • ì§€ëŠ¥ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.emotion_patterns = {}
    
    def analyze_emotion_evolution(self, history: List) -> Dict:
        # êµ¬í˜„ ì˜ˆì •
        return {}


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import json
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ê³ ê¸‰ ìŠ¤ì½”ì–´ëŸ¬ ì´ˆê¸°í™”
    scorer = AdvancedKeywordScorer()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_text = "Bitcoin surges amid institutional adoption and viral social media buzz"
    test_price_data = {
        "prices": [40000 + i * 100 + np.random.normal(0, 50) for i in range(100)]
    }
    test_volume_data = {
        "volumes": [1000000 + i * 10000 + np.random.normal(0, 50000) for i in range(100)]
    }
    test_social_data = {
        "twitter": {"likes": 1500, "retweets": 300, "replies": 150},
        "reddit": {"upvotes": 250, "comments": 45},
        "retweet_velocity": 50.5,
        "cross_platform_spread": 0.7
    }
    
    print("=== ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ ê°ì • ë¶„ì„ í…ŒìŠ¤íŠ¸ ===\n")
    
    # ê³ ê¸‰ ë¶„ì„ ì‹¤í–‰
    result = scorer.analyze_advanced(
        text=test_text,
        price_data=test_price_data,
        volume_data=test_volume_data,
        social_data=test_social_data
    )
    
    print(f"í…ìŠ¤íŠ¸: {test_text}")
    print(f"ì²˜ë¦¬ì‹œê°„: {result.processing_time*1000:.1f}ms\n")
    
    print("=== ë©€í‹°ëª¨ë‹¬ ê°ì • ìŠ¤ì½”ì–´ ===")
    print(f"ğŸ“ í…ìŠ¤íŠ¸ ê°ì •: {result.text_sentiment:.3f}")
    print(f"ğŸ“ˆ ê°€ê²© í–‰ë™ ê°ì •: {result.price_action_sentiment:.3f}")
    print(f"ğŸ“Š ê±°ë˜ëŸ‰ ê°ì •: {result.volume_sentiment:.3f}")
    print(f"ğŸ’¬ ì†Œì…œ ì°¸ì—¬ë„: {result.social_engagement:.3f}")
    
    print("\n=== ê³ ê¸‰ ì§€í‘œ ===")
    print(f"ğŸ˜¨ ê°ì • ìƒíƒœ: {result.emotional_state.value}")
    print(f"ğŸ¯ ì‹œì¥ êµ­ë©´: {result.market_regime.value}")
    print(f"ğŸ”¥ ë°”ì´ëŸ´ ì ìˆ˜: {result.viral_score:.3f}")
    print(f"ğŸŒ ë„¤íŠ¸ì›Œí¬ íš¨ê³¼: {result.network_effect:.3f}")
    
    print("\n=== ì‹œê³„ì—´ í”¼ì²˜ ===")
    print(f"âš¡ 1ì‹œê°„ ëª¨ë©˜í…€: {result.momentum_1h:.3f}")
    print(f"âš¡ 4ì‹œê°„ ëª¨ë©˜í…€: {result.momentum_4h:.3f}")
    print(f"âš¡ 24ì‹œê°„ ëª¨ë©˜í…€: {result.momentum_24h:.3f}")
    print(f"ğŸ“Š ë³€ë™ì„± êµ­ë©´: {result.volatility_regime}")
    
    print("\n=== ìœ„í—˜ ì§€í‘œ ===")
    print(f"ğŸ¦¢ ë¸”ë™ ìŠ¤ì™„ í™•ë¥ : {result.black_swan_probability:.3f}")
    print(f"ğŸ“‰ í…Œì¼ ë¦¬ìŠ¤í¬: {result.tail_risk:.3f}")
    print(f"ğŸ‘ êµ°ì§‘í–‰ë™: {result.herding_behavior:.3f}")
    print(f"ğŸ˜± íŒ¨ë‹‰ ì§€í‘œ: {result.panic_indicator:.3f}")
    
    print("\n=== ë©”íƒ€ í”¼ì²˜ ===")
    print(f"ğŸ¯ ì‹ ë¢°ë„ ìº˜ë¦¬ë¸Œë ˆì´ì…˜: {result.confidence_calibration:.3f}")
    print(f"ğŸ“ ì˜ˆì¸¡ ì•ˆì •ì„±: {result.prediction_stability:.3f}")
    print(f"ğŸ¤ ì•™ìƒë¸” í•©ì˜ë„: {result.ensemble_agreement:.3f}")
    
    # ê³ ê¸‰ í”¼ì²˜ ì¶”ì¶œ
    advanced_features = scorer.extract_advanced_features(result)
    print("\n=== ì¶”ì¶œëœ ê³ ê¸‰ í”¼ì²˜ ì„¸íŠ¸ ===")
    for category, features in advanced_features.__dict__.items():
        print(f"\n{category.upper()}:")
        for key, value in features.items():
            print(f"  {key}: {value:.3f}")